{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb814b1c",
   "metadata": {},
   "source": [
    "### Data Ingestion (https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92a8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53cb50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x2297e888a50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader1 = TextLoader(\"E:/Agentic AI/25-05-2025/DataIngestion/Speech.txt\")\n",
    "loader1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6becef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'E:/Agentic AI/25-05-2025/DataIngestion/Speech.txt'}, page_content='New Delhi : 31.01.2025\\n\\nDownload : Speeches ADDRESS BY THE HON’BLE PRESIDENT OF INDIA, SMT. DROUPADI MURMU TO PARLIAMENT(161.72 KB)\\nHon’ble Members,\\n\\n1. It gives me immense pleasure to address this session of Parliament.\\n\\nJust two months ago, we celebrated the 75th anniversary of adoption of our Constitution, and only a few days ago, the Indian Republic completed 75 years of its journey. This occasion will elevate India&#39;s pride as the mother of democracy to new heights. On behalf of all the citizens of the country, I pay tribute to Babasaheb Ambedkar and all the framers of the Constitution.\\n\\nHon’ble Members,\\n\\n2. The historic festival of Mahakumbh is also underway in the country. Mahakumbh is a festival of India&#39;s cultural tradition and social consciousness. Millions of devotees from across the country and the world have taken the holy dip at Prayagraj. I express my sorrow over the unfortunate incident that occurred on Mauni Amavasya and wish for the speedy recovery of the injured.\\n\\nA few days ago, we lost the former Prime Minister of the country, Dr. Manmohan Singhji. He served the nation as Prime Minister for ten years and was a long-time member of Parliament. I offer my heartfelt tribute to Manmohan Singhji.\\n\\nHon’ble Members,\\n\\n3. My government is infusing new energy through unprecedented achievements in this Amrit Kaal of India&#39;s development journey. The pace of work has tripled in this third term. Today, the nation is witnessing major decisions and policies being implemented at an extraordinary speed, with the highest priority given to the poor, the middle class, the youth, women, and farmers.\\n\\nIn the third term of my Government, concrete steps have been taken to achieve the goal of providing “Housing for All”. Extending the Pradhan Mantri Awas Yojana, we have decided to provide new homes to an additional three crore families. A budget of 5,36,000 crore rupees has been allocated for this purpose.\\n\\nMy government is committed to granting ownership rights of residential land to the rural poor and promoting financial inclusion. Under the SVAMITVA scheme, we have issued 2.25 crore property cards so far, of which approximately 70 lakh property cards have been distributed in the last six months alone.\\n\\nUnder the PM Kisan Samman Nidhi scheme, 41,000 crore rupees has been disbursed to crores of farmers in recent months. The &quot;Dharti Aaba Tribal Village Utkarsh&quot; campaign has been launched for the upliftment of five crore people from tribal communities, with an allocation of 80,000 crore rupees for this initiative.\\n\\nUnder the Ayushman Bharat scheme, six crore senior citizens aged 70 years and above will receive health insurance, with a health cover of 5 lakh rupees per year.\\n\\nFor small entrepreneurs, the loan limit under the MUDRA scheme has been increased from 10 lakh rupees to 20 lakh rupees. My government has placed special focus on education for the youth and creating new employment opportunities for them. The PM Vidyalakshmi scheme has been introduced to provide financial assistance to meritorious students for higher education. Additionally, one crore youth will be given internship opportunities in the top 500 companies. A new law has been enacted to prevent incidents of paper leaks and ensure transparency in recruitment. Following the spirit of prosperity through cooperation, the government has approved the proposal to establish the ‘Tribhuvan’ Cooperative University.\\n\\nUnder the fourth phase of the Pradhan Mantri Gram Sadak Yojana, the government has sanctioned 70,000 crore rupees to connect 25,000 habitations. As the country celebrates the centenary year of Atal Ji’s birth, the Pradhan Mantri Gram Sadak Yojana continues to embody his vision.\\n\\nCurrently, 71 Vande Bharat, Amrit Bharat, and Namo Bharat trains are operational across the country, with 17 new Vande Bharat trains and one Namo Bharat train added in the past six months. The government has also made swift progress on critical issues like \\'One Nation-One Election&quot; and the &quot;Waqf Act Amendment.\\'\\n\\nHon’ble Members,\\n\\n4. The decade-long tenure of my government has infused new energy into the journey of a ‘Viksit Bharat’. In the vision of a ‘Viksit Bharat’... there is the collective strength of public participation, a roadmap for the nation&#39;s economic progress, the power of technology in the form of a digital revolution, and the foundation of modern infrastructure.\\n\\nThe government is steering India towards becoming the world’s third-largest economy.\\n\\nTo ensure that the journey towards ‘Viksit Bharat’ continues to be guided by the ideals of our Constitution, the government has placed four key principles—service, good governance, prosperity, and pride—at the core of its governance.\\n\\nThe government is making rapid advances in its commitment to reform, perform, and transform.\\n\\nThe guiding mantra of my government is “Sabka Saath, Sabka Vikas, Sabka Vishwas, Sabka Prayas” and its goal is the creation of a ‘Viksit Bharat’.\\n\\nHon’ble Members,\\n\\n5. Development is truly meaningful when its benefits reach the person standing at the last rung of society. This is the essence of Antyodaya, to which my government has been unwaveringly committed.\\n\\nWhen poor people are provided with a dignified life, it instils a sense of empowerment that helps them fight poverty.\\n\\nInitiatives like the construction of 12 crore toilets under the Swachh Bharat Abhiyan, 10 crore free LPG connections under the Pradhan Mantri Ujjwala Yojana, ration for 80 crore needy citizens, the Saubhagya Yojana, and the Jal Jeevan Mission have given poor the confidence that they can live with dignity. Due to such efforts, 25 crore people have overcome poverty and are moving forward in life. They have formed a Neo Middle Class, a group that is infusing new energy into India’s growth journey.\\n\\nHon’ble Members,\\n\\n6. The economic progress of a nation like India is defined by the aspirations of the middle class and the fulfilment of those aspirations. The bigger the dreams of the middle class are, the higher the nation soars. It is my government that has, for the first time, openly acknowledged and appreciated the contributions of the middle class on every occasion.\\n\\nGovernment employees are also significant representatives of the middle class. Recently, my government decided to constitute the Eighth Pay Commission for the welfare of government employees. This decision will lay the foundation for substantial salary increase for government employees in the coming years.\\n\\nAdditionally, the central government has decided to provide 50% assured pension to lakhs of employees under the Unified Pension Scheme, which has been widely welcomed.\\n\\nMy government is equally committed to fulfilling the middle class’s dream of owning a home. Laws like RERA have been introduced to safeguard their dreams. Subsidies on home loans are being provided.\\n\\nThrough the UDAN scheme, about 1.5 crore people have fulfilled their dream of flying in airplanes. Jan Aushadhi Kendras, offering medicines at 80% concessional rates have helped save more than 30,000 crore rupees for citizens. The multifold increase in the number of seats for education in various disciplines has significantly benefited the middle class.\\n\\nAcknowledging the role of taxpayers in nation-building, my government has simplified tax-related processes. The introduction of faceless assessments has enhanced transparency and reduced tax disputes.\\n\\nSenior citizens aged 75 years and above who get only pension now have the choice of deciding about filing their income tax returns.\\n\\nHon’ble Members,\\n\\n7. My government firmly believes in empowering the nation through women-led development.\\n\\nThe Nari Shakti Vandan Adhiniyam, which provides reservation for women in the Lok Sabha and state assemblies, is a significant step in this direction.\\n\\nUnder the National Rural Livelihood Mission, more than 91 lakh self-help groups (SHGs) are being empowered, connecting over 10 crore women across the country. These groups have received over 9 lakh crore rupees through bank linkages.\\n\\nMy government has set a goal of 3 crore Lakhpati Didis. Today, over 1.15 crore Lakhpati Didis are leading dignified lives, with about 50 lakh becoming Lakhpati Didis in just the past six months.\\n\\nThese women are contributing to their family incomes as entrepreneurs.\\n\\nWith the spirit of “Insurance for All,” the Bima Sakhi campaign was launched a few months ago. Our Banking and Digi-Payment Sakhis are playing a crucial role in connecting remote areas with the financial system. Meanwhile, Krishi Sakhis are promoting natural farming, and Pashu Sakhis are strengthening our livestock resources.\\n\\nThe Drone Didi Yojana has become a medium for the economic and technological empowerment of women.\\n\\nIt is a matter of pride for this Parliament that India’s daughters are now flying fighter jets, joining the police force, and leading corporate companies. Following my government’s decision, girls have started enrolling in National Military Schools and National Defence Academy as cadets.\\n\\nOur daughters are also making the nation proud by winning medals in the Olympics.\\n\\nThrough this Parliament, I extend my heartfelt congratulations to the ‘Nari Shakti’.\\n\\nHon’ble Members,\\n\\n8. Over the past decade, India’s youth have come forward to take up the responsibility of driving every major national effort. Today, our youth are making the country proud in every field, from start-ups and sports to space exploration. Lakhs of young people are actively participating in nation-building efforts through the MY Bharat Portal.\\n\\nInitiatives like Make in India, Atmanirbhar Bharat, Startup India, Stand-Up India, and Digital India have created numerous employment opportunities for the youth. In the past two years, the government has provided a record 10 lakh permanent government jobs.\\n\\nTo enhance skills and create new opportunities for the youth, my government has approved a 2 lakh crore rupee package.\\n\\nAn internship program for 1 crore youth will provide them with hands-on experience in real-world work environments. India now has over 1.5 lakh start-ups, which are emerging as pillars of innovation.\\n\\nTo boost the space sector, a 1,000 crore rupees venture capital fund has been launched.\\n\\nIn the QS World Future Skills Index 2025, India has risen to second place globally, showcasing leadership in AI and digital technology adoption in the Future of Work category.\\n\\nSimilarly, India’s rank in the Global Innovation Index has improved significantly, moving up from 76 th to 39 th position.\\n\\nHon’ble Members,\\n\\n9. Through the National Education Policy, my government is establishing a modern education system for students.\\n\\nTo ensure no one is deprived of education, opportunities for learning in mother tongues are being provided. Additionally, various recruitment exams are being conducted in 13 Indian languages, eliminating language barriers.\\n\\nTo foster innovation among children, over 10,000 Atal Tinkering Labs have been established in schools.\\n\\nFor enhancing the Ease of Doing Research, the One Nation-One Subscription Scheme has recently been introduced, offering free access to international research materials.\\n\\nOver the past decade, the number of higher education institutions has increased significantly, and their quality has also improved. In the QS World University Asia Rankings, 163 Indian universities have been included.\\n\\nThe inauguration of the new Nalanda University campus has revived India’s ancient glory in education.\\n\\nThe day is not far when an Indian citizen will travel to space aboard the indigenously developed Gaganyaan spacecraft. The recent success in space docking has further paved the way for India to establish its own space station.\\n\\nJust a few days ago, ISRO conducted its 100 th launch, successfully placing the satellite in orbit. I congratulate ISRO and all the citizens of the country for this achievement.\\n\\nHon’ble Members,\\n\\n10. My government has taken significant steps to create a world- class sports ecosystem in the country. Initiatives like the Khelo India Scheme, the Target Olympic Podium Scheme (TOPS), and the establishment of the National Sports University are contributing to this vision.\\n\\nA special sports centre for Divyang athletes has been opened in Gwalior.\\n\\nIndia’s teams, whether at the Olympics or the Paralympics, have consistently delivered outstanding performances. Recently, India also achieved remarkable success at the World Chess Championship.\\n\\nThrough the Fit India Movement, we are building a strong and empowered youth force.\\n\\nHon’ble Members,\\n\\n11. In building a ‘Viksit Bharat’, the role of research along with the role of farmers, soldiers and science is of immense importance. Our goal is to establish India as a global innovation powerhouse.\\n\\nTo promote research in educational institutions, the National Research Foundation has been established with an outlay of 50,000 crore rupees.\\n\\nAdditionally, 10,000 crore rupees is being invested to foster innovation in science and technology under the Vigyan Dhara Yojana.\\n\\nIndia’s contribution in the field of Artificial Intelligence is being elevated through the launch of the India AI Mission.\\n\\nThe National Quantum Mission aims to position India among the leading nations in the field of frontier technology.\\n\\nMy government has introduced the BioE3 Policy to boost bio- manufacturing.\\n\\nThis policy will serve as the facilitator for the next industrial revolution. The focus of bio-economy is on the efficient utilization of natural resources to create new employment opportunities while preserving the environment.\\n\\nHon’ble Members,\\n\\n12. My government has worked with strong determination to lift the economy out of a state of policy paralysis. Despite global concerns such as the COVID-19 pandemic, its aftermath, and war- related uncertainties, the Indian economy has demonstrated remarkable stability and resilience, proving its strength.\\n\\nMy government has implemented several significant measures to promote Ease of Doing Business.\\n\\nWith the spirit of ‘One Nation, One Tax’, the GST system was introduced, which has been benefiting all states across the country.\\n\\nDue to policies like Make in India, many major global brands now proudly display the label ‘Made in India’ on their products.\\n\\nHon’ble Members,\\n\\n13. India’s small traders, from villages to cities, play a vital role in driving economic progress. My government considers small entrepreneurs as the backbone of the economy and is committed to providing them with new opportunities for self-employment. The Credit Guarantee Scheme for MSMEs and the establishment of e-commerce export hubs are promoting various industries. During this third term, the loan limit under the MUDRA scheme has been increased from 10 lakh rupees to 20 lakh rupees, benefiting crores of small entrepreneurs.\\n\\nMy government has made credit access easier, thereby democratizing financial services. Today, products like loans, credit cards, and insurance are easily accessible to everyone.\\n\\nFor decades, our brothers and sisters earning their livelihoods as street vendors remained excluded from the formal banking system. Today, they are benefiting from the PM SVANidhi Yojana, which allows them to access additional loans to expand their businesses based on their digital transaction records.\\n\\nThe ONDC initiative has made digital commerce more inclusive. Small businesses now have equal opportunities to grow in the online shopping ecosystem.\\n\\nHon’ble Members,\\n\\n14. My government, in the past ten years, has written new chapters of progress, one of which is the golden milestone of India’s digital revolution.\\n\\nToday, India has emerged as a major global player in the field of digital technology. The launch of 5G services in India, alongside other leading nations, stands as a significant milestone in this journey.\\n\\nIndia’s UPI technology has also impressed many developed countries. More than 50% of the world’s real-time digital transactions now take place in India.\\n\\nMy government has utilized digital technology as a tool for social justice and equality. Digital payments are no longer confined to select individuals or classes. Today, even the smallest shopkeepers in India benefit from this facility.\\n\\nBanking services and world class technology such as UPI are now accessible in villages as well. Over the last ten years, more than 5 lakh Common Service Centres have been established, providing citizens with access to dozens of government services online. To minimize government interference in people’s daily lives, my government has emphasized e-governance. For instance, DigiLocker has enabled individuals to access and display their important documents anytime, anywhere.\\n\\nHowever, in an increasingly digital society, cybersecurity has become a crucial issue of national importance. Digital fraud, cybercrime, and emerging technologies like deep fakes pose challenges to our social, economic, and national security. My government has taken numerous measures to control these cyber threats, creating opportunities for employment in the field of cybersecurity for the youth.\\n\\nMy government is continuously working to ensure competence in cybersecurity. As a result of these efforts, India has achieved Tier-1 status in the Global Cybersecurity Index.\\n\\nHon’ble Members,\\n\\n15. The modern infrastructure of any country not only provides its citizens with a better quality of life and gives the nation a new identity but also instils a renewed sense of confidence in the country. Over the past decade, India has achieved several milestones in constructing world-class infrastructure. This modern infrastructure has strengthened India’s image globally, increased investors’ trust in the nation, boosted industries, and created new employment opportunities.\\n\\nMy government is working in mission mode to connect every part of the country with highways and expressways. The PM Gati Shakti National Master Plan has accelerated the pace of project completion.\\n\\nTen years ago, the budget for capex was around 2 lakh crore rupees, which has now increased to over 11 lakh crore rupees in the last budget.\\n\\nContinuing the progress of the last decade, my government has made record investments in the past six months in infrastructure for the future.\\n\\nThe foundation has been laid for India’s first deep-water mega port at Vadhavan. This port, being built at a cost of 76,000 crore rupees, will rank among the top ten ports in the world.\\n\\nI am pleased to inform you that the Udhampur-Srinagar-Baramulla rail link project has been completed, connecting the nation from Kashmir to Kanyakumari through railway line. Under this ambitious project, the Chenab Bridge has been constructed, which is the highest railway bridge in the world. Additionally, India’s first rail cable bridge, the Anji Bridge, has been completed.\\n\\nWork on the Shinkun La tunnel is also progressing successfully. Upon completion in the near future, it will be the world’s highest tunnel, ensuring year-round connectivity between Ladakh and Himachal Pradesh.\\n\\nIndia’s aviation sector is growing rapidly. The country’s airline companies have placed orders for more than 1,700 new aircraft. We are expanding airports to operate such a large fleet. Over the past decade, the number of airports in the country has doubled.\\n\\nHon’ble Members,\\n\\n16. To accelerate the journey towards a Viksit Bharat, it is essential to make our cities future-ready.\\n\\nIn this direction, my government has focused on modernizing urban amenities and making them energy-efficient.\\n\\nSimultaneously, the foundation is being laid for the development of new cities.\\n\\nMy government has decided to invest approximately 28,000 crore rupees to establish 12 industrial nodes and build 100 industrial parks near cities across the country.\\n\\nEfforts to streamline urban transportation are continuously underway. Metro projects in Delhi, Pune, Thane, and Bengaluru, along with the recently launched Namo Bharat Rapid Rail Services on the Ahmedabad-Bhuj route, are shaping the cities of a Viksit Bharat. Just a few weeks ago, work began on the Rithala-Narela- Kundli corridor in Delhi, which will be one of the major sections of the Delhi Metro network. The metro routes in Delhi are expanding rapidly due to my government’s continuous efforts. In 2014, the total metro network in Delhi-NCR was less than 200 kilometres. Now, it has more than doubled.\\n\\nToday, I am extremely pleased to share that India\\'s metro network has crossed the milestone of 1,000 kilometres. India has now become the third largest country in the world in terms of metro networks.\\n\\nAdditionally, the decision to deploy 52,000 electric buses in the country, at an estimated cost of 8,000 crore rupees, will provide smooth and clean urban transportation. This initiative will alsocreate numerous employment opportunities. To enhance connectivity and promote urban tourism, work is also underway on 15 ropeway projects across the nation.\\n\\nHon’ble Members,\\n\\n17. My government has consistently worked on policies of multi- dimensional and inclusive development. Therefore, while emphasizing on physical infrastructure, equal efforts have also been made by my government for a social infrastructure revolution. Providing affordable, accessible, and quality healthcare to every section of society is a top priority for my government. With improved hospital facilities, treatment options, and the availability of medicines, healthcare expenses for ordinary families are steadily decreasing.\\n\\nTo ensure that better healthcare services reach citizens, 1,75,000 Ayushman Arogya Mandirs have been established across the country.\\n\\nConsidering the rising number of cancer patients and the high cost of treatment, several cancer drugs have been exempted from customs duty.\\n\\nNearly 9 crore women have been screened for cervical cancer. Due to the efforts of my government, significant progress has been made in combating encephalitis, with the mortality rate due to this disease reduced to 6%.\\n\\nUnder the National TB Eradication Programme, the number of TB patients has also decreased. I urge all citizens and Hon’ble MPs to contribute towards the success of the TB-free India campaign.\\n\\nIndia has also seen substantial improvements in maternal and infant mortality rates.\\n\\nTo ensure tracking of vaccination programmes for pregnant women and children, the U-WIN portal has been launched. So far, around 30 crore vaccine doses have been recorded on this platform.\\n\\nThrough telemedicine, over 30 crore e-teleconsultations have provided healthcare benefits to citizens.\\n\\nThe government is also working on creating 75,000 new seats in medical colleges over the next five years.\\n\\nThe government is boosting health infrastructure and medical equipment manufacturing. New bulk drug and medical devices parks are being developed in the country, creating numerous employment opportunities.\\n\\nHon’ble Members,\\n\\n18. A modern and self-reliant agricultural system in India is our goal. My government is working with dedication to ensure fair prices of crops to farmers and to increase their income.\\n\\nIn 2023-24, India achieved a record production of 332 million tons of foodgrains. Today, India is the largest producer of milk, pulses, and spices in the world.\\n\\nThe government has consistently increased the Minimum Support Price (MSP) for both Kharif and Rabi crops.\\n\\nOver the past decade, spending on the procurement of rice, wheat, pulses, oilseeds, and coarse grains has tripled.\\n\\nIn the past six months, 109 climate-resilient, bio-fortified, and high- yielding advanced crop varieties have been released to farmers. To strengthen agricultural infrastructure, the scope of the Agriculture Infrastructure Fund Scheme has been expanded. This initiative will boost employment opportunities in rural areas. To enhance oilseed production and achieve self-reliance in edible oils, a National Mission on Oilseeds has been approved.\\n\\nA National Mission is also being implemented to promote Natural Farming.\\n\\nEarlier this year, the duration of the special package for ensuring availability of DAP fertilizer at affordable rates to farmers was extended.\\n\\nTo promote fisheries, 11 Integrated Aqua Parks are being established.\\n\\nHon’ble Members,\\n\\n19. A few weeks ago, the India Meteorological Department completed 150 years. To build a weather-ready and climate-smart India, my government has launched the &quot;Mission Mausam&quot; at a cost of 2,000 crore rupees, which will also benefit our farmers. Following the vision of Babasaheb Ambedkar, my government has made headway on two historic river interlinking projects to provide irrigation and drinking water in the drought-affected areas of the country.\\n\\nThe Ken-Betwa Link Project, with a cost of over 44,000 crore rupees, will benefit millions of brothers and sisters in rural areas of Madhya Pradesh and Uttar Pradesh.\\n\\nThe revised Parbati-Kalisindh-Chambal Link Project will address irrigation and drinking water needs in Rajasthan and Madhya Pradesh.\\n\\nAn additional 12,000 crore rupees has been sanctioned to expedite the completion of the Polavaram Irrigation Project. Hon’ble Members,\\n\\n20. Our 8 lakh cooperative societies and their 29 crore stakeholder members represent nearly 90% of rural India. In recent years, cooperative societies have also expanded in urban areas.\\n\\nVarious initiatives taken for economic empowerment of the cooperative sector are creating numerous employment opportunities.\\n\\nThe year 2025 is being celebrated as the International Year of Cooperatives, and India will play a significant role in this global initiative.\\n\\nHon’ble Members,\\n\\n21. When we discuss the nation’s development and achievements, we are essentially highlighting the capabilities and accomplishments of its citizens. Today, there is collective participation of all in the development of the nation and that is why we are able to realize its true potential.\\n\\nThe greatest beneficiaries of my government’s efforts have been the Dalit, backward, and tribal communities.\\n\\nFor decades after independence, our tribal communities faced neglect. My government has prioritized their welfare.\\n\\nThe \"Dharti Aaba Janjatiya Gram Utkarsh Abhiyan\" and the \"PM- JANMAN Yojana\" are direct examples of this initiative. Nearly 1.25 lakh tribal children are receiving quality education through more than 470 Eklavya Model Residential Schools. In the last 10 years, 30 new medical colleges have been established in tribal-dominated areas.\\n\\nA special National Mission is addressing health issues related to sickle cell within tribal communities, with screening of around 5 crore individuals already completed.\\n\\nMy government has undertaken several initiatives to preserve tribal heritage. This year, the 150 th birth anniversary of Bhagwan Birsa Munda is being celebrated across the country as Janjatiya Gaurav Varsh.\\n\\nHon’ble Members,\\n\\n22. A significant measure of ‘Viksit Bharat’ is balanced development of the country. No region should feel left behind in the journey of progress.\\n\\nMy government is conscious of the aspirations of the people of the North East and has worked to eliminate their sense of alienation. Through more than 10 peace agreements, several factions have been brought onto the path of peace.\\n\\nTo showcase the potential of the eight states of the North East to the entire country, the first-ever Ashtalakshmi Mahotsav was organized.\\n\\nAlong with the development of the North East, the government has initiated a comprehensive development plan for the “Purvodaya” or the eastern states, which will also create new employment opportunities.\\n\\nIn Andaman &amp; Nicobar Islands and Lakshadweep, several developmental projects have been launched, according these regions a crucial role in the nation’s progress.\\n\\nAfter the abrogation of Article 370, there is a conducive environment for development in Jammu &amp; Kashmir. Both the Lok Sabha and Vidhan Sabha elections were conducted in a peaceful manner in Jammu &amp; Kashmir. The people of Jammu &amp; Kashmir deserve commendation for this achievement.\\n\\nHon’ble Members,\\n\\n23. The success of a nation or society is inclusive and all- encompassing only when it is guided by principles. Therefore, my government has always placed the fundamental principles outlined by our Constitution at the core of its policies. In the light of the Constitution, the primary ideological inspiration of my government is ‘Service’.\\n\\nMy government firmly believes that serving 140 crore citizens is its foremost duty, and it is working with utmost sensitivity in this direction.\\n\\nTo provide easy loans to the backward sections of society and sanitation workers, the scope of the PM-Suraj Yojana has been expanded.\\n\\nTo ensure the benefits of government schemes reach differently- abled individuals, over 1 crore Divyang ID cards have been issued. The “Namaste Yojana”, launched for sanitation workers, has been extended to include all those who take up the noble responsibility of cleanliness.\\n\\nWith the goal to ensure that no one is left behind in the journey of a ‘Viksit Bharat’, my government is working with a saturation approach.\\n\\nHon’ble Members,\\n\\n24. The past decade has been a period of revival of India’s cultural consciousness. With pride in our heritage and dedication to progress, we are shaping a future where culture and development move forward together.\\n\\nThis year, we will celebrate the 125 th birth anniversary of Dr. Syama Prasad Mookerjee, who said, “True nationalism lies not only in the physical unity of India but in strengthening its cultural unity.”\\n\\nIn the same spirit, the 2,550 th Nirvana Mahotsav of Lord Mahaveer was celebrated with reverence, and the 525 th birth anniversary of Sant Mirabai was enthusiastically observed across the country. Cultural centres are being established in several countries in memory of the great poet-saint Thiruvalluvar.\\n\\nMy government is promoting national unity through cultural initiatives like Kashi-Tamil Sangamam, Kashi-Telugu Sangamam, and Saurashtra-Tamil Sangamam.\\n\\nHon’ble Members,\\n\\n25. Our manuscripts are a priceless heritage, containing vast knowledge that needs to be studied, researched, and utilized for the benefit of humanity. The process of digitizing and preserving these manuscripts using advanced technology is being initiated on mission mode.\\n\\nA significant pillar of the nation\\'s heritage is our rich linguistic culture. I am pleased to inform you that the government has granted Classical Language status to Assamese, Marathi, Pali, Prakrit, and Bengali. For easy communication in all languages of India, the language platform Bhashini powered by AI is being widely used by the citizens of the country.\\n\\nHon’ble Members,\\n\\n26. Through the efforts of my government, India has established its identity as a global leader on the cultural stage.\\n\\nTo connect all Asian Buddhist countries, my government organized the first Asian Buddhist Conference. Last year, India also hosted the World Heritage Committee’s meeting, with participation from 140 countries.\\n\\nThrough the celebration of International Yoga Day, the entire world is now embracing India’s rich tradition of yoga.\\n\\nHon’ble Members,\\n\\n27. To take the grand edifice of progress to new glories, strong pillars are required. For India’s development, my government has established three strong pillars of Reform, Perform and Transform. Today, these words have become synonyms of India’s new governance model across the world.\\n\\nThe government has conducted an extensive review of laws enacted before the Constitution came into force. Many laws are being repealed or amended to ensure that the entire system can meet the current social and economic challenges.\\n\\nSo far, the government has repealed more than 1,500 obsolete laws. By removing colonial-era laws, a ‘Nyaya Sanhita’ has been introduced in place of the Penal Code.\\n\\nWith ‘Jan Vishwas’ (public trust) and ‘Jan Bhagidari’ (people’s participation), my government is working to make the lives of citizens better. The “Vivad se Vishwas” initiative has been launched to resolve disputes.\\n\\nIn the same spirit, over 40,000 regulations have been simplified or reduced and 3,500 provisions have been decriminalized.\\n\\nMy government has initiated the Aspirational Districts Programme in the country’s most backward areas, implementing a unique experiment in good governance. This programme has led to remarkable progress in health, nutrition, agriculture, social development, and education in these districts. A UNDP report has praised this initiative. Inspired by this success, the government has now launched a campaign for the holistic development of 500 aspirational blocks.\\n\\nFocusing on good governance, the i-GOT Karmayogi Digital Platform has been created, encouraging government employees to enhance their skills and become true Karmayogis. This platform offers 1,700 courses, and more than 2 crore course completions have taken place.\\n\\nHon’ble Members,\\n\\n28. This year, the country is celebrating the 150 th birth anniversary of Sardar Vallabhbhai Patel. Inspired by his vision, my government is moving ahead with the principle of “Nation First”. To ensure defence of the country’s borders and internal security, the government has undertaken historic initiatives.\\n\\nWe have seen highly encouraging results in achieving self-reliance particularly in the defence sector.\\n\\nFrom Make in India, we have transitioned to Make for the World, which is also generating new employment opportunities across the country.\\n\\nIn a historic moment, two warships and a submarine built in India were recently commissioned into the Indian Navy.\\n\\nWe are strengthening self-reliance and self-employment by establishing the Defence Industrial Corridor and promoting defence start-ups.\\n\\nAlong with securing the borders, the development of border areas is also a key component of our strategy. Roads in border areas, along with modern infrastructure like the Atal Tunnel, Sela Tunnel, and Sonamarg Tunnel, have enhanced both defence capabilities and tourism. The Vibrant Villages Programme has been launched in the country’s first villages located along the border.\\n\\nThe final phase of eliminating Left-wing Extremism has also begun. Due to the government’s efforts, the number of districts affected by Left-wing Extremism has reduced from 126 to 38 today.\\n\\nHon’ble Members,\\n\\n29. In an environment of global instability, India is emerging as a pillar of economic, social, and political stability, setting an example for the world. Whether it is the G7 Summit, QUAD, BRICS, SCO, or G20, the world has placed its trust in India’s strength, policy, and intent.\\n\\nToday, India firmly presents its interests even on the largest global platforms. The successful hosting of the G20 Summit and the Delhi Declaration are prime examples. At the Third Global South Summit, India-ASEAN Summit, and the India-CARICOM Summit, we have raised the voice on issues related to the Global South. We have also presented India’s “Vision for the Future” at the Summit of the Future.\\n\\nEarlier this month, my government organized the Pravasi Bharatiya Divas in Bhubaneswar.\\n\\nThe welfare and convenience of our brothers and sisters of the diaspora remain a priority, which is why my government has decided to open six new embassies and four new consulates. Strengthening India’s image as a ‘Vishwa Bandhu’, the country has extended immediate assistance to many disaster-stricken areas across the world.\\n\\nIndia has shared its Digital Public Infrastructure with several nations and set up Jan Aushadhi Kendra.\\n\\nHon’ble Members,\\n\\n30. My government is making decisions keeping in mind not only the present generation but also the future generations. We are steering the nation towards a green future and green jobs.\\n\\nSeveral significant decisions have been taken in the last six months towards achieving the target of 500 GW non-fossil fuel energy capacity by 2030.\\n\\nUnder the PM Surya Ghar Muft Bijli Yojana, rooftop solar systems are being installed at a cost of 75,000 crore rupees. So far, over 7.5 lakh homes have installed rooftop solar systems, creating numerous job opportunities.\\n\\nThe National Green Hydrogen Mission will have an investment of 8 lakh crore rupees and generate over 6 lakh jobs.\\n\\nWe are also accelerating efforts to expand nuclear energy. My government has introduced the Vehicle Scrapping Policy to ensure scientific disposal of old vehicles, which will also generate new employment opportunities.\\n\\nIn line with this, the ‘Ek Ped Maa ke Naam’ campaign was launched on World Environment Day 2024. Millions of citizens have enthusiastically participated, and this initiative has been appreciated worldwide.\\n\\nHon’ble Members,\\n\\n31. Our Bharat is a country of 140 crore people. We have diverse states, diverse regions, and diverse languages, yet as one nation, we have only one identity – Bharat.\\n\\nAnd we have only one resolution, one goal – ‘Viksit Bharat’! We are all firmly committed to making India a developed Nation in the coming years.\\n\\nThis resolution is inspired by the sacrifices of the martyrs of the nation, the compassionate ideals of revered Bapu, and the oath of unity administered to us by the sons of Mother India like Sardar Patel. We must carry forward these inspirations and, with the strength of unity, fulfill the commitment of a Viksit Bharat. Let us once again reaffirm our resolution of unity and commit ourselves to realizing the dreams of India!\\n\\nWhen we move forward together, our future generations will surely witness a developed, empowered, capable, and prosperous Bharat in 2047.\\n\\nI wish you all the very best.\\n\\nThank you. \\nJai Hind! \\nJai Bharat!')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_doc = loader1.load()\n",
    "text_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cb029",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b04184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 0, 'page_label': '1'}, page_content='(CONTINUED)\\nGAME OF THRONES #302\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 8/7/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. ASTAPOR - APPROACH TO PRESENTATION AREA - DAY2.14 2.14\\n(s3e2sc2-14_1.mp3)\\nKRAZNYS\\nTell the Westerosi whore that these Unsullied have been standing \\nhere a day and a night, with no food or water.\\nTRANSLATION\\nIvetra ji live Vesterozia sko bezi Dovoghedhi kizir jortis me tovi \\nsi me banti, do havor dore jedhar dos.\\nPHONETIC\\ni-ve-TRA ji LI-ve ves-te-ro-ZI-a sko BE-zi do-vo-GHE-dhi KI-zir JOR-\\ntis me TO-vi si me BAN-ti, do HA-vor DO-re JE-dhar dos.\\nTell the whore Westerosi that these Unsullied here have stood a day \\nand a night, no food nor water with.\\n-------------------------------------------------------------------\\nEXT. PRESENTATION AREA - CONTINUOUS2.14A 2.14A\\n(s3e2sc2-14A_1.mp3)\\n(CONTINUED)'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 1, 'page_label': '2'}, page_content='CONTINUED:2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     2.\\nKRAZNYS\\nTell the silver-haired slut they will each stand until they drop. \\nSuch is their obedience.\\nTRANSLATION\\nIvetra ji rene eji oghrar gelinko sko majorozlivis eva ruhilis \\n(ERROR: Should be vrogilis). Vagizi poja pihtenkave sa.\\nPHONETIC\\ni-ve-TRA ji RE-ne E-ji OGH-rar ge-LIN-ko sko ma-jo-roz-LI-vis E-va \\nru-HI-lis. va-GI-zi PO-ja pih-ten-KA-ve sa.\\nTell the slut with-the hair of silver that they will stand until \\nthey drop. Such their obedience is.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_2.mp3)\\nMISSANDEI\\nThe Westerosi woman is pleased but speaks no praise to keep the \\nprice down. She wishes to know how they were trained.\\nTRANSLATION\\nJ’abra Vesterozia las kreni, y ivetras dori rije vaghoma gidhmilas \\nqova j’odre. Ebas gimigho skokydho mazmedhis bodmari.\\nPHONETIC\\nJAB-ra ves-te-ro-ZI-a las KRE-ni, yi i-VE-tras DO-re RI-je va-GHO-ma \\ngidh-MI-las KO-va JO-dre. E-bas GI-mi-gho sko-ki-DHO maz-ME-dhis bod-\\nMA-ri.\\nThe woman Westerosi is pleased, but speaks no praise to keep low the \\nprice. She wants to know how they received training.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_3.mp3)\\nKRAZNYS\\nAre all Westerosi pigs so ignorant? Tell her what she would know and \\nbe quick about it. The day is hot.\\nTRANSLATION\\nUni begistos Vesterozii lis kuni dovodedhi, kiz? Ivetra zer skure \\nebilas si adhirikydho. Ji tovi las bani.\\nPHONETIC\\nU-ni be-GIS-tos ves-te-ro-ZI-i lis KU-ni do-vo-DE-dhi, kiz? i-ve-TRA \\nzer SKU-re e-BI-las si a-dhi-ri-KI-dho. ji TO-vi las BA-ni.\\nAll pigs Westerosi are so ignorant, yes? Tell her what she should \\nwant and do so quickly. The day is hot.\\n------------------------------------------------------------------\\nCONTINUED:2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     2.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 2, 'page_label': '3'}, page_content='CONTINUED: (2)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     3.\\n(s3e2sc2-14A_4.mp3)\\nKRAZNYS\\nThe Unsullied have discipline, absolute obedience, absolute loyalty. \\nThey fear nothing.\\nTRANSLATION\\nZer govlimis ji Dovoghedhi ji dogmorve; ji pihtenkave tida; ji \\npazavorve tida. Do zughis doru.\\nPHONETIC\\nzer gov-LI-mis ji do-vo-GHE-dhi ji dog-MOR-ve; ji pih-ten-KA-ve TI-\\nda; ji pa-za-VOR-ve TI-da. do ZU-ghis DO-ru.\\nIt’s with the Unsullied the discipline; the obedience absolute; the \\nloyalty absolute. They don’t fear nothing.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_5.mp3)\\nMISSANDEI\\nThe knight says even the brave men fear death.\\nTRANSLATION\\nJ’azanty ivetras ji vali nedhinki sizi zughilis vi murgho.\\nPHONETIC\\nja-ZAN-ty i-VE-tras ji VA-li ne-DHIN-ki SI-zi zu-GHI-lis vi MUR-gho.\\nThe knight says the brave men even fear the death.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_6.mp3)\\nKRAZNYS\\nTell the old man he smells of piss.\\nTRANSLATION\\nIvetra ji veby tuzis ez orgoz.\\nPHONETIC\\ni-ve-TRA ji VE-by TU-zis ez OR-goz.\\nTell the old man he smells of piss.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_7.mp3)\\nMISSANDEI\\nTruly, master?\\nCONTINUED: (2)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     3.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 3, 'page_label': '4'}, page_content='CONTINUED: (3)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     4.\\nTRANSLATION\\nZvagizi, aeske?\\nPHONETIC\\nzva-GI-zi, AIS-ke?\\nTruly, master?\\n------------------------------------------------------------------\\n(s3e2sc2-14A_8.mp3)\\nKRAZNYS\\nNo, not truly, are you a girl or a goat to ask such a thing? Say \\nthat the Unsullied are not men. Say that death means nothing to \\nthem.\\nTRANSLATION\\nDo zvagizi, ska tala ja hubre pindagho kuno masino? Ivetra sko ji \\nDovoghedhi do si vali. Ivetra sko vi murgho do vetras doru va pon.\\nPHONETIC\\ndoz-va-GI-zi, ska TA-la ja HU-bre PIN-da-gho KU-no MA-si-no? i-ve-\\nTRA sko ji do-vo-GHE-dhi DO si VA-li. i-ve-TRA sko vi MUR-gho do VE-\\ntras DO-ru va pon.\\nNot truly, you are girl or goat to ask such a thing? Say that the \\nUnsullied are not men. Say that the death doesn’t mean nothing to \\nthem.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_9.mp3)\\nKRAZNYS\\nTell this ignorant whore of a Westerner to open her eyes and watch.\\nTRANSLATION\\nIvetra beza live endia dovodedha ezimagho po leos si runevagho.\\nPHONETIC\\ni-ve-TRA BE-za LI-ve en-DI-a do-vo-DE-dha e-ZI-ma-gho po LE-os si ru-\\nNE-va-gho.\\nTell this whore western ignorant to open the eyes and to watch.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_10.mp3)\\nKRAZNYS\\nYour short sword.\\nTRANSLATION\\nOa azandy.\\nCONTINUED: (3)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     4.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 4, 'page_label': '5'}, page_content='CONTINUED: (4)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     5.\\nPHONETIC\\nO-a a-ZAN-dy.\\nYour short sword.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_11.mp3)\\nKRAZNYS\\nTell the midget to stop bleating. This will do him no great harm. \\nMen have no need of nipples.\\nTRANSLATION\\nIvetra ji krubo klimagho ez grigrigho. Kizi do zer honuzlivas kara \\nodreta. Vali do ezi jini va d’ovistos.\\nPHONETIC\\ni-ve-TRA ji KRU-bo KLI-ma-gho ez gri-GRI-gho. KI-zi do zer ho-nuz-LI-\\nvas KA-ra o-DRE-ta. VA-li do E-zi JI-ni va do-VIS-tos.\\nTell the midget to stop with bleating. This not him will see much \\nharmed. Men don’t have need of no nipples.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_12.mp3)\\nKRAZNYS\\nHere, I’m done with you.\\nTRANSLATION\\nAot, av ididan.\\nPHONETIC\\nAOT, av i-DI-dan.\\nHere, you I have finished with.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_13.mp3)\\nSLAVE 2\\nThis one is pleased to have served you.\\nTRANSLATION\\nBezy las kreni av doertagho.\\nPHONETIC\\nBE-zi las KRE-ni av do-ER-ta-gho.\\nThis one is pleased you to have served.\\n------------------------------------------------------------------\\nCONTINUED: (4)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     5.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 5, 'page_label': '6'}, page_content='CONTINUED: (5)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     6.\\n(s3e2sc2-14A_14.mp3)\\nKRAZNYS\\nTo win his shield, an Unsullied must go to the slave marts with a \\nsilver mark, find some wailing newborn, and kill it before its \\nmother’s eyes. In this way, we make certain there is no weakness \\nleft in them.\\nTRANSLATION\\nManeragho zya sumby, sydlivas me Dovoghedhy jagho va po buzdari me \\ngelebo dos, umazigho me ruo limari, si zer senagho po leos eji mysa \\nnejo. Vagizi, loduli sko do nagostovave umbilas ez pon.\\nPHONETIC\\nma-NE-ra-gho ZI-a SUM-bi, sid-LI-vas me do-vo-GHE-dhi JA-gho va po \\nbuz-DA-ri me ge-LE-bo-dos, u-MA-zi-gho me RU-o li-MA-ri, si zer SE-\\nna-gho po LE-os E-ji MI-sa NE-jo. va-GI-zi, lo-DU-li sko do na-gos-\\nto-VA-ve UM-bi-las ez pon.\\nTo gain his shield, is to an Unsullied to go to the slave markets a \\nsilver coin with, to come upon a baby crying, and it to kill the \\neyes of the mother before. In this way, we become certain that no \\nweakness will remain with them.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_15.mp3)\\nMISSANDEI\\nShe is offended. She asks if you pay a silver coin to the mother for \\nher dead baby.\\nTRANSLATION\\nLas angoda. Pindas lu ghozzila me gelebo va ji mysa zya ruo murghi \\nzy.\\nPHONETIC\\nlas AN-go-da. PIN-das lu GHOZ-zi-la me ge-LE-bo va ji MI-sa ZI-a RU-\\no MUR-ghi-zi.\\nShe is offended. She asks if you pay a silver coin to the mother her \\nbaby dead for.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_16.mp3)\\nKRAZNYS\\nWhat a soft mewling fool this one is. Tell her the mark is for the \\nbaby’s owner, not the mother.\\nTRANSLATION\\nKuna mitty raba vaovaori bezy! Ivetra zer ji gelebo sa ji marizzo \\neji ruo zy, do ji mysa.\\nCONTINUED: (5)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     6.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 6, 'page_label': '7'}, page_content='CONTINUED: (6)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     7.\\nPHONETIC\\nKU-na MIT-ti RA-ba vao-VAO-ri BE-zi! i-ve-TRA zer ji ge-LE-bo sa ji \\nma-RIZ-zo E-ji RU-o-zi, do ji MI-sa.\\nSuch a fool soft mewling this one! Tell her the silver coin is the \\nowner of the baby for, not the mother.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_17.mp3)\\nMISSANDEI\\nShe asks if an enemy offers the Unsullied freedom...?\\nTRANSLATION\\nPindas lu me qrinunty iruhilas va ji Dovoghedhi ji derve...?\\nPHONETIC\\nPIN-das lu me kri-NUN-ty i-ru-HI-las va ji do-vo-GHE-dhi ji DER-\\nve...?\\nShe asks if an enemy offers to the Unsullied the freedom...?\\n------------------------------------------------------------------\\n(s3e2sc2-14A_18.mp3)\\nKRAZNYS\\nThey would kill him and bring her his head. They have no life \\noutside their duty. They are soldiers, that is all.\\nTRANSLATION\\nZer senizi si zer imazmizi ji borto. Do ezi do glezo ez poja bude \\nhin. Si mintys, dombo.\\nPHONETIC\\nzer SE-ni-zi si zer i-maz-MI-zi ji BOR-to. do E-zi do GLE-zo ez PO-\\nja BU-de hin. si MIN-tis, DOM-bo.\\nHim they would kill and her would bring the head. They don’t have no \\nlife aside their duty from. They are soldiers, no more.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_19.mp3)\\nMISSANDEI\\nShe asks how many Unsullied are for sale.\\nTRANSLATION\\nPindas skoverdi Dovoghedhi lis lerraski.\\nPHONETIC\\nPIN-das sko-VER-di do-vo-GHE-dhi lis ler-RAS-ki.\\nShe asks how many Unsullied are for sale.\\nCONTINUED: (6)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     7.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 7, 'page_label': '8'}, page_content='CONTINUED: (7)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     8.\\n------------------------------------------------------------------\\n(s3e2sc2-14A_20.mp3)\\nKRAZNYS\\nTell the Westerosi whore she has until tomorrow.\\nTRANSLATION\\nIvetra ji live Vesterozia kisa eva vaneqo.\\nPHONETIC\\ni-ve-TRA ji LI-ve ves-te-ro-ZI-a KI-sa E-va va-NE-ko.\\nTell the whore Westerosi it is until tomorrow.\\n------------------------------------------------------------------\\nGAME OF THRONES #306\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 7/31/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. RIVERLANDS - DAY6.5 6.5\\n(s3e6sc6-5_1.mp3)\\nTHOROS\\nValar morghulis.\\nCONTINUED: (7)2.14A 2.14A\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     8.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 8, 'page_label': '9'}, page_content='CONTINUED:6.5 6.5\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     9.\\nTRANSLATION\\nValar morghulis.\\nPHONETIC\\nVA-lar mor-GHUU-lis.\\nAll men must die.\\n-------------------------------------------------------------------\\n(s3e6sc6-5_2.mp3)\\nMELISANDRE\\nValar dohaeris.\\nTRANSLATION\\nValar dohaeris.\\nPHONETIC\\nVA-lar do-HAI-ris.\\nAll men must serve.\\n-------------------------------------------------------------------\\n(s3e6sc6-5_3.mp3)\\nTHOROS\\nI don’t see many priestesses of R’hllor in the Riverlands.\\nTRANSLATION\\nOlvi vokti Rulloro Qelbria undessun daor.\\nPHONETIC\\nOL-vi VOK-ti rul-LO-ro KEL-bri-a un-DES-sun DAOR.\\nMany priestesses of R’hllor in the Riverlands I don’t see.\\n-------------------------------------------------------------------\\n(s3e6sc6-5_4.mp3)\\nMELISANDRE\\nYou are Thoros of Myr.\\nTRANSLATION\\nThoros hen Myrot iksa.\\nPHONETIC\\nTHO-ros hen MYU-rot IK-sa.\\nThoros of Myr you are.\\nCONTINUED:6.5 6.5\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     9.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 9, 'page_label': '10'}, page_content='CONTINUED: (2)6.5 6.5\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     10.\\n-------------------------------------------------------------------\\n(s3e6sc6-5_5.mp3)\\nMELISANDRE\\nThe High Priest gave you a mission. Turn King Robert away from his \\nidols and toward the Lord of Light. What happened?\\nTRANSLATION\\nVoktys Eglie aot gaomilaksir teptas: Roberti Dari zyhi nekepti se \\nAeksiot Oño jemagon. Skorion massitas?\\nPHONETIC\\nVOK-tis EG-li-e A-oot gao-mi-LAK-sir TEP-tas: ro-BER-ti DA-ri ZYI-hi \\nne-KEP-ti se AIK-si-ot OON-yo je-MA-gon. SKO-ri-on MAS-si-tas?\\nThe High Priest you a mission gave: Robert the King from his idols \\nto the Lord of Light to lead. What came to be?\\n-------------------------------------------------------------------\\n(s3e6sc6-5_6.mp3)\\nTHOROS\\nI failed.\\nTRANSLATION\\nQringontan.\\nPHONETIC\\nkrin-GOON-tan\\nI failed.\\n-------------------------------------------------------------------\\n(s3e6sc6-5_7.mp3)\\nMELISANDRE\\nYou quit, you mean. The heathens continue to slaughter each other \\nand you continue to get drunk.\\nTRANSLATION\\nAole ruda, numazma issa. Quptyssy pontali johegzi se jomozu.\\nPHONETIC\\na-OO-le RUU-da, nuu-MAAZ-ma IS-sa. qup-TYIS-si poon-TAA-li jo-HEG-zi \\nse jo-MOO-zu.\\nCONTINUED: (2)6.5 6.5\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     10.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 10, 'page_label': '11'}, page_content=\"CONTINUED: (3)6.5 6.5\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     11.\\nYourself you dropped, the meaning is. The heathens themselves \\ncontinue to slaughter and you continue to drink.\\n-------------------------------------------------------------------\\n(s3e6sc6-5_8.mp3)\\nTHOROS\\nYou worship Him your way, and I’ll worship Him mine. Do you speak \\nthe Common Tongue?\\nTRANSLATION\\nAohoso ziry rijibia, se ñuhoso ziry rijibin. Quptenkos Engoso \\nydrassis?\\nPHONETIC\\na-OO-ho-so ZI-ri ri-JII-bi-a, se nyu-HO-so ZI-ri ri-JII-bin. qup-TEN-\\nkos EEN-go-so yii-DRAS-sis?\\nBy yours him you worship, and by mine him I worship. Common Tongue \\ndo you speak?\\n-------------------------------------------------------------------\\nINT. HOLLOW HILL - DAY6.8 6.8\\n(s3e6sc6-8_1.mp3)\\nMELISANDRE\\nThat’s not possible.\\nTRANSLATION\\nKonir sagon kostos daor.\\nPHONETIC\\nKO-nir SA-gon KOS-tos DAOR.\\nThat to be can not.\\n-------------------------------------------------------------------\\n(s3e6sc6-8_2.mp3)\\nTHOROS\\nThe Lord has smiled upon me.\\nTRANSLATION\\nAeksio yne iliritan.\\nCONTINUED: (3)6.5 6.5\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     11.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 11, 'page_label': '12'}, page_content='CONTINUED:6.8 6.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     12.\\nPHONETIC\\nAIK-si-o YI-ne i-LII-ri-tan.\\nThe Lord me has smiled upon.\\n-------------------------------------------------------------------\\n(s3e6sc6-8_3.mp3)\\nMELISANDRE\\nYou should not have these powers.\\nTRANSLATION\\nKesys ondor avy sytilibus daor.\\nPHONETIC\\nKE-sis ON-dor A-vyu syi-ti-LII-bus DAOR.\\nThese powers for you should be not.\\n-------------------------------------------------------------------\\n(s3e6sc6-8_4.mp3)\\nTHOROS\\nI have no powers. I ask the Lord for his favor, and he responds as \\nhe will. You know this.\\nTRANSLATION\\nOndor emon daor. Aeksiot zyhon vaoreznon jepin, se ziksoso udlissis. \\nKesir gimi.\\nPHONETIC\\nON-dor E-mon DAOR. AIK-si-ot ZYI-hon VAO-rez-non JE-pin, se zik-SO-\\nso ud-LIS-sis. KE-sir GII-mi.\\nPowers have I not. The Lord his favor I ask, and by his whim he \\nresponds. This you know.\\n-------------------------------------------------------------------\\nGAME OF THRONES #307\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/18/12\\nKEY:\\nCONTINUED:6.8 6.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     12.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 12, 'page_label': '13'}, page_content='CONTINUED: (2)6.8 6.8\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     13.\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. ESSOS - OVERLOOK - DAY7.1 7.1\\n(s3e7sc7-1_1.mp3)\\nDANY\\nSend a man to the city gates.\\nTRANSLATION\\nVa oktio remyti vale jikas.\\nPHONETIC\\nva OK-ti-o re-MYI-ti VA-le ji-KAS.\\nTo the city’s gates a man send.\\n-------------------------------------------------------------------\\n(s3e7sc7-1_2.mp3)\\nDANY\\nTell the slavers I will receive them here and accept their \\nsurrender. Otherwise, Yunkai will suffer the same fate as Astapor.\\nTRANSLATION\\nBelmurti ivestras kesir ponte jiorinna se pojon obuljarion \\nmazorinna. Lodaor henkos vejose hae Astaprot Yunkai botilza.\\nPHONETIC\\nbel-MUR-ti i-ves-TRAS KE-sir PON-te ji-o-RIN-na se PO-jon o-BUL-ya-\\nri-on ma-zo-RIN-na. lo-DAOR HEN-kos VE-jo-se hai as-ta-PROT YUN-kai \\nbo-TIL-za.\\nThe slavers tell here them I will receive and their surrender \\naccept. If not, by the same fate as Astapor Yunkai will suffer.\\n-------------------------------------------------------------------\\nCONTINUED: (2)6.8 6.8\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     13.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 13, 'page_label': '14'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     14.\\nINT. ROBB’S TENT - NIGHT7.17C 7.17C\\n(s3e7sc7-17C_1.mp3)\\nTALISA\\nHello.\\nTRANSLATION\\nRytsas.\\nPHONETIC\\nRIT-sas.\\nHello.\\n-------------------------------------------------------------------\\n(s3e7sc7-17C_2.mp3)\\nROBB\\nHelloo. (mispronounced)\\nTRANSLATION\\nRistas.\\nPHONETIC\\nRIS-tas.\\nHelloo.\\n-------------------------------------------------------------------\\nINT. DANY’S TENT - CONTINUOUS7.4D 7.4D\\n(s3e7sc7-4D_1.mp3)\\nRAZDAL\\nI ought to slap that insolent mucus-skinned ass-whore...\\nTRANSLATION\\nInkan undagho buna gundjabo jorydrare evi rungo pulgarinko...\\nPHONETIC\\nIN-kan UN-da-gho BU-na gun-DJA-bo jo-ri-DRA-re evi RUN-go pul-ga-RIN-\\nko...\\nI ought to slap that ass-whore insolent with the skin mucous...\\nGAME OF THRONES #308\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 8/20/12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     14.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 14, 'page_label': '15'}, page_content='CONTINUED:7.4D 7.4D\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     15.\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. DANY’S TENT - DAY8.7 8.7\\n(s3e8sc8-7_1.mp3)\\nGREY WORM\\nMy queen, shall this one slice out his tongue for you?\\nTRANSLATION\\nNya dare, beza unehtelas jaa engo ozy?\\nPHONETIC\\nNI-ya DA-re, BE-za u-neh-TE-las JA-a EN-go O-zy?\\nMy queen, this one shall slice out his tongue for you?\\n-------------------------------------------------------------------\\n(s3e8sc8-7_2.mp3)\\nDANY\\nThese men are our guests.\\nTRANSLATION\\nBisi vali ilvyz zentyssy issi.\\nPHONETIC\\nBI-si VA-li IL-viz zen-TIS-si IS-si.\\nThese men our guests are.\\n-------------------------------------------------------------------\\nCONTINUED:7.4D 7.4D\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     15.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 15, 'page_label': '16'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     16.\\nINT. DANY’S TENT - NIGHT\\n(s3e8sc8-20_1.mp3)\\nDANY\\nDrogo said I spoke Dothraki like one born to it. It gave him great \\npride.\\nTRANSLATION\\nZhey Drogo ast me-Dothraki thasho h’anhaan ven anha ray yol mehas. \\nMe azh maan atjakhar.\\nPHONETIC\\nzhey DRO-go ast me-DOTH-ra-ki THA-sho han-ha-AN ven AN-ha ray YOL me-\\nHAS. me AZH ma-AN at-ja-KHAR.\\nDear Drogo said that Dothraki softened to me like I had been born \\nfor it. It gave him pride.\\n-------------------------------------------------------------------\\n(s3e8sc8-20_2.mp3)\\nMISSANDEI\\nAthjahakar.\\nTRANSLATION\\nAthjahakar.\\nPHONETIC\\nath-ja-ha-KAR.\\nPride.\\n-------------------------------------------------------------------\\n(s3e8sc8-20_3.mp3)\\nDANY\\nAth jakhar.\\nTRANSLATION\\nAth jakhar.\\nPHONETIC\\nath ja-KHAR.\\nPride.\\n-------------------------------------------------------------------\\n(s3e8sc8-20_4.mp3)\\nMISSANDEI\\nAth. Ja. Hakar.\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     16.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 16, 'page_label': '17'}, page_content='CONTINUED:\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     17.\\nTRANSLATION\\nAth. Ja. Hakar.\\nPHONETIC\\nath. ja. ha-KAR.\\nPride.\\n-------------------------------------------------------------------\\n(s3e8sc8-20_5.mp3)\\nDANY\\nAthjahakar.\\nTRANSLATION\\nAthjahakar.\\nPHONETIC\\nath-ja-ha-KAR.\\nPride.\\nGAME OF THRONES #309\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/24/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. DANY’S TENT - OUTSIDE YUNKAI - DUSK9.9 9.9\\n(s3e9sc9-9_1.mp3)\\nCONTINUED:\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     17.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 17, 'page_label': '18'}, page_content='CONTINUED:9.9 9.9\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     18.\\nDANY\\nYou command the Unsullied. What do you think?\\nTRANSLATION\\nJentys Dovaogedyro syt iksa. Skoros otapa?\\nPHONETIC\\nJEN-tis do-vao-GE-di-ro-sit IK-sa. SKO-ros o-TA-pa?\\nCommander to the Unsullied you are. What do you think?\\n-------------------------------------------------------------------\\n(s3e9sc9-9_2.mp3)\\nDANY (CONT’D)\\nIf leadership is about anything, it’s about making hard choices.\\nTRANSLATION\\nLo jention mirre numazme eza, iderenna qopsa verdagon issa.\\nPHONETIC\\nlo JEN-ti-on MIR-re nu-MAZ-me E-za, i-de-REN-na KOP-sa VER-da-gon IS-\\nsa.\\nIf leadership any meaning has, choices difficult making it is.\\n-------------------------------------------------------------------\\n(s3e9sc9-9_3.mp3)\\nGREY WORM\\nThis one thinks he is telling the truth.\\nTRANSLATION\\nBezy odhabas sko ydras drejikydho.\\nPHONETIC\\nBE-zi o-DHA-bas sko YI-dras dre-ji-KI-dho.\\nThis one thinks that he speaks truly.\\n-------------------------------------------------------------------\\n(s3e9sc9-9_4.mp3)\\nGREY WORM\\nI think he is telling the truth.\\nTRANSLATION\\nOdhaban sko ydras drejikydho.\\nPHONETIC\\no-DHA-ban sko YI-dras dre-ji-KI-dho.\\nI think that he speaks truly.\\nCONTINUED:9.9 9.9\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     18.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 18, 'page_label': '19'}, page_content='CONTINUED: (2)9.9 9.9\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     19.\\nGAME OF THRONES #310\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 8/29/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. YUNKAI (OUTSIDE CITY WALLS) - BACK OF CITY - NIGHT10.13 10.13\\n(s3e10sc10-36_1.mp3)\\nUNSEEN GUARD\\nWho goes there?\\nTRANSLATION\\nSparo kunir las?\\nPHONETIC\\nSPA-ro KU-nir las?\\nWho there stands?\\n-------------------------------------------------------------------\\nEXT. YUNKAI - CITY WALLS - DAY10.36 10.36\\n(s3e10sc10-36_1.mp3)\\nMISSANDEI\\nThis is Daenerys Targaryen, the Stormborn, the Unburnt, the Queen of \\nthe Seven Kingdoms of Westeros, the Mother of Dragons. It is to her \\nyou owe your freedom.\\nCONTINUED: (2)9.9 9.9\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     19.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 19, 'page_label': '20'}, page_content='CONTINUED:10.36 10.36\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     20.\\nTRANSLATION\\nBizy sa Daenerys Targaryen, Jelmazmo, Dorzalty, Daria Sikudo Daryti \\nVestero, Muña Zaldrizoti. Sa va zer sko enkat jiva derve.\\nPHONETIC\\nBI-zi sa DAI-ne-ris tar-GAR-yen, jel-MAZ-mo, dor-ZAL-ti, DA-ri-a, SI-\\nku-do da-RY-ti VES-te-ro, MUN-ya zal-dri-ZO-ti. Sa va ZER sko EN-kat \\nJI-va DER-ve.\\nThis is Daenerys Targaryen, th Stormborn, the Unburnt, Queen of the \\nSeven Kingdoms of Westeros, the Mother of Dragons. Is to her that \\nyou owe your freedom.\\n------------------------------------------------------------------\\n(s3e10sc10-36_2.mp3)\\nDANY\\nYou do not owe me your freedom. I cannot give it to you. Your \\nfreedom is not mine to give. It belongs to you and you alone. If you \\nwant it back, you must take it for yourselves. Each and every one of \\nyou. \\nTRANSLATION\\nDaervose jevosy yne enkot daor. Jemot ziry tepagon koston daor. \\nDaerves jevys tepagon yne sytilibos daor. Jemele meri sytilibas. Lo \\nziry arli jaelat, jamelo syt ziry mazemagon jemo bevilza. Tolvies \\njemys.\\nPHONETIC\\nDAIR-vo-se je-VO-si YI-ne EN-kot DAOR. JE-mot ZI-ri te-PA-gon KOS-\\nton DAOR. DAIR-ves JE-vis te-PA-gon YI-ne si-ti-LI-bos DAOR. je-ME-\\nle si-ti-LI-bas. lo ZI-ri AR-li JAI-lat, ja-ME-lo-sit ZI-ri ma-ze-MA-\\ngon JE-mo be-VIL-za. TOL-vi-es JE-mis.\\nFreedom your me you owe not. To you it give I can not. Freedom your \\nto give me it is for not. For yourselves alone it is. If it back you \\nwant, yourselves for it to take you must. Each and every one of you.\\n------------------------------------------------------------------\\n(s3e10sc10-36_3.mp3)\\nDANY\\nFly. \\nTRANSLATION\\nSovetes.\\nPHONETIC\\nso-ve-TES\\nFly.\\n------------------------------------------------------------------\\nCONTINUED:10.36 10.36\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     20.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 20, 'page_label': '21'}, page_content='CONTINUED: (2)10.36 10.36\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     21.\\n(s3e10sc10-36_4.mp3)\\nDANY\\nLet me pass. \\nTRANSLATION\\nYnot rebagon.\\nPHONETIC\\nYI-not RE-ba-gon.\\nLet me pass.\\nGAME OF THRONES #305\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/5/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. RIVERLANDS - DAY5.24A 5.24A\\n(s3e5sc5-24A_1.mp3)\\nDANY\\nYou are free men. From this day forward, you will choose your own \\nnames.\\nTRANSLATION\\nJeme vali daeri iksat. Hezir, broza jevi jemele iderebilatas.\\nPHONETIC\\nJE-me VA-li DAI-ri IK-sat. HE-zir, BRO-za JE-vi je-ME-le i-de-re-bi-\\nla-TAS.\\nCONTINUED: (2)10.36 10.36\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     21.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 21, 'page_label': '22'}, page_content=\"CONTINUED:5.24A 5.24A\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     22.\\nYou men free are. From now on, names yours to yourselves you will \\nchoose.\\n-------------------------------------------------------------------\\n(s3e5sc5-24A_2.mp3)\\nDANY\\nYou will tell all your fellow soldiers to do the same.\\nTRANSLATION\\nMentyri idañe jevi ivestrilatas keskydoso gaomagon.\\nPHONETIC\\nMEN-ti-ri i-DAN-ye JE-vi i-ves-tri-la-TAS kes-ki-DO-so GAO-ma-gon.\\nAll soldiers fellow your you will tell the same to do.\\n-------------------------------------------------------------------\\n(s3e5sc5-24A_3.mp3)\\nDANY\\nYou will select your own leader, from amongst your own ranks.\\nTRANSLATION\\nJenti jevi jemele iderebilatas, qogrondo jevo hedry.\\nPHONETIC\\nJEN-ti JE-vi je-ME-le i-de-re-bi-la-TAS, ko-GRON-do JE-vo HE-dri.\\nleader your to yourselves you will select, ranks your from amongst.\\n-------------------------------------------------------------------\\n(s3e5sc5-24A_4.mp3)\\nGREY WORM\\nThis one has the honor.\\nTRANSLATION\\nBezy eza ji rigle.\\nPHONETIC\\nBE-zi E-za ji RI-gle.\\nThis one has the honor.\\n-------------------------------------------------------------------\\n(s3e5sc5-24A_5.mp3)\\nDANY\\nWhat is your name?\\nCONTINUED:5.24A 5.24A\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     22.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 22, 'page_label': '23'}, page_content='CONTINUED: (2)5.24A 5.24A\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     23.\\nTRANSLATION\\nSkoroso jemele broza?\\nPHONETIC\\nsko-RO-so je-ME-le BRO-za?\\nBy what yourself do you name?\\n-------------------------------------------------------------------\\n(s3e5sc5-24A_6.mp3)\\nGREY WORM\\nGrey Worm.\\nTRANSLATION\\nTorgo Nudho.\\nPHONETIC\\nTOR-go NU-dho.\\nWorm Grey.\\n-------------------------------------------------------------------\\n(s3e5sc5-24A_7.mp3)\\nDANY\\nThrow away your slave name. Choose a name that gives you pride: the \\nname your parents gave you, or any other.\\nTRANSLATION\\nGadbag aohe qridrughas. Avy hoskas lue brozi iderebas: muñar aot \\nteptas lue brozi, ia mirre tolie.\\nPHONETIC\\nGAD-bag a-O-he kri-dru-GHAS. A-vi HOS-kas LU-e BRO-zi i-de-re-BAS: \\nMUN-yar a-OT TEP-tas LU-e BRO-zi, ia MIR-re TO-li-e.\\nSlave’s name your discard. To you gives pride kind of name choose: \\n(your) parents you gave kind of name, or some other.\\n-------------------------------------------------------------------\\n(s3e5sc5-24A_8.mp3)\\nGREY WORM\\n“Grey Worm” gives this one pride. It is a lucky name. The name this \\none was born with was cursed. That was the name he had when he was \\ntaken as a slave. But Grey Worm is the name this one had the day \\nDaenerys Stormborn set him free.\\nTRANSLATION\\n“Torgo Nudho” hokas bezy. Sa me broji beri. Ji broji ez bezo sene \\nstas qimbroto. Kuny iles ji broji meles esko mazedhas derari va \\nCONTINUED: (2)5.24A 5.24A\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     23.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 23, 'page_label': '24'}, page_content=\"CONTINUED: (3)5.24A 5.24A\\nTRANSLATION (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     24.\\nbuzdar. Y Torgo Nudho sa ji broji ez bezy eji tovi Daenerys Jelmazmo \\nji teptas ji derve.\\nPHONETIC\\nTOR-go NU-dho HO-kas BE-zi. sa me BRO-ji BE-ri. ji BRO-ji ez BE-zo \\nSE-ne stas kim-BRO-to. KU-ni I-les ji BRO-ji ME-les ES-ko ma-ZE-dhas \\nde-RA-ri va BUZ-dar. yi TOR-go NU-dho sa ji BRO-ji ez BE-zi E-ji TO-\\nvi DE-ne-ris jel-MAZ-mo ji TEP-tas ji DER-ve.\\n“Grey Worm” gives pride to this one. It is a name lucky. The name of \\nthis one’s birth was cursed. That was the name he had when he was \\ntaken for slave. But Grey Worm is the name for this one on the day \\nDaenerys Stormborn him gave the freedom.\\n-------------------------------------------------------------------\\nEXT. ROAD TO YUNKAI - DAY5.24B 5.24B\\n(s3e5sc5-24B_1.mp3)\\nDANY\\nYou did not choose this life.\\nTRANSLATION\\nKeso glaesot idereptot daor.\\nPHONETIC\\nKE-so GLAI-sot i-de-REP-tot DAOR.\\nThis life you chose not.\\n-------------------------------------------------------------------\\n(s3e5sc5-24B_2.mp3)\\nDANY\\nBut you are free men now. And free men make their own choices.\\nTRANSLATION\\nYn daeri vali sir issi. Se daeri vali pontalo syt gaomoti iderebzi.\\nPHONETIC\\nyin DAI-ri VA-li sir IS-si. se DAI-ri VA-li pon-TA-lo-sit GAO-mo-ti \\ni-de-REB-zi.\\nBut free men now you are. And free men themselves for matters \\nchoose.\\n-------------------------------------------------------------------\\n(s3e5sc5-24B_3.mp3)\\nDANY\\nYou will select your own leader, from amongst your own ranks.\\nCONTINUED: (3)5.24A 5.24A\\nTRANSLATION (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     24.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 24, 'page_label': '25'}, page_content='CONTINUED:5.24B 5.24B\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     25.\\nTRANSLATION\\nJenti jevi jemele iderebilatas, qogrondo jevo hedry.\\nPHONETIC\\nJEN-ti JE-vi je-ME-le i-de-re-bi-la-TAS, ko-GRON-do JE-vo HE-dri.\\nleader your to yourselves you will select, ranks your from amongst.\\n-------------------------------------------------------------------\\n(s3e5sc5-24B_4.mp3)\\nDANY\\nWhat is your name?\\nTRANSLATION\\nSkoroso jemele broza?\\nPHONETIC\\nsko-RO-so je-ME-le BRO-za?\\nBy what yourself do you name?\\n-------------------------------------------------------------------\\n(s3e5sc5-24B_5.mp3)\\nGREY WORM\\nGrey Worm.\\nTRANSLATION\\nTorgo Nudho.\\nPHONETIC\\nTOR-go NU-dho.\\nWorm Grey.\\n-------------------------------------------------------------------\\n(s3e5sc5-24B_6.mp3)\\nDANY\\nFrom this day forward, you will choose your own names. You will tell \\nall your fellow soldiers to do the same.\\nTRANSLATION\\nHezir, broza jevi jemele iderebilatas. Mentyri idañe jevi \\nivestrilatas keskydoso gaomagon.\\nPHONETIC\\nHE-zir, BRO-za JE-vi je-ME-le i-de-re-bi-la-TAS. MEN-ti-ri i-DAN-ye \\nJE-vi i-ves-tri-la-TAS kes-ki-DO-so GAO-ma-gon.\\nFrom now on, names yours to yourselves you will choose. All soldiers \\nfellow your you will tell the same to do.\\nCONTINUED:5.24B 5.24B\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     25.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 25, 'page_label': '26'}, page_content='CONTINUED: (2)5.24B 5.24B\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     26.\\n-------------------------------------------------------------------\\n(s3e5sc5-24B_7.mp3)\\nDANY\\nThrow away your slave name. Choose the name your parents gave you, \\nor any other. A name that gives you pride.\\nTRANSLATION\\nGadbag aohe qridrughas. Muñar aot teptas lue brozi, ia mirre tolie \\niderebas. Avy hoskas lue brozi.\\nPHONETIC\\nGAD-bag a-O-he kri-dru-GHAS. MUN-yar a-OT TEP-tas LU-e BRO-zi, ia \\nMIR-re TO-li-e i-de-re-BAS.  A-vi HOS-kas LU-e BRO-zi.\\nSlave’s name your discard. (Your) parents you gave kind of name, or \\nsome other choose. To you gives pride kind of name.\\n-------------------------------------------------------------------\\nGAME OF THRONES #304\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 8/16/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. PLAZA OF PRIDE - DAY4.22 4.22\\n(s3e4sc4-22_1.mp3)\\nCONTINUED: (2)5.24B 5.24B\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     26.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 26, 'page_label': '27'}, page_content='CONTINUED:4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     27.\\nKRAZNYS\\nThey have not been tested. The slut had better blood them soon. \\nThere are many small cities from here to Yunkai. All the plunder \\nwill be hers; the Unsullied care nothing for gold. If she takes any \\nslaves, the Masters will take the healthy ones--and will pay well. \\nWho knows? Maybe in ten years some of them will be Unsullied \\nthemselves. Thus all shall prosper.\\nTRANSLATION\\nDo hundaski mazmagho engrari. Ji rene sydlivas pon zoldagho adhiri. \\nLis kara lintori hin kizir va Junkai. Po ghraji uni kisi zya; po \\nDovoghedhi do ezi do jini va ji stizzy. Lu mazmilas angepo buzdari, \\np’Aeske jerozlivis po rysti--si ghozzozlivis syri. Sparo gimis? \\nKotaso vaduli ampa jedhari angez pontal sozlivis Dovoghedhi. Vagizi \\nuni ubrilezlivis.\\nPHONETIC\\ndo HUN-das-ki maz-MA-gho en-GRA-ri. ji RE-ne sid-LI-vas pon ZOL-da-\\ngho a-DHI-ri. lis KA-ra LIN-to-ri hin KI-zir va jun-KAI. po GHRA-ji \\nU-ni KI-si ZI-a; po do-vo-GHE-dhi do E-zi do JI-ni va ji STIZ-zi. lu \\nmaz-MI-las AN-ge-po buz-DA-ri, PAIS-ke je-roz-LI-vis po RIS-ti--si \\nghoz-zoz-LI-vis SI-ri. SPA-ro GI-mis? ko-ta-SO va-DU-li AM-pa JE-dha-\\nri AN-gez pon-TAL soz-LI-vis do-vo-GHE-dhi. va-GI-zi U-ni u-bri-lez-\\nLI-vis.\\nNo were seen to get testing. The slut must them blood quickly. There \\nare many small cities from here to Yunkai. If she will take any \\nslaves, the Masters will accept the healthy (ones)--and pay well. \\nWho knows? Maybe after ten years some of themselves will be \\nUnsullied. In this way all will prosper.\\n------------------------------------------------------------------\\n(s3e4sc4-22_2.mp3)\\nMISSANDEI\\nShe asks if it is now done. \\nTRANSLATION\\nPindas lu sa sir tida.\\nPHONETIC\\nPIN-das lu sa sir TI-da.\\nShe asks if it is now done.\\n------------------------------------------------------------------\\n(s3e4sc4-22_3.mp3)\\nKRAZNYS\\nIt is done. She holds the whip. The bitch has her army. \\nCONTINUED:4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     27.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 27, 'page_label': '28'}, page_content='CONTINUED: (2)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     28.\\nTRANSLATION\\nSa tida. Pelos ji qlony. J’aspo eza zya azantyr.\\nPHONETIC\\nsa TI-da. PE-los ji KLO-ni. JAS-po E-za ZI-a a-ZAN-tir.\\nIt’s done. She holds the whip. The bitch has her army.\\n------------------------------------------------------------------\\n(s3e4sc4-22_4.mp3)\\nDANY\\nUnsullied! Forward march! \\nTRANSLATION\\nDovaogedys! Naejot memebatas!\\nPHONETIC\\ndo-vao-GE-dis! NAI-jot me-me-ba-TAS!\\nUnsullied! Forward march!\\n------------------------------------------------------------------\\n(s3e4sc4-22_5.mp3)\\nDANY\\nHalt!\\nTRANSLATION\\nKelitis!\\nPHONETIC\\nke-li-TIS!\\nHalt!\\n------------------------------------------------------------------\\n(s3e4sc4-22_6.mp3)\\nKRAZNYS\\nTell the bitch her beast won’t come. \\nTRANSLATION\\nIvetra j’aspo zya dyni do majis.\\nPHONETIC\\ni-ve-TRA JAS-po ZI-a DI-ni do MA-jis.\\nTell the bitch her beast doesn’t come.\\n------------------------------------------------------------------\\n(s3e4sc4-22_7.mp3)\\nCONTINUED: (2)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     28.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 28, 'page_label': '29'}, page_content='CONTINUED: (3)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     29.\\nDANY\\nA dragon is not a slave. \\nTRANSLATION\\nZaldrizes buzdari iksos daor.\\nPHONETIC\\nzal-DRI-zes buz-DA-ri IK-sos DAOR.\\nA dragon a slave is not.\\n------------------------------------------------------------------\\n(s3e4sc4-22_8.mp3)\\nKRAZNYS\\nYou speak Valyrian? \\nTRANSLATION\\nYdra ji Valyre?\\nPHONETIC\\ni-DRA ji va-LI-re?\\nYou speak the Valyrian?\\n------------------------------------------------------------------\\n(s3e4sc4-22_9.mp3)\\nDANY\\nI am Daenerys Stormborn of the House Targaryen, of the blood of Old \\nValyria. \\nTRANSLATION\\nNyke Daenerys Jelmazmo hen Targario Lentrot, hen Valyrio Uepo anogar \\niksan.\\nPHONETIC\\nNI-ke DAI-ne-ris jel-MAZ-mo hen tar-GA-ri-o LEN-trot, hen va-LI-ri-o \\nu-E-po A-no-gar IK-san.\\nI Daenerys Stormborn from the Targaryen House, from Valyria Old’s \\nblood am.\\n------------------------------------------------------------------\\n(s3e4sc4-22_10.mp3)\\nDANY\\nValyrian is my mother tongue. \\nTRANSLATION\\nValyrio muño engos ñuhys issa.\\nCONTINUED: (3)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     29.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 29, 'page_label': '30'}, page_content='CONTINUED: (4)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     30.\\nPHONETIC\\nva-LI-ri-o MUN-yo EN-gos NYU-his IS-sa.\\nValyrian mother’s tongue my is.\\n------------------------------------------------------------------\\n(s3e4sc4-22_11.mp3)\\nDANY\\nUnsullied! Slay the masters, slay the soldiers, slay every man who \\nholds a whip, but harm no child. Strike the chains off every slave \\nyou see! \\nTRANSLATION\\nDovaogedys! Aeksia ossenatas, menti ossenatas, qiloni pilos lue vale \\ntolvie ossenatas, yn riñe dore odrikatas. Urnet luo buzdaro tolvio \\nbelma pryjatas!\\nPHONETIC\\ndo-vao-GE-dys! AIK-si-a os-se-na-TAS, MEN-ti os-se-na-TAS, qi-LO-ni \\nPI-los LU-e VA-le TOL-vi-e os-se-na-TAS, yin RIN-ye DO-re o-dri-ka-\\nTAS. UR-net LU-o buz-DA-ro TOL-vi-o BEL-ma pri-ja-TAS!\\nUnsullied! The masters slay, the soldiers slay, a whip holds who man \\nevery slay, but child no harm. You see who slave every’s chains \\nsunder!\\n------------------------------------------------------------------\\n(s3e4sc4-22_12.mp3)\\nKRAZNYS\\nI am your master! Kill her! Kill her! \\nTRANSLATION\\nNyk skan jiva aeske! Zer sena! Zer sena!\\nPHONETIC\\nNIK skan JI-va AIS-ke! zer se-NA! zer se-NA!\\nI am your master! Her kill! Her kill!\\n------------------------------------------------------------------\\n(s3e4sc4-22_13.mp3)\\nDANY\\nDracarys. \\nTRANSLATION\\nDrakarys.\\nCONTINUED: (4)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     30.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 30, 'page_label': '31'}, page_content='CONTINUED: (5)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     31.\\nPHONETIC\\ndra-KA-rys.\\nDragon fire.\\n------------------------------------------------------------------\\nEXT. PLAZA OF PRIDE - LATER4.23 4.23\\n(s3e4sc4-23_14.mp3)\\nDANY\\nYou have been slaves all your life. Today I give you freedom.\\nTRANSLATION\\nJevo glaesoti ry buzdari istiat. Kesy tubi jemot daervi tepan.\\nPHONETIC\\nJE-vo GLAI-so-ti-ry buz-DA-ri IS-ti-at. KE-si TU-bi JE-mot DAIR-vi \\nTE-pan.\\nYour lives during slaves you have been. This day you freedom I give.\\n------------------------------------------------------------------\\n(s3e4sc4-23_15.mp3)\\nDANY\\nAny man who wishes to leave may leave, and no one will harm him. I \\ngive you my word. \\nTRANSLATION\\nHenujagon jaelza lua vala mirre henujagon kostas, se daorys ziry \\nodrikilza. Jemot kivio ñuhe tepan.\\nPHONETIC\\nhe-nu-JA-gon JAIL-za LU-a VA-la MIR-re he-nu-JA-gon KOS-tas, se DAO-\\nris ZI-ri o-dri-KIL-za. JE-mot KI-vi-o NYU-he TE-pan.\\nTo leave wants who man any to leave may, and no one him will harm. \\nTo you oath my I give.\\n------------------------------------------------------------------\\n(s3e4sc4-23_16.mp3)\\nDANY\\nWill you fight for me? As free men? \\nTRANSLATION\\nYne sytivilibilat? Hae daero valoti?\\nCONTINUED: (5)4.22 4.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     31.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 31, 'page_label': '32'}, page_content='CONTINUED:4.23 4.23\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     32.\\nPHONETIC\\nYI-ne si-ti-vi-li-BI-lat? hae DAI-ro va-LO-ti?\\nMe will you fight for? As free men?\\n------------------------------------------------------------------\\nGAME OF THRONES #303\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 8/29/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. SLAVERS GUILD - DAY3.17 3.17\\n(s3e3sc3-17_1.mp3)\\nMISSANDEI\\nShe wants to buy them all.\\nTRANSLATION\\nEbas pon sindigho uni.\\nPHONETIC\\nE-bas pon SIN-di-gho U-ni.\\nShe wants them to buy all.\\n------------------------------------------------------------------\\n(s3e3sc3-17_2.mp3)\\nCONTINUED:4.23 4.23\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     32.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 32, 'page_label': '33'}, page_content='CONTINUED:3.17 3.17\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     33.\\nKRAZNYS\\nShe can’t afford them. The slut thinks she can flash her tits and \\nmake us give her whatever she wants. \\nTRANSLATION\\nDo kotas pon oregho. Ji rene odhabas sko kotilas kimagho po bembemi \\nse ilo ruhilas ji tebagho tulikor ebilas.\\nPHONETIC\\ndo KO-tas pon o-RE-gho. ji RE-ne o-DHA-bas sko KO-ti-las ki-MA-gho \\npo BEM-be-mi se I-lo ru-HI-las ji te-BA-gho TU-li-kor e-BI-las.\\nNo can them afford. The slut thinks that she can flash the tits and \\nus make to her give whatever she wants.\\n------------------------------------------------------------------\\n(s3e3sc3-17_3.mp3)\\nMISSANDEI\\nShe says again she wants all of them. She says she also wants all of \\nthe Unsullied in training. \\nTRANSLATION\\nIvetras tuli sko pon ebilas uni. Ivetras ebilas sizi po Ginilaros \\nuni.\\nPHONETIC\\ni-VE-tras TU-li sko pon e-BI-las U-ni. i-VE-tras e-BI-las SI-zi po \\ngi-ni-LA-ros U-ni.\\nShe says again that them she wants all. She says she wants also the \\nUnsullied-in-training all.\\n------------------------------------------------------------------\\n(s3e3sc3-17_4.mp3)\\nMASTER SLAVER\\nThey have yet to kill their sucklings, they are half-trained. If \\nthey fail on the battlefield, they will shame Astapor. \\nTRANSLATION\\nDo honezi senagho (ERROR: Should be honeski sentagho) sir poji bive, \\nsi jimeri bodedhi. Lu qringomilis evi ninke eji vilivazma, \\nnarijozlivis j’Astapor.\\nPHONETIC\\ndo HO-ne-zi SE-na-gho sir PO-ji BI-ve, si ji-ME-ri bo-DE-dhi. lu \\nkrin-GO-mi-lis E-vi NIN-ke E-ji vi-li-VAZ-ma, na-ri-joz-LI-vis JAS-\\nta-por.\\nNot have they killed yet their sucklings, they are half trained. if \\nthey fall in the field of the battle, they will shame the Astapor.\\nCONTINUED:3.17 3.17\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     33.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 33, 'page_label': '34'}, page_content='CONTINUED: (2)3.17 3.17\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     34.\\n------------------------------------------------------------------\\n(s3e3sc3-17_5.mp3)\\nKRAZNYS\\nThe slut cannot pay for all this. Her ship will buy her one hundred \\nUnsullied, no more--and this because I like the curve of her ass. \\nWhat is left will buy her ten, I will give her twenty if it stops \\nher ignorant whimpering. Her Dothraki smell of shit, but may be \\nuseful as pig feed. I will give her three for those. So, ask this \\nbeggar queen, how will she pay for the remaining 7,877? And the raw \\nboys as well?\\nTRANSLATION\\nJi rene do kotas ghozzagho ez kizy une. Zyo loghor ji sindezlivas \\ngar Dovoghedhi, domba--si kizy vasko v’uvar ez zya gundja yn hilas. \\nJ’umbor ji sindezlivas ampa, ji tebozlivan lantespa lu klimas zyo \\nvaovaono dovodedho. Zyi Dotraki tuzis ez qrugh, y kotis jagho syri \\nva ruhebor. Ji tebozlivan har va buni. Sizi pinda beza dare espo \\nhoghdozi, skokydho ghozzozlivas va po sigudhi pyrys, jengar si \\nsigudha sigudhespa umbaros? Si po Ginilaros sizi?\\nPHONETIC\\nji RE-ne do KO-tas GHOZ-za-gho ez KI-zy U-ne. ZI-o LO-ghor ji sin-\\ndez-LI-vas gar do-vo-GHE-dhi, DOM-ba--si KI-zi VAS-ko VUV-ar ez ZI-a \\nGUND-ja yin HI-las. JUM-bor ji sin-dez-LI-vas AM-pa, ji te-boz-LI-\\nvan lan-TES-pa lu KLI-mas ZI-yo vao-VAO-no do-vo-DE-dho. ZI-yi DO-\\ntra-ki TU-zis ez krugh, yi KO-tis JA-gho SI-ri va ru-HE-bor. ji te-\\nboz-LI-van HA-ri va BU-ni. SI-zi pin-DA BE-za beggar DA-re ES-po \\nhogh-DO-zi: sko-ki-DHO ghoz-zoz-LI-vas va po SI-gu-dhi PI-ris, JEN-\\ngar si SI-gu-dha si-gu-dhes-PA UM-ba-ros? si po gi-ni-LA-ros SI-zi?\\nThe slut no can pay for this all! Her ship her will buy 100 \\nUnsullied, no more--and this because the curve of her ass me \\npleases. The rest her will buy 10, to her I will give 20 if it stops \\nher whimpering ignorant. Her Dothraki smell of shit, but they can go \\nwell as slop. Her I will give 3 for those. So ask this queen of the \\nbeggars: How will she pay for the 7,877 remaining? And the raw boys \\nalso?\\n------------------------------------------------------------------\\n(s3e3sc3-17_6.mp3)\\nMISSANDEI\\nShe says she will give you a dragon. \\nTRANSLATION\\nIvetras sko o tebozlivas me zaldrize.\\nPHONETIC\\ni-VE-tras sko o te-boz-LI-vas me zal-DRI-ze.\\nShe says that you she will give a dragon.\\nCONTINUED: (2)3.17 3.17\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     34.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 34, 'page_label': '35'}, page_content='CONTINUED: (3)3.17 3.17\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     35.\\n------------------------------------------------------------------\\n(s3e3sc3-17_7.mp3)\\nKRAZNYS\\nA dragon! Did you hear? The little idiot! \\nTRANSLATION\\nMe zaldrize! Ryptat? Ji mittisto!\\nPHONETIC\\nme zal-DRI-ze! RIP-tat? ji mit-TIS-to!\\nA dragon! Did you hear? The little idiot!\\n------------------------------------------------------------------\\n(s3e3sc3-17_8.mp3)\\nMASTER SLAVER\\nIf she offers one, she will give three! Take them all! \\nTRANSLATION\\nLu iruhilas mer, tebozlivas har! Mazma pon uni!\\nPHONETIC\\nlu i-ru-HI-las mer, te-boz-LI-vas har! maz-MA pon U-ni!\\nIf she offers one, she will give three! Take them all!\\n------------------------------------------------------------------\\n(s3e3sc3-17_9.mp3)\\nKRAZNYS\\nTell her we want the biggest one. \\nTRANSLATION\\nIvetra zer ebi ji rovaja.\\nPHONETIC\\ni-ve-TRA zer E-bi ji ro-VA-ja.\\nTell her we want the biggest one.\\n------------------------------------------------------------------\\n(s3e3sc3-17_10.mp3)\\nMISSANDEI\\nShe asks that you give me to her, as a present. She asks that you do \\nthis now. \\nTRANSLATION\\nPindas sko ji yn tebila, va me rudhy. Pindas sko gomila kizi sir.\\nCONTINUED: (3)3.17 3.17\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     35.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 35, 'page_label': '36'}, page_content='CONTINUED: (4)3.17 3.17\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     36.\\nPHONETIC\\nPIN-das sko ji yin te-BI-la, va me RU-dhi. PIN-das sko GO-mi-la KI-\\nzi sir.\\nShe asks that to her me you give, as a present. She asks that you do \\nthis now.\\n------------------------------------------------------------------\\nGAME OF THRONES #401\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 7/14/13\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. REMNANTS OF DANY’S CAMP - DAY1.4 1.4\\n(e401_1.mp3)\\nGREY WORM\\nThe sellsword speaks many words to give him courage. But look at the \\nsweat dripping down his face. He wants to quit. Only pride keeps him \\ngoing.\\nTRANSLATION\\nJ’azdribe ydras kara odri vaghoma ji tebagho ji nedhinkave. Y jorne \\nji herg sko lutlus hin vi nejo. Ebas zer elzigho. Meri vi hozno zer \\nzbajas beo.\\nPHONETIC\\njaz-DRI-be I-dras KA-ra O-dri va-GHO-ma ji te-BA-gho ji ne-dhin-KA-\\nve. i jor-NE ji HERG sko LUT-lus hin vi NE-jo. E-bas zer EL-zi-gho. \\nME-ri vi HOZ-no zer ZBA-jas BE-o.\\nCONTINUED: (4)3.17 3.17\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     36.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 36, 'page_label': '37'}, page_content=\"CONTINUED:1.4 1.4\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     37.\\nThe-sellsword speaks many words for him to give the courage. But \\nlook at the sweat that drips from the face. He wants it to throw \\n(expression). Only the pride him takes upwards (expression).\\n-------------------------------------------------------------------\\n(e401_2.mp3)\\nDAARIO\\nYou like this girl?\\nTRANSLATION\\nAv hilas, beza tala?\\nPHONETIC\\nav HI-las, BE-za TA-la?\\n(She) you pleases, this girl?\\n-------------------------------------------------------------------\\n(e401_3.mp3)\\nDAARIO\\nMust be frustrating.\\nTRANSLATION\\nSydlivas av ledagho.\\nPHONETIC\\nsid-LI-vas av le-DA-gho.\\n(It) must you frustrate.\\n-------------------------------------------------------------------\\n(e401_4.mp3)\\nGREY WORM\\nYou are not a smart man, Daario Naharis.\\nTRANSLATION\\nDo ska me vala qana, Daario Naharis.\\nPHONETIC\\nDO ska me VA-la KA-na, DA-ri-o na-HA-ris.\\nNot are you a man smart, Daario Naharis.\\n-------------------------------------------------------------------\\nCONTINUED:1.4 1.4\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     37.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 37, 'page_label': '38'}, page_content='CONTINUED: (2)1.4 1.4\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     38.\\n(e401_5.mp3)\\nDAARIO\\nI’d rather have no brains and two balls.\\nTRANSLATION\\nYn umbas sidri emagho do ribazma y lanta kokosi.\\nPHONETIC\\ni-NUM-bas SI-dri e-MA-gho DO ri-BAZ-ma i LAN-ta ko-KO-si.\\n(It) me suits better to have no brain but two balls.\\n-------------------------------------------------------------------\\nEXT. ROAD TO MEEREEN - DAY1.22 1.22\\n(e401_6.mp3)\\nUNSULLIED\\nHalt! Halt! Halt!\\nTRANSLATION\\nKlimada! Klimada! Klimada!\\nPHONETIC\\nkli-ma-DA! kli-ma-DA! kli-ma-DA!\\nHalt! Halt! Halt!\\nGAME OF THRONES #403\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/25/13\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\nCONTINUED: (2)1.4 1.4\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     38.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 38, 'page_label': '39'}, page_content='CONTINUED:1.22 1.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     39.\\n-------------------------------------------------------------------\\nEXT. MEEREEN - DAY3.27 3.27\\n(e403_1.mp3)\\nGREY WORM\\nAllow me this honor, Mother of Dragons. I will not disappoint you.\\nTRANSLATION\\nYn teba kiza rigle, Mysa espo Zaldrizes. Do vrogozlivan oa greze.\\nPHONETIC\\nin te-BA KI-za RI-gle, MI-sa ES-po zal-DRI-zes. DO vro-goz-LI-van O-\\na GRE-ze.\\nMe give this honor, Mother of Dragons. Not will I drop your train \\n(expression).\\n-------------------------------------------------------------------\\n(e403_2.mp3)\\nDANY\\nYou are the commander of the Unsullied. I cannot risk you.\\nTRANSLATION\\nDovaogedot jentys iksa. Avy hinikagon koston daor.\\nPHONETIC\\ndo-vao-GE-dot JEN-tis IK-sa. A-vi hi-ni-KA-gon KOS-ton DAOR.\\nUnsullied’s commander you are. You to risk I can not.\\n-------------------------------------------------------------------\\nEXT. MEEREEN - DAY3.28 3.28\\n(e403_3.mp3)\\nDANY\\nI am Daenerys Stormborn. Your masters may have told you lies about \\nme, or they may have told you nothing. It does not matter. I have \\nnothing to say to them. I speak only to you.\\nTRANSLATION\\nDaenerys Jelmazmo iksan. Kostilus jevi aeksia yno be pirtra jemot \\nvestretis, ia daoruni jemot vestretis. Daoriot jemas. Doriar udra \\nponto syt eman. Meri jemi ivestran.\\nCONTINUED:1.22 1.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     39.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 39, 'page_label': '40'}, page_content='CONTINUED:3.28 3.28\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     40.\\nPHONETIC\\nDAI-ne-ris jel-MAZ-mo IK-san. KOS-ti-lus JE-vi AEK-si-a I-no-be PIR-\\ntra JE-mot ves-TRE-tis, ya DAO-ru-ni JE-mot ves-TRE-tis. DAO-ri-ot \\nJE-mas. DO-ri-ar UD-ra PON-to-sit E-man. ME-ri JE-mi i-VES-tran.\\nDaenerys Stormborn am I. Perhaps your masters about me lies to you \\nhave told, or nothing to you have told. Nowhere it leads. \\n(expression) No words for them have I. Only to you I speak.\\n-------------------------------------------------------------------\\n(e403_4.mp3)\\nDANY\\nFirst, I went to Astapor. Those who were slaves in Astapor now stand \\nbehind me, free. Next I went to Yunkai. Those who were slaves in \\nYunkai now stand behind me, free. (beat) Now I have come to Meereen.\\nTRANSLATION\\nEli Astaprot istan. Astaprot dohaertrossa sir yno inkot iorzi, \\ndaeri. Hembar Yunkaihot istan. Yunkaihi dohaertrossa sir yno inkot \\niorzi, daeri. (beat) Sesir Mirinot mastan.\\nPHONETIC\\nE-li AS-ta-prot IS-tan. AS-ta-prot do-hair-TROS-sa sir I-no in-kot \\nIOR-zi, DAI-ri. HEM-bar yun-KAI-hot IS-tan. yun-KAI-hi do-hair-TROS-\\nsa sir I-no in-kot IOR-zi, DAI-ri. (beat) SE-sir mi-RI-not MAS-tan.\\nFirst to Astapor I went. In Astapor those who used to be slaves now \\nbehind me stand, free. Thereafter to Yunkai I went. In Yunkai those \\nwho used to be slaves now behind me stand, free. (beat) And now to \\nMeereen I have come.\\n-------------------------------------------------------------------\\n(e403_5.mp3)\\nDANY\\nI am not your enemy. Your enemy is beside you. Your enemy steals and \\nmurders your children. Your enemy has nothing for you but chains and \\nsuffering and commands. (beat) I do not bring you commands. I bring \\nyou a choice. And I bring your enemies what they deserve. (beat) \\nForward!\\nTRANSLATION\\nJevy qrinuntys ikson daor. Jevy qrinuntys jemo paktot issa. Jevy \\nqrinuntys jevor riñar laodissis ossenis. Jevy qrinuntys jemo syt \\nmeri belma se boteri se udrazmi ezi. (beat) Udrazmi jemot maghon \\ndaor. Iderennon maghan. Se jevo qrinuntoti pojor gurotriri maghan. \\n(beat) Naejot!\\nPHONETIC\\nJE-vi kri-NUN-tis IK-son DAOR. JE-vi kri-NUN-tis JE-mo PAK-tot IS-\\nsa. JE-vi kri-NUN-tis JE-vor RIN-yar lao-DIS-sis os-se-NIS. JE-vi \\nCONTINUED:3.28 3.28\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     40.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 40, 'page_label': '41'}, page_content=\"CONTINUED: (2)3.28 3.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     41.\\nkri-NUN-tis JE-mo-sit ME-ri BEL-ma se bo-TE-ri se u-DRAZ-mi E-zi. \\n(beat) u-DRAZ-mi JE-mot MA-ghon DAOR. i-de-REN-non MA-ghan. se JE-vo \\nkri-NUN-to-ti PO-jor gu-ro-TRI-ri MA-ghan. (beat) NAE-jot!\\nYour enemy I am not. Your enemy by your side is. Your enemy your \\nchildren abducts and murders. Your enemy for you only chains and \\nsuffering and commands has. (beat) Commands you I bring not. A \\nchoice I bring. And to your enemies their (just) desserts I bring. \\n(beat) Forward!\\n-------------------------------------------------------------------\\n(e403_6.mp3)\\nDANY\\nFire!\\nTRANSLATION\\nNabematas!\\nPHONETIC\\nna-be-ma-TAS!\\nFire!\\n-------------------------------------------------------------------\\n(e403_7.mp3)\\nBACKGROUND\\nI fart in your general direction, son of a window-dresser! Your \\nmother was a hamster, and your father smelt of elderberries!\\nTRANSLATION\\nByjan vavi demble eva o, trezy eme verdje espo jimi! Oa mysa iles me \\nnynyghi, si oa kiba tuziles espo tomistos!\\nPHONETIC\\nBI-jan va-vi DEM-ble e-va O, TRE-zi e-me VER-dje es-po JI-mi! O-a MI-\\nsa I-les me NI-ni-ghi, si O-a KI-ba tu-ZI-les es-po to-MIS-tos!\\nI-fart towards-the area of you, son of-an arrange of-the windows! \\nYour mother was a hamster, and your father smelt from elderberries!\\n-------------------------------------------------------------------\\n(e403_8.mp3)\\nBACKGROUND\\nGo and boil your bottom, son of a silly person!\\nTRANSLATION\\nJá si hojgá oa gundja, trezy eme mero dovodedha!\\nCONTINUED: (2)3.28 3.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     41.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 41, 'page_label': '42'}, page_content='CONTINUED: (3)3.28 3.28\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     42.\\nPHONETIC\\nJA si hoj-GA O-a GUN-dja, TRE-zi E-me ME-ro do-vo-DE-dha!\\nGo and boil your bottom, son of-a person silly!\\n-------------------------------------------------------------------\\n(e403_9.mp3)\\nBACKGROUND\\nI wave my private parts at your aunties!\\nTRANSLATION\\nKiman nya másina orvorta va oi sodjistos!\\nPHONETIC\\nKI-man NI-a MA-si-na or-VOR-ta va O-i so-DJIS-tos!\\nI-flash my things private at your aunties!\\n-------------------------------------------------------------------\\n(e403_10.mp3)\\nBACKGROUND\\nI don’t want to talk to you no more you empty-headed animal food \\ntrough wiper!\\nTRANSLATION\\nDo eban av kimívagho dombo, o doru-borto pame espo gruzi evi havor \\nespo begistos!\\nPHONETIC\\nDO E-ban av ki-MI-va-gho DOM-bo, o DO-ru BOR-to PA-me es-po GRU-zi e-\\nvi HA-vor es-po be-GIS-tos!\\nNo I-want you to-talk no-more, you nothing-head cleaner of the \\ntroughs of the food of animals!\\n-------------------------------------------------------------------\\n(e403_11.mp3)\\nBACKGROUND\\nI blow my nose at you, so-called Dragon Queen, you and all your \\nsilly Westerosi kaniggets!\\nTRANSLATION\\nGhorgan ji pungo va o, nynta Dare espo Zaldrizes, o si une oi \\ndovodedhi, Vesterozi azzzzzantys.\\nPHONETIC\\nGHOR-gan ji PUN-go va O, NIN-ta DA-re es-po zal-DRI-zes, O si U-ne O-\\ni do-vo-DE-dhi ves-te-RO-zi azzzzzzzzz-ZAN-tis. \\nCONTINUED: (3)3.28 3.28\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     42.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 42, 'page_label': '43'}, page_content=\"CONTINUED: (4)3.28 3.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     43.\\nI-blow the nose at you, said Queen of-the Dragons, you and all your \\nsilly, Westerosi kaniggets.\\n-------------------------------------------------------------------\\nGAME OF THRONES #404\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/29/13\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. INT. TENT - DAY4.2 4.2\\n(e404_1.mp3)\\nGREY WORM\\nYou are from the Summer Isles as well?\\nTRANSLATION\\nO la sizi hin Jedre?\\nPHONETIC\\nO la SI-ZI hin JE-dre?\\nYou are also from the Summer Isles?\\n-------------------------------------------------------------------\\n(e404_2.mp3)\\nMISSANDEI\\nSpeak Common. It’s the only way to learn.\\nCONTINUED: (4)3.28 3.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     43.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 43, 'page_label': '44'}, page_content='CONTINUED:4.2 4.2\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     44.\\nTRANSLATION\\nYdra ji Quhtinky. Sa ji meri hov va bezbagho.\\nPHONETIC\\ni-DRA ji quh-TIN-ki. SA ji ME-ri HOV va bez-BA-gho.\\nSpeak the Common. It’s the only way for learning.\\n-------------------------------------------------------------------\\n(e404_3.mp3)\\nGREY WORM\\nWhen did they--\\nTRANSLATION\\nSkuri av laetis--\\nPHONETIC\\nSKU-ri av LAI-tis--\\nWhen you did they take--\\n(NOTE: This is just the whole line. Cut it off where appropriate.)\\n-------------------------------------------------------------------\\n(e404_4.mp3)\\nGREY WORM\\nI want to kill the masters.\\nTRANSLATION\\nEban senagho p’aeske.\\nPHONETIC\\nE-ban SE-na-gho PAIS-ke.\\nI want to kill the-masters.\\n-------------------------------------------------------------------\\nINT. MEEREENESE SLAVE BARRACKS - NIGHT4.7 4.7\\n(e404_5.mp3)\\nYOUNGER SLAVE\\nYou heard her! She said she came to free us!\\nTRANSLATION\\nYel rit! Poghethash mathash wang yel sherwa!\\nCONTINUED:4.2 4.2\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     44.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 44, 'page_label': '45'}, page_content='CONTINUED:4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     45.\\nPHONETIC\\nyel RIT! po-GHE-thash MA-thash wang yel sher-WA!\\nHer (you) heard! (She) said (she) came for us to-save!\\n-------------------------------------------------------------------\\n(e404_6.mp3)\\nELDER SLAVE\\nYou’re a fool. The masters are too strong.\\nTRANSLATION\\nShka ma khurf. P’ashkesh she kraj waov.\\nPHONETIC\\nshka ma KHURF. PASH-kesh she KRAJ WAOV.\\nYou’re a fool. The-masters are strong too much.\\n-------------------------------------------------------------------\\n(e404_7.mp3)\\nYOUNGER SLAVE\\nShe will protect us. She defeated the masters’ champion.\\nTRANSLATION\\nYel mizozliwash. Erntash ye kosh shp’ashkesh.\\nPHONETIC\\nyel mi-zoz-LI-wash. ERN-tash ye KOSH SHPASH-kesh\\nUs (she) will protect. (She) defeated the champion of the masters.\\n-------------------------------------------------------------------\\n(e404_8.mp3)\\nYOUNGER SLAVE (CONT’D)\\nShe has a great army. You want to live the rest of your days in \\nchains?\\nTRANSLATION\\nEz m’azanchil kraz. Evath khiofa w’umvol es yiv towish filma thosh?\\nPHONETIC\\nEZ ma-ZAN-chil KRAZ. EV-ath khi-o-FA WUM-vol es yiv TO-wish FIL-ma-\\nthosh?\\n(She) has an-army great. (You) want to live the-remainder of your \\ndays chains-by? \\nCONTINUED:4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     45.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 45, 'page_label': '46'}, page_content='CONTINUED: (2)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     46.\\n-------------------------------------------------------------------\\n(e404_9.mp3)\\nELDER SLAVE #2\\nI want to live. You saw what they did to those children. What do you \\nthink they’ll do to us?\\nTRANSLATION\\nEvang khiofa. Wandh shkul khonchish ya fun aj. Shkul odhav \\nkhomozliwish ya yelong?\\nPHONETIC\\nE-vang khi-o-FA. WANDH shkul KHON-chish ya fun AJ. SKHUL o-DHAV kho-\\nmoz-LI-wish ya ye-LONG?\\n(I) want to live. (You) saw what (they) did to those children. What \\n(you) think (they) will-do to us?\\n-------------------------------------------------------------------\\n(e404_10.mp3)\\nELDER SLAVE\\nI’ve been through two slave revolts, boy. They always end the same \\nway: the masters in power and the slaves dead.\\nTRANSLATION\\nOnyeshkh khiofetha ya lant yornazma, tow. Thoghrish porjil ye ow \\nshenk: p’ashkesh pilush ye qlony me pa puzdhal she mul.\\nPHONETIC\\nON-yeshkh khi-o-fe-THA ya LANT yor-NAZ-ma, TOW. THO-grish por-JIL ye \\nOW SHENK: PASH-kesh PI-lush ye QLONY me pa puz-DHAL she MUL.\\n(I) have lived through two slave-revolts, boy. (They) end always the \\nway same: the-masters hold the whip and the slaves are dead.\\n-------------------------------------------------------------------\\n(e404_11.mp3)\\nGREY WORM (O.S.)\\nAll men must die.\\nTRANSLATION\\nValar morghulis.\\nPHONETIC\\nVA-lar mor-GHU-lis.\\nAll men must die.\\nCONTINUED: (2)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     46.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 46, 'page_label': '47'}, page_content='CONTINUED: (3)4.7 4.7\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     47.\\n-------------------------------------------------------------------\\n(e404_12.mp3)\\nGREY WORM\\nBut I promise you: a single day of freedom is worth more than a \\nlifetime in chains.\\nTRANSLATION\\nY dinan kizy ez jim: meri tovi eji derve sa mubyhta hime glezor espo \\nbilma.\\nPHONETIC\\ni DI-nan KI-zi ez JIM: ME-ri TO-vi e-ji DER-ve sa mu-BIH-ta HI-me \\nGLE-zor es-po BIL-ma.\\nBut I put this with you (expression): One day of-the freedom is more-\\nvaluable than-a lifetime in-the chains.\\n-------------------------------------------------------------------\\n(e404_13.mp3)\\nELDER SLAVE\\nWho are you?\\nTRANSLATION\\nShpal shka?\\nPHONETIC\\nshpal SHKA?\\nWho are you?\\n-------------------------------------------------------------------\\n(e404_14.mp3)\\nGREY WORM\\nThis one is called Grey Worm. I was taken as a baby by the masters \\nof Astapor, raised and trained as Unsullied. Now I fight for \\nDaenerys, the mother of dragons and breaker of chains.\\nTRANSLATION\\nJi broji ez bezy sa Torgo Nudho. Mazedhan lodhiri hime bive p’aeske \\nej’Astapor dos, grudvari me bodmari he Dovoghedhy. Sir ozvilivan \\nDaenerys zy, ji mysa espo zaldrizes me ji pryjage espo bilma.\\nPHONETIC\\nji BRO-ji ez BE-zi sa TOR-go NU-dho. ma-ZE-dhan LO-dhi-ri hi-me BI-\\nve PAIS-ke e-jas-ta-POR dos, grud-VA-ri me bod-MA-ri he do-vo-GHE-\\ndhi. SIR oz-vi-LI-van DAI-ne-ris-zi, ji MI-sa es-po zal-DRI-zes me \\nCONTINUED: (3)4.7 4.7\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     47.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 47, 'page_label': '48'}, page_content=\"CONTINUED: (4)4.7 4.7\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     48.\\nji pri-JA-ge es-po BIL-ma.\\nThe name of this one is Worm Grey. I was taken as-a baby the masters \\nof-the-Astapor by, raised and trained as Unsullied. Now I fight \\nDaenerys for, the mother of the dragons and the breaker of the \\nchains.\\n-------------------------------------------------------------------\\n(e404_15.mp3)\\nELDER SLAVE\\nYou are Unsullied? They taught you how to fight before you could \\nwalk. We are not soldiers!\\nTRANSLATION\\nShka Thowoa? A fojej ozwiliwa nyeshk koshil fendha. Tha shke \\nminchish!\\nPHONETIC\\nSHKA tho-wo-A? a fo-JEJ oz-wi-LI-wa nyeshk ko-SHIL fen-DHA. THASH-ke \\nMIN-chish!\\n(You) are Unsullied? You (they) taught to fight before-that (you) \\ncould walk. Not are (we) soldiers!\\n-------------------------------------------------------------------\\n(e404_16.mp3)\\nELDER SLAVE #2\\nWe have no training, no weapons--\\nTRANSLATION\\nYem thol fojeny, thol khemp--\\nPHONETIC\\nyem thol fo-JENY, thol KHEMP.\\n(We) have no training, no weapons.\\n-------------------------------------------------------------------\\n(e404_17.mp3)\\nGREY WORM\\nThere are three slaves in this city for every master.\\nTRANSLATION\\nHonesk hari buzdari ez kizo ohte ez tuve aeske zy.\\nCONTINUED: (4)4.7 4.7\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     48.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 48, 'page_label': '49'}, page_content='CONTINUED: (5)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     49.\\nPHONETIC\\nHO-nesk HA-ri buz-DA-ri ez KI-zo OH-te ez TU-ve AIS-ke-zi.\\nThere are three slaves in this city with-respect-to every master \\nfor.\\n-------------------------------------------------------------------\\n(e404_18.mp3)\\nGREY WORM\\nNo one can give you your freedom, brothers. (beat) If you want it, \\nyou must take it.\\nTRANSLATION\\nDory jim kotas tebagho jiva derve, sombazi. (beat) Lu ji ebat, jimi \\nsydlivas zer mazmagho.\\nPHONETIC\\nDO-ri jim KO-tas te-BA-gho DER-ve, som-BA-zi. (beat) lu ji E-bat, JI-\\nmi sid-LI-vas zer maz-MA-gho.\\nNo one to-you can give your freedom, brothers. (beat) If it you \\nwant, you must it take.\\n-------------------------------------------------------------------\\nEXT. MEEREEN STREET - NIGHT4.8 4.8\\n(e404_19.mp3)\\nSLAVE MASTER\\nWhat is this filth?\\nTRANSLATION\\nShkur’sa kish’qrugh?\\nPHONETIC\\nSHKUR-sa kish-KRUGH?\\nWhat’s this filth?\\n-------------------------------------------------------------------\\nNEW SCENE4.9 4.9\\n(Dany_404_20.mp3)\\nDANY\\nOne hundred and sixty-three.\\nCONTINUED: (5)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     49.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 49, 'page_label': '50'}, page_content='CONTINUED:4.9 4.9\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     50.\\nTRANSLATION\\nGar hari byrepsa.\\nPHONETIC\\nGAR HA-ri bi-rep-SA.\\nOne-hundred three and-sixty.\\n-------------------------------------------------------------------\\nGAME OF THRONES #405\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 7/16/13\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. MEEREEN - DANY’S PENTHOUSE - DAY5.8 5.8\\n(e405_1.mp3)\\nMISSANDEI\\nDo not waste your Queen’s time.\\nTRANSLATION\\nQerga do ji jedha ez oa Dare.\\nPHONETIC\\nqer-ga-DO ji JE-dha ez O-a DA-re.\\nWaste not the time of your Queen.\\n-------------------------------------------------------------------\\nCONTINUED:4.9 4.9\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     50.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 50, 'page_label': '51'}, page_content='CONTINUED:5.8 5.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     51.\\n(e405_2.mp3)\\nMISSANDEI (CONT’D)\\nShe is not here for your language practice.\\nTRANSLATION\\nDo las kizir vurnino oo gureno ej’engo.\\nPHONETIC\\nDO las KI-zir vur-NI-no O-o gu-RE-no e-JEN-go.\\nNot is she here for your practice of language.\\n-------------------------------------------------------------------\\n(e405_3.mp3)\\nGREY WORM\\nThe city is under your control, my Queen.\\nTRANSLATION\\nV’ohte las oi leos go, nya Dare.\\nPHONETIC\\nVOH-te las O-i LE-os-go, NI-a DA-re.\\nThe-city is your eyes under (expression), my Queen.\\n-------------------------------------------------------------------\\n(e405_4.mp3)\\nDANY\\nThank you.\\nTRANSLATION\\nKirimvose.\\nPHONETIC\\nki-RIM-vo-se.\\nThank you.\\n-------------------------------------------------------------------\\n(e405_5.mp3)\\nDANY\\nGo on.\\nTRANSLATION\\nToli ydra.\\nCONTINUED:5.8 5.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     51.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 51, 'page_label': '52'}, page_content='CONTINUED: (2)5.8 5.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     52.\\nPHONETIC\\nTO-li i-DRA.\\nFurther speak.\\n-------------------------------------------------------------------\\n(e405_6.mp3)\\nGREY WORM\\nThere have been killings. Retribution killings.\\nTRANSLATION\\nHundask senna. Senna evi tuinno.\\nPHONETIC\\nHUN-dask SEN-na. SEN-na E-vi tu-IN-no.\\nThere have been killings. Killings of retribution.\\nGAME OF THRONES #406\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 8/5/12\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. GREAT PYRAMID OF MEEREEN - AUDIENCE HALL - DAY6.3 6.3\\n(Missandei_406_1.mp3)\\nMISSANDEI\\nYou stand before Daenerys Stormborn of the House Targaryen, the \\nFirst of Her Name, the Unburnt, the Queen of Meereen...\\nCONTINUED: (2)5.8 5.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     52.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 52, 'page_label': '53'}, page_content='CONTINUED:6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     53.\\nTRANSLATION\\nDaenerys Jelmazmo ao naejot demas, hen Lentrot Targarien, Zyho \\nBrozio Elos, Dorzalty, Mirino Daria...\\nPHONETIC\\nDAI-ne-ris jel-MAZ-mo A-o NAI-jot DE-mas, hen LEN-trot tar-GAR-yen, \\nZI-ho BRO-zi-o E-los, dor-ZAL-ti, mi-RI-no DA-ri-a...\\nDaenerys Stormborn you before sits, from the House Targaryen, Her \\nName’s First, the Unburnt, the Queen of Meereen...\\n-------------------------------------------------------------------\\n(Missandei_406_2.mp3)\\nMISSANDEI\\nQueen of the Andals and the First Men, Khaleesi of the Great Grass \\nSea, Breaker of Chains and Mother of Dragons.\\nTRANSLATION\\nAndalot se Elio Valot Daria, hen Parmenko Embazma Khaleesi, Belmot \\nPryjatys se Muña Zaldrizoti.\\nPHONETIC\\nan-DA-lot se E-li-o VA-lot DA-ri-a, hen par-MEN-ko em-BAZ-ma KHA-le-\\ne-si, BEL-mot pri-JA-tis se MUN-ya zal-DRI-zo-ti.\\nAndals’ and First Men’s Queen, of the Grass Great Sea Khaleesi, \\nChains’ Breaker and Mother of Dragons.\\n-------------------------------------------------------------------\\n(e406_3.mp3)\\nDANY\\nDon’t be afraid, my friend. What would you ask of me?\\nTRANSLATION\\nZugagon daor, ñuhys raqiros. Skoros ynot epilu?\\nPHONETIC\\nZU-ga-gon DAOR, NYU-his ra-KI-ros. SKO-ros I-not e-PI-lu?\\nFear not, my friend. What of me would you ask?\\n-------------------------------------------------------------------\\n(e406_4.mp3)\\nGOATHERD\\nForgive me, your grace. I don’t understand.\\nCONTINUED:6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     53.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 53, 'page_label': '54'}, page_content='CONTINUED: (2)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     54.\\nTRANSLATION\\nYeng shijetra, osh eghlish. Tha shifang.\\nPHONETIC\\nyeng shi-JET-ra, osh EGH-lish. tha-SHI-fang.\\nMe forgive, your grace. I don’t understand.\\n-------------------------------------------------------------------\\n(e406_5.mp3)\\nMISSANDEI\\nThe Queen says you may approach and speak.\\nTRANSLATION\\nYe Thal poghash koth nyesha she yedhra.\\nPHONETIC\\nye THAL PO-ghash koth nye-SHA she YEDH-ra.\\nThe Queen says (you) can approach and speak.\\n-------------------------------------------------------------------\\n(e406_6.mp3)\\nMISSANDEI\\nI am a simple goatherd, my queen. I prayed for your victory against \\nthe slave masters.\\nTRANSLATION\\nShkang ma figh pragh shpa uvresh, nya Thal. Weghnyethang wa’riny ya \\np’ashkesh.\\nPHONETIC\\nSHKANG ma FIGH PRAGH shpa UV-resh, nya THAL. wegh-NYE-thang wa-RINY-\\nya PASH-kesh.\\nI am a simple keeper of-the goats, my Queen. I prayed your-victory \\nagainst the slave-masters.\\n-------------------------------------------------------------------\\n(e406_7.mp3)\\nMISSANDEI\\nQueen Daenerys thanks you.\\nTRANSLATION\\nThal Daenerys a krimwash.\\nCONTINUED: (2)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     54.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 54, 'page_label': '55'}, page_content='CONTINUED: (3)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     55.\\nPHONETIC\\nTHAL DAI-ne-ris a KRIM-wash.\\nQueen Daenerys you thanks.\\n-------------------------------------------------------------------\\n(e406_8.mp3)\\nGOATHERD\\nIt was the dragons, my queen. Your dragons. They came this morning \\nfor my flock.\\nTRANSLATION\\nShtash pa saldhrijesh, nya Thal. We saldhrijesh. Mashish kiz nyekh \\nwa nya qlof.\\nPHONETIC\\nSHTASH pa sal-DHRI-jesh, nya THAL. WE sal-DHRI-jesh. MA-shish kiz \\nNYEKH wa nya QLOF.\\n(It) was the dragons, my Queen. Your dragons. They came this morning \\nto my flock.\\n-------------------------------------------------------------------\\n(e406_9.mp3)\\nGOATHERD\\nI hope I have not offended your grace, but... Now I have nothing...\\nTRANSLATION\\nYelang th’anghothang osh eghlish, ye... Shil yemang thol...\\nPHONETIC\\nYE-lang THAN-gho-thang osh EGH-lish, ye... SHIL YE-mang THOL...\\nI hope not-did I offend your grace, but... Now I have nothing...\\n-------------------------------------------------------------------\\n(e406_10.mp3)\\nMISSANDEI\\nHer Grace Queen Daenerys is truly sorry for your hardship. And while \\nshe cannot bring back your goats, she will see that you are paid \\ntriple for their value.\\nTRANSLATION\\nOsh Eghlish Thal Daenerys majez odhreghal wa meshiv thosh swaghij. \\nShe ro tha kothash tujeva we uvresh shij, a orozliwash she al wa mel \\npoj odhra ye.\\nCONTINUED: (3)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     55.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 55, 'page_label': '56'}, page_content='CONTINUED: (4)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     56.\\nPHONETIC\\nosh EGH-lish THAL DAI-ne-ris ma-JEZ o-dhre-GHAL wa me-SHIV THOSH swa-\\nGHIJ. she ro tha KO-thash tu-je-VA we UV-resh SHIJ, a o-roz-LI-wash \\nshe AL wa MEL poj O-dhra-ye.\\nHer Grace Queen Daenerys is hurt your hardship by truly. And if not \\ncan she return your goats even, to-you she will pay as three to one \\ntheir value for.\\n-------------------------------------------------------------------\\n(e406_11.mp3)\\nGOATHERD\\nThank you, your Grace. Thank you, thank you--\\nTRANSLATION\\nKrimwa, osh Eghlish. Krimwa, krimwa...\\nPHONETIC\\nKRIM-wa, osh EGH-lish. KRIM-wa, KRIM-wa...\\nThank you, your grace. Thank you, thank you...\\n-------------------------------------------------------------------\\n(e406_12.mp3)\\nMISSANDEI\\nThe Queen bids you rise.\\nTRANSLATION\\nYe Thal a pindhash yora.\\nPHONETIC\\nye THAL a PIN-dhash YO-ra.\\nThe Queen you asks to rise.\\n-------------------------------------------------------------------\\n(e406_13.mp3)\\nMISSANDEI\\nShe asks you to look at her.\\nTRANSLATION\\nA yel pindhash yornye.\\nCONTINUED: (4)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     56.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 56, 'page_label': '57'}, page_content='CONTINUED: (5)6.3 6.3\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     57.\\nPHONETIC\\na yel PIN-dhash YORN-ye.\\n(She) you her asks to look.\\n-------------------------------------------------------------------\\n(e406_14.mp3)\\nMISSANDEI\\nHer Grace says you have stood face to face with your Queen. Now you \\ncan look any man in the eye without fear.\\nTRANSLATION\\nOsh Eghlish poghash onyeshkh yorta nyej es nyej wa wa Thal. Shil \\nkoth yornya ya ye resh shing anghes wal thol so thosh.\\nPHONETIC\\nosh EGH-lish PO-ghash ON-yeshkh YOR-ta NYE-es NYEJ wa wa THAL. SHIL \\nKOTH YORN-ya ya ye RESH shing AN-ghes WAL THOL SO-thosh.\\nHer Grace says you have stood face with face to your Queen. Now you \\ncan look in the eye from any man no fear with.\\n-------------------------------------------------------------------\\n(e406_15.mp3)\\nMANSERVANT\\nThe noble Hizdahr zo Loraq begs an audience with the Queen.\\nTRANSLATION\\nJi rhedessiarza Hizdahr zo Loraq pindas me jere eji Dare.\\nPHONETIC\\nji re-des-si-AR-za HIZ-dar zo LO-rak PIN-das me JE-re e-ji DA-re.\\nThe noble Hizdahr zo Loraq asks an audience with-the Queen.\\n-------------------------------------------------------------------\\n(e406_16.mp3)\\nDANY\\nThe noble Hizdahr zo Loraq can speak to me himself.\\nTRANSLATION\\nRhedessiarza Hizdahr zo Loraq zirylo syt ynot vestragon kostas.\\nPHONETIC\\nrhe-des-SIAR-za HIZ-dahr zo LO-rak zi-RI-lo-sit I-not VES-tra-gon \\nKOS-tas.\\nCONTINUED: (5)6.3 6.3\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     57.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 57, 'page_label': '58'}, page_content=\"CONTINUED: (6)6.3 6.3\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     58.\\n(The) noble Hizdahr zo Loraq himself for to-me speak can.\\n-------------------------------------------------------------------\\nGAME OF THRONES #408\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 7/16/13\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. AUDIENCE HALL - NIGHT8.14 8.14\\n(e408_1.mp3)\\nGREY WORM\\nI have come to apologize.\\nTRANSLATION\\nHonesk matagho vaghoma nygel ovulegho.\\nPHONETIC\\nHO-nesk MA-ta-gho va-GHO-ma NI-gel o-VU-le-gho.\\nI have come for myself to apologize.\\n-------------------------------------------------------------------\\n(e408_2.mp3)\\nMISSANDEI\\nYou don’t need to apologize.\\nCONTINUED: (6)6.3 6.3\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     58.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 58, 'page_label': '59'}, page_content='CONTINUED:8.14 8.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     59.\\nTRANSLATION\\nDo ima jini va aol ovulegho.\\nPHONETIC\\nDO I-ma JI-ni va AOL o-VU-le-gho.\\n(You) do not have a need for yourself to apologize.\\n-------------------------------------------------------------------\\n(e408_3.mp3)\\nGREY WORM\\nI hope I did not frighten you.\\nTRANSLATION\\nJelan sko do av rudhan zughagho.\\nPHONETIC\\nJE-lan sko do av RU-dhan ZU-gha-gho.\\n(I) hope that not you (I) dropped to fear (expression).\\n-------------------------------------------------------------------\\n(e408_4.mp3)\\nMISSANDEI\\nNo.\\nTRANSLATION\\nDo.\\nPHONETIC\\nDO.\\nNo.\\n-------------------------------------------------------------------\\n(e408_5.mp3)\\nMISSANDEI\\nGrey Worm.\\nTRANSLATION\\nTorgo Nudho.\\nPHONETIC\\nTOR-go NU-dho.\\nWorm Grey.\\nCONTINUED:8.14 8.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     59.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 59, 'page_label': '60'}, page_content='CONTINUED: (2)8.14 8.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     60.\\n-------------------------------------------------------------------\\n(e408_6.mp3)\\nMISSANDEI\\nI’m glad you saw me.\\nTRANSLATION\\nLan kreni sko yn unda.\\nPHONETIC\\nlan KRE-ni sko i-NUN-da.\\nI’m glad that me you saw.\\n-------------------------------------------------------------------\\n(e408_7.mp3)\\nGREY WORM\\nSo am I.\\nTRANSLATION\\nNygel sizi.\\nPHONETIC\\nNI-gel SI-zi.\\nMyself as well.\\nGAME OF THRONES #410\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 09/14/13\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\nCONTINUED: (2)8.14 8.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     60.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 60, 'page_label': '61'}, page_content='CONTINUED: (3)8.14 8.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     61.\\n-------------------------------------------------------------------\\nINT. GREAT PYRAMID - AUDIENCE HALL - DAY10.7 10.7\\n(Missandei_410_1.mp3)\\nMISSANDEI\\nYou stand before Daenerys Stormborn, the Unburnt, Queen of Meereen, \\nQueen of the Andals and the Rhoynar and the First Men...\\nTRANSLATION\\nDaenerys Jelmazmo ao naejot demas, Dorzalty, Mirino Daria, Andalot \\nse Rhoinaro se Elio Valot Daria...\\nPHONETIC\\nDAI-ne-ris jel-MAZ-mo A-o NAI-jot DE-mas, dor-ZAL-ti, mi-RI-no DA-ri-\\na, an-DA-lot se RHOI-na-ro se E-li-o VA-lot DA-ri-a...\\nDaenerys Stormborn you before sits, the Unburnt, Meereen’s Queen, \\nthe Andals’ and Rhoynar’s and First Men’s Queen...\\n-------------------------------------------------------------------\\n(Missandei_410_2.mp3)\\nMISSANDEI (O.S.)\\nKhaleesi of the Great Grass Sea, Breaker of Chains, and Mother of \\nDragons.\\nTRANSLATION\\nHen Parmenko Embazma Khaleesi, Belmot Pryjatys se Muña Zaldrizoti.\\nPHONETIC\\nhen par-MEN-ko em-BAZ-ma KHA-le-e-si, BEL-mot pri-JA-tis se MUN-ya \\nzal-DRI-zo-ti.\\nFrom the Grass Great-Sea Khaleesi, Chains Breaker and Mother of \\nDragons.\\n-------------------------------------------------------------------\\n(e410_3.mp3)\\nDANY\\nYou may approach.\\nTRANSLATION\\nAot vaodekuragon.\\nCONTINUED: (3)8.14 8.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     61.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 61, 'page_label': '62'}, page_content='CONTINUED:10.7 10.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     62.\\nPHONETIC\\nAOT vao-de-ku-RA-gon.\\nYou approach (may).\\n-------------------------------------------------------------------\\n(e410_4.mp3)\\nFENNESZ\\nThank you for seeing me, Your Grace. My name is Fennesz.\\nTRANSLATION\\nYne urnio syt kirimvose avy rytsuran, Aohys Eglivys. Ñ uha brozi \\nFennesz issa.\\nPHONETIC\\nI-ne UR-ni-o-sit ki-RIM-vo-se A-vi rit-SU-ran, a-O-his E-gli-vis. \\nNYU-ha BRO-zi fe-NEZ IS-sa.\\nMe seeing for by gladness you I greet (expression), Your Grace. My \\nname Fennesz is.\\n-------------------------------------------------------------------\\n(e410_5.mp3)\\nGOATHERD\\nI do not understand, my Queen.\\nTRANSLATION\\nTha shifang, nya Thal.\\nPHONETIC\\ntha-SHI-fang, nya THAL.\\nNot do I understand, my Queen.\\n-------------------------------------------------------------------\\n(e410_6.mp3)\\nMISSANDEI\\nThe Queen says you may approach.\\nTRANSLATION\\nYe Thal poghash koth nyesha.\\nPHONETIC\\nye THAL PO-ghash koth nye-SHA.\\nThe Queen says (you) can approach.\\nCONTINUED:10.7 10.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     62.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 62, 'page_label': '63'}, page_content='CONTINUED: (2)10.7 10.7\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     63.\\n-------------------------------------------------------------------\\n(e410_7.mp3)\\nGOATHERD\\nI brought you... he came down from the sky... the black one, the \\nwinged shadow...\\nTRANSLATION\\nA yenjedhang... mathash cha yeng we yedhrol... ye sovla, we shindhol \\nshpa chnyeny...\\nPHONETIC\\na yen-JE-dhang... MA-thash CHA yeng we YE-dhrol... ye SOV-la, we \\nSHIN-dhol shpach-NYENY...\\nTo-you (I) brought... (He) came down from the sky... the black one, \\nthe shadow with wings...\\n-------------------------------------------------------------------\\n(e410_8.mp3)\\nGOATHERD\\nHe came down from the sky, and... and...\\nTRANSLATION\\nMathash cha yeng we yedhrol, she... she...\\nPHONETIC\\nMA-thash CHA yeng we YE-dhrol, she... she...\\n(He) came down from the sky, and... and...\\n-------------------------------------------------------------------\\n(e410_9.mp3)\\nGOATHERD\\nMy girl... My little girl...\\nTRANSLATION\\nNya pikh... Nya pikhisht...\\nPHONETIC\\nnya PIKH... nya pi-KISHT...\\nmy girl... my little-girl...\\n-------------------------------------------------------------------\\nCONTINUED: (2)10.7 10.7\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     63.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 63, 'page_label': '64'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     64.\\nINT. BELOW THE GREAT PYRAMID - DAY10.15 10.15\\n(Dany_410_10.mp3)\\nDANY\\nAnd still no word of Drogon?\\nTRANSLATION\\nSe vasir Drogon undetoks daor?\\nPHONETIC\\nse va-SIR DRO-gon UN-de-toks DAOR?\\nAnd as-yet Drogon has-been-seen not?\\n-------------------------------------------------------------------\\n(e410_11.mp3)\\nGREY WORM\\nSailors saw him flying over the Black Cliffs three days ago, my \\nQueen. Nothing since then.\\nTRANSLATION\\nLuhtys vi ornilis sovegho jao Gavori Zobri hari tovis go, nya Dare. \\nDoru himbar.\\nPHONETIC\\nLUH-tis vi or-NI-lis SO-ve-gho jao ga-VO-ri ZO-bri HA-ri TO-vis GO, \\nNI-a DA-re. DO-ru HIM-bar.\\nSailors him saw to fly over Cliffs Black three days ago, my Queen. \\nNothing from then.\\n-------------------------------------------------------------------\\n(Dany_410_12.mp3)\\nDANY\\nMeet me at the catacombs.\\nTRANSLATION\\nGoviliriot yne imazumbas.\\nPHONETIC\\ngo-vi-LI-ri-ot I-ne i-ma-zum-BAS.\\nAt-the-catacombs me meet.\\nGAME OF THRONES #501\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     64.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 64, 'page_label': '65'}, page_content='CONTINUED:10.15 10.15\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     65.\\nRevised 07/14/14\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. MEEREENESE BROTHEL CHAMBER - DAY1.14 1.14\\n(WHORE_501_1.mp3)\\nWHORE\\nYou want the same? Same as always?\\nTRANSLATION\\nEva we shenk? Shenk sha porjil?\\nPHONETIC\\nE-va we SHENK? SHENK sha por-JIL?\\nYou-want the same? Same like always?\\n-------------------------------------------------------------------\\n(WHITE_RAT_501_2.mp3)\\nWHITE RAT\\nYou... You don’t have to.\\nTRANSLATION\\nTha... Tha a shidhliwash.\\nPHONETIC\\nTHA... THA a shidh-LI-wash.\\nNo... No you have-to.\\n-------------------------------------------------------------------\\nCONTINUED:10.15 10.15\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     65.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 65, 'page_label': '66'}, page_content='CONTINUED:1.14 1.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     66.\\n(WHORE_501_3.mp3)\\nWHORE\\nHabit.\\nTRANSLATION\\nShing pa shishkosh.\\nPHONETIC\\nSHING pa SHISH-kosh.\\nFrom the feet (expression).\\n-------------------------------------------------------------------\\nINT. GREAT PYRAMID - DANY’S PENTHOUSE - DAY1.15 1.15\\n(KEYR_501_4.mp3)\\nKEYR\\nThey do not see us as people, Your Grace.\\nTRANSLATION\\nTha yel ornyej sha merosh, Osh Eghlish.\\nPHONETIC\\nTHA yel orn-YEJ sha ME-rosh, osh EGH-lish.\\nNo us they-see like people, Your Grace.\\n-------------------------------------------------------------------\\n(DANY_501_5.mp3)\\nDANY\\nThen they will have to learn to see things differently.\\nTRANSLATION\\nSepar tolkydoso urnegon gureñagon ponto bevulza.\\nPHONETIC\\nSE-par tol-ki-DO-so UR-ne-gon gu-REN-ya-gon PON-to be-VUL-za.\\nThen differently to-see to-learn they will-have-to.\\n-------------------------------------------------------------------\\nINT. GREAT PYRAMID - UNSULLIED OFFICERS’ CHAMBER - DAY1.16 1.16\\n(MISSANDEI_501_6.mp3)\\nCONTINUED:1.14 1.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     66.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 66, 'page_label': '67'}, page_content='CONTINUED:1.16 1.16\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     67.\\nMISSANDEI\\nGrey Worm.\\nTRANSLATION\\nTorgo Nudho.\\nPHONETIC\\nTOR-go NU-dho.\\nWorm Grey.\\n-------------------------------------------------------------------\\n(GREY_WORM_501_7.mp3)\\nGREY WORM\\nMissandei of Naath.\\nTRANSLATION\\nMissandei hin Naath.\\nPHONETIC\\nmi-SAN-dei hin NATH.\\nMissandei from Naath.\\n-------------------------------------------------------------------\\nGAME OF THRONES #502\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 07/14/14\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nCONTINUED:1.16 1.16\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     67.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 67, 'page_label': '68'}, page_content='GAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     68.\\nINT. MEEREENESE CELL BLOCK - DAY2.29 2.29\\n(SON_OF_THE_HARPY_502_1.mp3)\\nSON OF THE HARPY\\nShe doesn’t belong here. She will never belong here. And no matter \\nhow many of you traitors call her “Mother”, she will never be your \\nmother.\\nTRANSLATION\\nFendhash eje khrej kijil. Fendhozliwash eje khrej kijil porjil. She \\nro wany pa páleghesh ejing yel frojilath “Mhysa”, waval tha kis wa \\nmhysa thol.\\nPHONETIC\\nFEN-dhash e-je KHREJ KI-jil. fen-dhoz-LI-wash e-je KHREJ KI-jil por-\\nJIL. she ro WANY pa PA-le-ghesh e-jing yel fro-JI-lath MI-sa, wa-VAL \\ntha kis wa MI-sa THOL.\\nShe-treads on-the trail (expression) here. She-will-tread on-the \\ntrail here always. And if all the traitors from-you-all her call \\n“Mother”, still not she-will-be your mother never.\\n------------------------------------------------------------------\\nGAME OF THRONES #503\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 02/11/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\nROMAJI\\nfo-NE-ti-ku REN-da-rin\\n-------------------------------------------------------------------\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     68.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 68, 'page_label': '69'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     69.\\nEXT. LONG BRIDGE - VOLANTIS - DUSK3.33 3.33\\n(RED_PRIESTESS_503_1.mp3)\\nRED PRIESTESS\\nLord cast your light upon us...\\nTRANSLATION\\nAeksios aohos oñoso ilon jehikas...\\nPHONETIC\\nAIK-si-os a-O-hos on-YO-so I-lon je-hi-KAS...\\nLord your light on-us cast...\\nROMAJI\\nAI-ku-si-o-su a-OU-ho-su o-NYO-so II-roun je-hi-KAA-su... \\n-------------------------------------------------------------------\\n(RED_PRIESTESS_503_2.mp3)\\nRED PRIESTESS\\n...for the night is dark, and full of terrors!\\nTRANSLATION\\n...kesrio syt bantis zobrie issa se ossyngnoti ledys!\\nPHONETIC\\n...KES-ri-o-sit BAN-tis ZO-bri-e IS-sa se os-SING-no-ti LE-dis!\\n...for-that night dark is and terrors full-of.\\nROMAJI\\n...KE-su-ri-o-su-to BAN-ti-su ZOU-bu-ri-e IS-sa se os-SIIN-gu-no-ti \\nREI-di-su!\\n-------------------------------------------------------------------\\n(CROWD_503_3.mp3)\\nCROWD\\nFor the night is dark and full of terrors!\\nTRANSLATION\\nKesrio syt bantis zobrie issa se ossyngnoti ledys!\\nPHONETIC\\nKES-ri-o-sit BAN-tis ZO-bri-e IS-sa se os-SING-no-ti LE-dis!\\nFor-that night dark is and terrors full of.\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     69.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 69, 'page_label': '70'}, page_content='CONTINUED:3.33 3.33\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     70.\\n-------------------------------------------------------------------\\n(RED_PRIESTESS_503_4.mp3)\\nRED PRIESTESS\\nI was once as you are now.\\nTRANSLATION\\nHae jeme istin.\\nPHONETIC\\nhai JE-me IS-tin.\\nLike you-all I-once-was.\\nROMAJI\\nhai JE-me I-su-ti-no.\\n-------------------------------------------------------------------\\n(RED_PRIESTESS_503_5.mp3)\\nRED PRIESTESS\\nBought and sold, scourged and branded, raped and defiled. They tried \\nto silence my voice, as they try to silence yours. But they cannot. \\nThe Lord of Light hears your voice. He hears the king as he hears \\nthe slave; he hears the Stone Men in their misery...\\nTRANSLATION\\nSindity se liorty, qilonty se ozbarty, qrillaetty se vaogedy. Eleni \\nñuhe jogeltigon sylutis, separ jevon jogeltigon sylussi. Yn kostosy \\ndaor. Aeksio Oño eleni jeve rybis. Darys rybis separ dohaeriros \\nrybis; mundari dorenki rybis...\\nPHONETIC\\nSIN-di-ti se LIOR-ti, ki-LON-ti se oz-BAR-ti, qril-LAIT-ti se vao-GE-\\ndi. e-LE-ni NYU-he jo-GEL-ti-gon si-LU-tis, SE-par JE-von jo-GEL-ti-\\ngon si-LUS-si. in KOS-to-si DAOR. AIK-si-o ON-yo e-LE-ni JE-ve RI-\\nbis. DA-ris RI-bis SE-par do-HAI-ri-ros RI-bis; MUN-da-ri do-REN-ki \\nRI-bis...\\nBought-one and sold-one, whipped-one and branded-one, raped-one and \\ndefiled-one. Music my silence they-tried, just-as yours silence they-\\ntry. But they-can not. Lord of-Light music your hears. King he-hears \\njust-as slave he-hears; in-misery stone-men he-hears...\\nROMAJI\\nSIN-di-ti se LI-O-ru-ti, ki-LON-ti se o-zu-BAA-ru-ti, ku-rir-RAIT-ti \\nse va-o-GEI-di. e-REI-ni NYU-he jo-GE-ru-ti-gon si-RU-ti-su, SE-paa-\\nru JE-von jo-GE-ru-ti-gon si-RUS-si. in KO-su-to-si da-O-ru. AI-ku-\\nsi-o O-nyo e-REI-ni JE-ve RI-bi-su. DAA-ri-su RI-bi-su SE-paa-ru do-\\nHAI-ri-ro-su RI-bi-su; MUN-da-ri do-REN-ki RI-bi-su...\\nCONTINUED:3.33 3.33\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     70.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 70, 'page_label': '71'}, page_content='CONTINUED: (2)3.33 3.33\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     71.\\n-------------------------------------------------------------------\\n(RED_PRIESTESS_503_6.mp3)\\nRED PRIESTESS\\nHe has heard your prayers and answered them. He has sent you a \\nsavior! From the fire she was reborn to remake the world! The Dragon \\nQueen!\\nTRANSLATION\\nJeva jorepna ryptas se ponte udlitas. Jemot kaerinio jittas! Vys \\nverdlios perzomy siglitaks! Daria Zaldrizoti!\\nPHONETIC\\nJE-va jo-REP-na RIP-tas se PON-te ud-LI-tas. JE-mot kai-RI-ni-o JIT-\\ntas! VIS VERD-li-os PER-zo-mi sig-LI-taks! DA-ri-a zal-DRI-zo-ti!\\nYour prayers he-has-heard and them he-has-answered. To-you savior he-\\nhas-sent! World would-remake of-fire she-was-born! Queen of-dragons!\\nROMAJI\\nJE-va jo-RE-pu-na RI-pu-ta-su se POUN-te u-du-RI-ta-su. JE-mo-to kai-\\nRII-ni-o JIT-ta-su! VII-su VE-ru-do-ri-o-su PE-ru-zo-mi si-gu-RI-ta-\\nku-su! DAA-ri-a za-ru-do-RII-zo-ti!\\n-------------------------------------------------------------------\\n(RED_PRIESTESS_503_7.mp3)\\nRED PRIESTESS\\nAll those who die fighting for her shall be reborn in kind, and \\ndeath itself will bend the knee.\\nTRANSLATION\\nTolvi zijo syt vilibari morghuljosy li henkiri sigliliks, se morghon \\njale obulilza.\\nPHONETIC\\nTOL-vi ZI-jo-sit vi-LI-ba-ri mor-GHUL-yo-si li HEN-ki-ri si-GLI-\\nliks, se MOR-ghon JA-le o-bu-LIL-za.\\nAll her for fighting would-die those similarly will-be-reborn, and \\ndeath itself will-kneel.\\nROMAJI\\nTO-ru-vi ZI-jo-si-to vi-RII-ba-ri mo-ru-GU-ryo-si rii HEN-ki-ri si-\\ngu-RI-ri-ku-su, se MO-ru-gon JAA-re o-bu-RI-ru-za.\\n-------------------------------------------------------------------\\n(RED_PRIESTESS_503_8.mp3)\\nCONTINUED: (2)3.33 3.33\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     71.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 71, 'page_label': '72'}, page_content='CONTINUED: (3)3.33 3.33\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     72.\\nRED PRIESTESS\\nHer dragons are fire made flesh, a gift from the Lord of Light. They \\nwill purify nonbelievers by the thousands, burning their sins and \\nflesh away.\\nTRANSLATION\\nVa ñellyrty perzys zyhyz zaldrizesse issi, Aeksio Oño irudy. Pyryrzy \\nnapasirossa vokemilzi, va daorunta ñelli qringaomna pojo zalari.\\nPHONETIC\\nva nyel-LIR-ti PER-zis ZI-hiz zal-dri-ZES-se IS-si, AIK-si-o ON-yo i-\\nRU-di. pi-RIR-zi na-pa-si-ROS-sa vo-ke-MIL-zi, va dao-RUN-ta NYEL-li \\nkrin-gaom-NA PO-jo ZA-la-ri.\\nTo flesh fire her dragons are, Lord Of-Light’s gift. By-the-\\nthousands nonbelievers they-will-purify, to nothing flesh and-sins \\ntheir burning.\\nROMAJI\\nva nyer-RI-ru-ti PE-ru-zi-su ZII-hi-zu za-ru-do-rii-ZES-se IS-si, AI-\\nku-si-o O-nyo i-RU-di. pi-RI-ru-zi na-pa-si-ROS-sa vo-ke-MI-ru-zi, \\nva da-o-RUN-ta NYER-ri ku-rin-ga-on-NAA POU-jo ZAA-ra-ri.\\n-------------------------------------------------------------------\\n(RED_PRIESTESS_503_9.mp3)\\nRED PRIESTESS\\nShe has freed the slaves from their chains and crucified the Masters \\nfor their sins.\\nTRANSLATION\\nDohaerirossa hen pojo belmondo daeredas se hen pojo qringaomnoti \\nAeksia ilinurtas.\\nPHONETIC\\ndo-hai-ri-ROS-sa hen PO-jo bel-MON-do dai-RE-das se hen PO-jo krin-\\nGAOM-no-ti AIK-si-a i-li-NUR-tas.\\nSlaves from their chains she-has-freed and from their sins masters \\nshe-has-crucified.\\nROMAJI\\ndo-hai-ri-ROS-sa hen POU-jo be-ru-MON-do dai-REI-da-su se hen POU-jo \\nku-rin-GA-on-no-ti AI-ku-si-a ii-ri-NU-ru-ta-su.\\n-------------------------------------------------------------------\\nINT. BROTHEL VESTIBULE - DUSK3.34 3.34\\n(DOOR_GUARD_503_10.mp3)\\nCONTINUED: (3)3.33 3.33\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     72.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 72, 'page_label': '73'}, page_content='CONTINUED:3.34 3.34\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     73.\\nDOOR GUARD\\nIt’s good luck to rub a dwarf’s head.\\nTRANSLATION\\nSa beri nidragho ji borto eme krubo.\\nPHONETIC\\nsa BE-ri ni-DRA-gho ji BOR-to E-me KRU-bo.\\nIt’s lucky to rub the head of-a dwarf.\\n------------------------------------------------------------------\\n(RED_PRIESTESS_503_11.mp3)\\nRED PRIESTESS\\nYes.\\nTRANSLATION\\nKessa.\\nPHONETIC\\nKES-sa.\\nYes.\\nROMAJI\\nKES-sa.\\n-------------------------------------------------------------------\\nGAME OF THRONES #504\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 02/06/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nCONTINUED:3.34 3.34\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     73.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 73, 'page_label': '74'}, page_content='CONTINUED: (2)3.34 3.34\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     74.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. DANY’S AUDIENCE CHAMBER - DAY4.28 4.28\\n(KEYR_504_1.mp3)\\nKEYR\\nFor you, Mhysa. You wanted the Harpy dead, but your hands were tied.\\nTRANSLATION\\nA she, Mhysa. Evil shka ye yazdhang mrúlilash, e we wandhosh yelish \\neshpa filma.\\nPHONETIC\\nA-she, MI-sa. e-VIL shka ye YAZ-dhang MRU-li-lash, e we WAN-dhosh YE-\\nlish esh-pa FIL-ma.\\nYou for, Mother. You-wanted that the Harpy die, but yourhands were \\nin chains.\\n-------------------------------------------------------------------\\n(KEYR_504_2.mp3)\\nKEYR (CONT’D)\\nI set you free, as you did all of us.\\nTRANSLATION\\nA chetang ye sherwa, she yela chet.\\nPHONETIC\\na CHE-tang ye SHER-wa, she YE-la CHET.\\nYou I-gave the freedom, like us you-gave.\\n-------------------------------------------------------------------\\n(DANY_504_3.mp3)\\nDANY\\nHe was our prisoner, awaiting trial. You had no right.\\nTRANSLATION\\nIlva ozguroty iles, iderenni jumbare. Drivi mijeta.\\nCONTINUED: (2)3.34 3.34\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     74.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 74, 'page_label': '75'}, page_content='CONTINUED:4.28 4.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     75.\\nPHONETIC\\nIL-va oz-GU-ro-ti I-les, i-de-REN-ni JUM-ba-re. DRI-vi mi-JE-ta.\\nOur prisoner he-was, trial awaiting. The-right you-lacked.\\n-------------------------------------------------------------------\\n(KEYR_504_4.mp3)\\nKEYR\\nHe would rather rip your city apart than see slaves lifted from the \\ndirt.\\nTRANSLATION\\nMel prijashish a och nyeshk oreshish fuzdhal khroj shing we che.\\nPHONETIC\\nMEL pri-JA-shish a OCH nyeshk o-RE-shish fuz-DHAL KHROJ shing we \\nCHE.\\nFirst he-would-raze your city before he-would-see slaves raised from \\nthe dirt.\\n-------------------------------------------------------------------\\n(DANY_504_5.mp3)\\nDANY\\nThere are no more slaves. There are no more masters.\\nTRANSLATION\\nBuzdari hezir ilusy daor. Aeksia hezir ilusy daor.\\nPHONETIC\\nbuz-DA-ri HE-zir i-LU-si DAOR. AIK-si-a HE-zir i-LU-si DAOR.\\nSlaves from-now there-are not. Masters from-now there-are not.\\n-------------------------------------------------------------------\\n(KEYR_504_6.mp3)\\nKEYR\\nThen who lives in the pyramids? Who wears gold masks and murders \\nyour children?\\nTRANSLATION\\nShe shpal shemash pa khamvaj? Shpal nyiwesh pa prokh ej shil she \\nshenash wa aj?\\nCONTINUED:4.28 4.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     75.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 75, 'page_label': '76'}, page_content='CONTINUED: (2)4.28 4.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     76.\\nPHONETIC\\nshe SHPAL SHE-mash pa KHAM-vaj? SHPAL NYI-wesh pa PROKH ej SHIL she \\nSHE-nash wa AJ?\\nThen who lives-in the pyramids? Who wears the masks of gold and \\nmurders your children?\\n-------------------------------------------------------------------\\n(HIZDAHR_504_7.mp3)\\nHIZDAHR\\nNaming every Master an enemy would make it easier for his people, \\nwouldn’t it?\\nTRANSLATION\\nQrinuntomy tolvie Aeksio brozagon gaomon zyho quptyro syt \\nnaqopsemagon issa, kesos daor?\\nPHONETIC\\nkri-NUN-to-mi TOL-vi-e AIK-si-o BRO-za-gon GAO-mon ZI-ho KUP-ti-ro-\\nsit na-kop-se-MA-gon IS-sa, KE-sos DAOR?\\nEnemy every master to-name the-matter his people for to-simplify is, \\nwould-it-be-so not?\\n-------------------------------------------------------------------\\n(KEYR_504_8.mp3)\\nKEYR\\nYour people owned my people. Of course you are my enemy.\\nTRANSLATION\\nWa rush rálilesh nya rush. Swaghij shka nya qrinunch.\\nPHONETIC\\nWA rush RA-li-lesh NYA rush. swa-GHIJ shkan-ya kri-NUNCH.\\nYour people owned my people. Of-course you-are-my enemy.\\n-------------------------------------------------------------------\\n(KEYR_504_9.mp3)\\nKEYR\\nWhen this one came to us, I was the first to take up the sword for \\nyou. I remember the look on my father’s face as I struck down his \\nMaster, who had traded his infant son for a dog.\\nCONTINUED: (2)4.28 4.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     76.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 76, 'page_label': '77'}, page_content='CONTINUED: (3)4.28 4.28\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     77.\\nTRANSLATION\\nEshka fej majidhash wa yel, nyghel khurong nya azanj a ye mel. \\nOdhavang shil ewe nyej es nya kiv eshka qweang ye ashke shka chetash \\nye fiwa trej yosh ye.\\nPHONETIC\\nESH-ka FEJ ma-JI-dhash wa YEL, ny-GHEL KHU-rong nya a-ZANJ A-ye MEL. \\no-DHA-vang SHIL e-we NYEJ es nya KIV esh-ka KWE-ang ye ASH-ke shka \\nCHE-tash ye FI-wa TREJ YOSH-ye.\\nWhen this-one came to us, I raised my sword for you first. I-think \\nnow of-the face of my father when I-laid-low his master who gave his \\ninfant son a-dog for.\\n-------------------------------------------------------------------\\n(KEYR_504_10.mp3)\\nKEYR\\nMy father died in the fighting. If we allow the Sons of the Harpy to \\nreturn us to chains, he never lived.\\nTRANSLATION\\nNya kiv mrultash eshka ozwilíwilesh. Ro rushij pa Trej eshpa \\nYazdhang yel tujeva wa pa filma, khiotash thol.\\nPHONETIC\\nnya KIV MRUL-tash esh-ka oz-wi-LI-wi-lesh. RO RU-shij pa TREJ eshpa \\nYAZ-dhang yel tu-JE-va wa pa FIL-ma, khi-O-tash THOL.\\nMy father died while he-was-fighting. If we-allow the Sons of-the \\nHarpy us to-return to the chains, he-lived never.\\n-------------------------------------------------------------------\\n(HIZDAHR_504_11.mp3)\\nHIZDAHR\\nYou swore that all men would be equal in the Queen’s eyes. Former \\nslaves and former Masters alike. If we can’t trust our Queen, what \\nwill keep us from civil war?\\nTRANSLATION\\nKivyso valar Dario laehot gida kesosy. Dohaertrossa dohaertiarzi \\nhenkiri. Lo ilve darie pasagon kostoty daor, skoros lentot vilibazme \\nkeliemilus?\\nPHONETIC\\nki-VI-so VA-lar DA-ri-o LAI-hot GI-da ke-SO-si. do-hair-TROS-sa do-\\nhair-ti-ar-ZI HEN-ki-ri. LO IL-ve DA-ri-e PA-sa-gon KOS-to-ti DAOR, \\nSKO-ros LEN-tot vi-li-BAZ-me ke-li-e-MI-lus?\\nBy-oath all-men queen’s eyes-under equal would-be. Former-slaves and-\\nCONTINUED: (3)4.28 4.28\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     77.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 77, 'page_label': '78'}, page_content=\"CONTINUED: (4)4.28 4.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     78.\\nformer-masters simultaneously. If our queen trust we-can not, what \\ninternal war would-stop?\\n-------------------------------------------------------------------\\n(DANY_504_12.mp3)\\nDANY\\nThe Harpy’s life was not yours to take. Once, the Masters were the \\nlaw--\\nTRANSLATION\\nGurogon Jazdano glaeson aohon istos daor. Sirgo, Aeksia vettrir ilis-\\n-\\nPHONETIC\\nGU-ro-gon JAZ-da-no GLAI-son a-O-hon IS-tos DAOR. SIR-go, AIK-si-a \\nVET-trir I-lis--\\nTo-take harpy’s life yours was not. Before, masters the-law were--\\n-------------------------------------------------------------------\\n(KEYR_504_13.mp3)\\nKEYR\\nAnd now you are the law.\\nTRANSLATION\\nShe shil ol shka ye fetril.\\nPHONETIC\\nshe shil OL shka she ye FET-ril.\\nAnd now you are the law.\\n-------------------------------------------------------------------\\n(DANY_504_14.mp3)\\nDANY\\nThe law is the law.\\nTRANSLATION\\nVettrir vettrir issa.\\nPHONETIC\\nVET-trir VET-trir IS-sa.\\nThe-law the-law is.\\nCONTINUED: (4)4.28 4.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     78.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 78, 'page_label': '79'}, page_content='CONTINUED: (5)4.28 4.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     79.\\n-------------------------------------------------------------------\\nEXT. MEEREEN PLAZA - DAY4.29 4.29\\n(DANY_504_15.mp3)\\nDANY\\nYou opened your gates to me because I promised you freedom and \\njustice. One cannot exist without the other.\\nTRANSLATION\\nKivyso daervi drivi emiluks vestretan, separ jevi remia ynot \\ndrañedat.  Jahe idañe mijerior tolion sagon kostos daor.\\nPHONETIC\\nki-VI-so DAIR-vi dri-VI e-MI-luks VES-tre-tan, SE-par JE-vi RE-mi-a \\nI-not DRAN-ye-dat. JA-he i-DAN-ye mi-JE-ri-or TO-li-on SA-gon KOS-\\ntos DAOR.\\nBy-oath freedom and-justice you-would-have I-said, therefore your \\ngates to-me you-opened. Its twin lacking the-other to-exist can not.\\n-------------------------------------------------------------------\\n(KEYR_504_16.mp3)\\nKEYR\\nMhysa, please! Forgive me.\\nTRANSLATION\\nMhysa, kotlo! Ing yeliré.\\nPHONETIC\\nMI-sa, KOT-lo! ING ye-li-RE.\\nMother, please! Me forgive.\\n------------------------------------------------------------------\\n(FREEDMEN_504_17.mp3)\\nFREEDMEN\\nBrother! Brother!\\nTRANSLATION\\nSomvash! Somvash!\\nPHONETIC\\nSOM-vash! SOM-vash!\\nBrother! Brother!\\nCONTINUED: (5)4.28 4.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     79.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 79, 'page_label': '80'}, page_content='CONTINUED:4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     80.\\n------------------------------------------------------------------\\n(DANY_504_18.mp3)\\nDANY\\nYou have murdered a citizen of Meereen. The punishment is death.\\nTRANSLATION\\nMirino pasti ossenta. Qilonarion morghon issa.\\nPHONETIC\\nmi-RI-no PAS-ti os-SEN-ta. qi-LO-na-ri-on MOR-ghon IS-sa.\\nMeereen’s citizen you-have-murdered. Punishment death is.\\n-------------------------------------------------------------------\\n(FREEDMEN_504_19.mp3)\\nFREEDMEN\\nMercy!\\nTRANSLATION\\nYe chevelya!\\nPHONETIC\\nye che-VEL-ya!\\nThe mercy!\\n-------------------------------------------------------------------\\n(KEYR_504_20.mp3)\\nKEYR\\nMhysa.\\nTRANSLATION\\nMhysa.\\nPHONETIC\\nMI-sa.\\nMother\\n-------------------------------------------------------------------\\n(DANY_504_22.mp3)\\nDANY\\nA citizen of Meereen was awaiting trial and this man murdered him. \\nThe punishment is death.\\nCONTINUED:4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     80.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 80, 'page_label': '81'}, page_content='CONTINUED: (2)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     81.\\nTRANSLATION\\nMirino pastys iderenni jumbiles se bisa vala ziry ossentas. \\nQilonarion morghon issa.\\nPHONETIC\\nmi-RI-no PAS-tys i-de-REN-ni JUM-bi-les se BI-sa VA-la zi-ri os-SEN-\\ntas. qi-LO-na-ri-on MOR-ghon IS-sa.\\nMeereen’s citizen trial was-awaiting and this man him murdered. \\nPunishment death is.\\n-------------------------------------------------------------------\\nGAME OF THRONES #505\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 02/11/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. MEEREEN - SMALL SQUARE - DAY5.6 5.6\\n(MEEREENESE_WHORE_505_4.mp3)\\nMEEREENESE WHORE\\nSons of the Harpy.\\nTRANSLATION\\nTrej eya yazdhang.\\nCONTINUED: (2)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     81.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 81, 'page_label': '82'}, page_content='CONTINUED:5.6 5.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     82.\\nPHONETIC\\nTREJ e-ya YAZ-dhang.\\nSons of-the Harpy.\\n-------------------------------------------------------------------\\nINT. CASTLE BLACK - LIBRARY - DAY5.12 5.12\\n(SAMWELL_505_3.mp3)\\nSAMWELL\\nIf the light remains in the east, the darkness will rule the west.\\nTRANSLATION\\nLo oños ñaqot umbos, syndror endie jemebilza.\\nPHONETIC\\nLO ON-yos NYA-kot UM-bos, SIN-dror EN-di-e je-me-BIL-za.\\nIf light in-east remains, darkness west will-rule.\\n-------------------------------------------------------------------\\nINT. MEEREEN - DRAGON CHAMBER - NIGHT5.25 5.25\\n(HIZDAHR_505_2.mp3)\\nHIZDAHR\\nValar morghulis.\\nTRANSLATION\\nValar morghulis.\\nPHONETIC\\nVA-lar mor-GHU-lis.\\nAll-men must-die.\\n-------------------------------------------------------------------\\n(MASTER_505_5.mp3)\\nMASTER\\nYou don’t actually mean to do this.\\nTRANSLATION\\nKesir gaomagon drivose aole indio daor.\\nCONTINUED:5.6 5.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     82.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 82, 'page_label': '83'}, page_content='CONTINUED:5.25 5.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     83.\\nPHONETIC\\nKE-sir GAO-ma-gon DRI-vo-se a-O-le IN-di-o DAOR.\\nThis to-do actually yourself intend don’t.\\n-------------------------------------------------------------------\\n(LOOP_505_6.mp3)\\nLOOP\\nRun!\\nTRANSLATION\\nThakhothosh!\\nPHONETIC\\ntha-kho-THOSH!\\nRun!\\n-------------------------------------------------------------------\\n(LOOP_505_7.mp3)\\nLOOP\\nGo!\\nTRANSLATION\\nYathash!\\nPHONETIC\\nya-THASH!\\nGo!\\n-------------------------------------------------------------------\\n(LOOP_505_8.mp3)\\nLOOP\\nFight!\\nTRANSLATION\\nShmonchathash!\\nPHONETIC\\nshmon-cha-THASH!\\nFight!\\n-------------------------------------------------------------------\\nCONTINUED:5.25 5.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     83.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 83, 'page_label': '84'}, page_content='CONTINUED: (2)5.25 5.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     84.\\n(LOOP_505_9.mp3)\\nLOOP\\nSons of the Harpy, attack!\\nTRANSLATION\\nTrej eya Yazdhang, yakhuthush!\\nPHONETIC\\nTREJ e-ya YAZ-dhang, ya-khu-THUSH!\\nSons of-the Harpy, attack!\\n-------------------------------------------------------------------\\n(LOOP_505_10.mp3)\\nLOOP\\nSons of the Harpy are everywhere!\\nTRANSLATION\\nTrej eya Yazdhang lish tholwa!\\nPHONETIC\\nTREJ e-ya YAZ-dhang lish THOL-wa!\\nSons of-the Harpy are everywhere!\\n-------------------------------------------------------------------\\nGAME OF THRONES #506\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 09/03/14\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\nCONTINUED: (2)5.25 5.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     84.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 84, 'page_label': '85'}, page_content='CONTINUED: (3)5.25 5.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     85.\\n-------------------------------------------------------------------\\nINT. GREAT PYRAMID - SMALL CHAMBER - MORNING6.3 6.3\\n(GREY_WORM_506_1.mp3)\\nGREY WORM\\nHow long have I been here?\\nTRANSLATION\\nSkoverda jedha eldan kizir?\\nPHONETIC\\nsko-VER-da JE-dha EL-dan KI-zir?\\nHow-much time I-have-laid here?\\n------------------------------------------------------------------\\n(MISSANDEI_506_2.mp3)\\nMISSANDEI\\nThree days.\\nTRANSLATION\\nHari tovis.\\nPHONETIC\\nHA-ri TO-vis.\\nThree days.\\n------------------------------------------------------------------\\n(GREY_WORM_506_3.mp3)\\nGREY WORM\\nI failed him. I failed my men. I failed my queen.\\nTRANSLATION\\nJi qringontan. Pon qringontan nyi vali. Ji qringontan nya dare.\\nPHONETIC\\nji krin-GON-tan. pon krin-GON-tan NI-i VA-li. ji krin-GON-tan NI-a \\nDA-re.\\nHim I-failed. Them I-failed my men. Her I-failed my queen.\\n------------------------------------------------------------------\\n(MISSANDEI_506_4.mp3)\\nCONTINUED: (3)5.25 5.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     85.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 85, 'page_label': '86'}, page_content='CONTINUED:6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     86.\\nMISSANDEI\\nYou failed no one. You fought bravely. You’ll fight again.\\nTRANSLATION\\nJi qringonta dory. Ozvilihta nedhinkakydho. Ozvilivozliva tuli.\\nPHONETIC\\nji krin-GON-ta DO-ri. oz-vi-LIH-ta ne-DHIN-ka-KI-dho. oz-vi-li-voz-\\nLI-va TU-li.\\nThem you-failed no one. You-fought bravely. You-will-fight again.\\n------------------------------------------------------------------\\n(GREY_WORM_506_5.mp3)\\nGREY WORM\\nWhy are you here? Why aren’t you with the queen?\\nTRANSLATION\\nSkurji la kizir? Skurji do la eji dare?\\nPHONETIC\\nSKUR-ji la KI-zir? SKUR-ji DO la e-ji DA-re?\\nWhy are-you here? Why not are-you with-the queen?\\n------------------------------------------------------------------\\n(MISSANDEI_506_6.mp3)\\nMISSANDEI\\nIt was the queen’s order--\\nTRANSLATION\\nV’odhir dos eji dare sko--\\nPHONETIC\\nVO-dhir-dos e-ji DA-re sko--\\nThe-word-by of-the queen that--\\n------------------------------------------------------------------\\n(GREY_WORM_506_7.mp3)\\nGREY WORM\\nYou should be with her, not sitting uselessly by a wounded man.\\nTRANSLATION\\nInka lagho ez jyl, do nuórrari dovodedhakydho eme vala odreta.\\nCONTINUED:6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     86.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 86, 'page_label': '87'}, page_content='CONTINUED: (2)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     87.\\nPHONETIC\\nIN-ka LA-gho ez JIL, DO nu-OR-ra-ri do-vo-DE-dha-KI-dho e-me VA-la o-\\nDRE-ta.\\nYou-should be with her, not sitting uselessly with-a man wounded.\\n------------------------------------------------------------------\\n(GREY_WORM_506_8.mp3)\\nGREY WORM\\nWhat?\\nTRANSLATION\\nSkure?\\nPHONETIC\\nSKU-re?\\nWhat?\\n------------------------------------------------------------------\\n(MISSANDEI_506_9.mp3)\\nMISSANDEI\\nYou’re wounded but you’re not dead. You are the commander of the \\nUnsullied. (beat) And I haven’t left your side in days, so you’ll \\nspeak to me with respect.\\nTRANSLATION\\nLa odreta y do ska murghi. Ska ji jinty espo Dovoghedhys. (beat) Si \\ndo honesk itagho hin kizir hin kari tovis, sibar yn ydrozliva \\njodahtenuzo.\\nPHONETIC\\nla o-DRE-ta y DO ska MUR-ghi. SKA ji JIN-ti es-po do-vo-GHE-dhis. \\n(beat) si DO HO-nesk i-TA-gho hin KI-zir hin KA-ri TO-vis, SI-bar in \\ni-droz-LI-va jo-dah-te-NU-zo.\\nYou’re wounded but not you-are dead. You-are the commander of-the \\nUnsullied. (beat) And not I-have gone from here from many days, so \\nto-me you-will-speak respectfully. \\n------------------------------------------------------------------\\nEXT. MEEREEN - ALLEY - DAY6.6 6.6\\n(DANY_506_9.mp3)\\nCONTINUED: (2)6.3 6.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     87.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 87, 'page_label': '88'}, page_content='CONTINUED:6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     88.\\nDANY\\nWho were those men?\\nTRANSLATION\\nShparosh shchish fej wal?\\nPHONETIC\\nSHPA-rosh chish FEJ WAL?\\nWho were those men?\\n------------------------------------------------------------------\\n(PEDDLERS_WIFE_506_10.mp3)\\nPEDDLER’S WIFE\\nThey used to fight in the pits. Now they collect the tax.\\nTRANSLATION\\nKhimilish shmonchel espa rev. Shil threwish we yerujil.\\nPHONETIC\\nkhi-MI-lish shmon-CHEL es-pa REV. SHIL THRE-wish we ye-RU-jil.\\nThey-knew fighting in-the pits. Now they-collect the tax.\\n------------------------------------------------------------------\\n(DANY_506_11.mp3)\\nDANY\\nTax? For whom?\\nTRANSLATION\\nWe yerujil? Wa shpal?\\nPHONETIC\\nwe ye-RU-jil? wa SHPAL?\\nThe tax? For whom?\\n------------------------------------------------------------------\\n(PEDDLERS_WIFE_506_12.mp3)\\nPEDDLER’S WIFE\\nFor the men who own this street.\\nTRANSLATION\\nWa pa wal shka ralish kiz khil.\\nCONTINUED:6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     88.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 88, 'page_label': '89'}, page_content='CONTINUED: (2)6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     89.\\nPHONETIC\\nwa pa WAL shka RA-lish kiz KHIL.\\nFor the men that they-own this street.\\n------------------------------------------------------------------\\n(DANY_506_13.mp3)\\nDANY\\nThis street belongs to everyone.\\nTRANSLATION\\nKiz khil mazmash ralal wan thosh.\\nPHONETIC\\nkiz KHIL MAZ-mash RA-lal WAN-thosh.\\nThis street is owned by-everyone.\\n------------------------------------------------------------------\\n(PEDDLER_506_14.mp3)\\nPEDDLER\\nNot anymore.\\nTRANSLATION\\nShil thol.\\nPHONETIC\\nSHIL THOL.\\nNow no-more.\\n------------------------------------------------------------------\\n(DANY_506_15.mp3)\\nDANY\\nBut the queen has said that all--\\nTRANSLATION\\nE ye thal onyeshkh yewetretha shka wan--\\nPHONETIC\\ne ye THAL ON-yeshkh ye-WE-tre-tha shka WAN--\\nBut the queen has said that all--\\n------------------------------------------------------------------\\nCONTINUED: (2)6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     89.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 89, 'page_label': '90'}, page_content='CONTINUED: (3)6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     90.\\n(PEDDLER_506_27.mp3)\\nPEDDLER\\nThe queen let those monsters loose in the first place.\\nTRANSLATION\\nYe thal rudh sowa kij thinyish shing ye yel.\\nPHONETIC\\nye THAL rudh SO-wa kij THIN-yish shing ye YEL.\\nThe queen let to-fly those beasts from the first.\\n------------------------------------------------------------------\\n(MISSANDEI_506_19.mp3)\\nMISSANDEI\\nThe queen set us free.\\nTRANSLATION\\nYe thal yel chetash ye sherwa.\\nPHONETIC\\nye THAL yel CHE-tash ye SHER-wa.\\nThe queen us gave the freedom.\\n------------------------------------------------------------------\\n(PEDDLER_506_20.mp3)\\nPEDDLER\\nThis is freedom? She comes from I don’t know where, she says some \\npretty words, and now this is my life.\\nTRANSLATION\\nKiz sa ye sherwa? Majish shing tha kiming shkul, yewetrash anghes \\nmarta odhre, she shil kiz sa nya klez.\\nPHONETIC\\nKIZ sa ye SHER-wa? MA-jish shing THA KI-ming SHKUL, ye-WE-trash  AN-\\nghes MAR-ta O-dhre, she SHIL KIZ sa nya KLEZ.\\nThis is the freedom? She-comes from no I-know where, she-says some \\npretty words, and now this is my life.\\n------------------------------------------------------------------\\n(PEDDLERS_WIFE_506_21.mp3)\\nCONTINUED: (3)6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     90.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 90, 'page_label': '91'}, page_content='CONTINUED: (4)6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     91.\\nPEDDLER’S WIFE\\nQuiet, you idiot. If her people hear she’ll have you fed to her \\ndragons.\\nTRANSLATION\\nRa righ, khurf. Ro shie merosh riwilish, a tevozliwash sha awol wa \\nshie saldhrijesh.\\nPHONETIC\\nra RIGH, KHURF. ro SHI-e Me-rosh ri-WI-lish, a te-voz-LI-wash sha A-\\nwol wa SHI-e sal-DHRI-jesh.\\nBe quiet, idiot. If her people hear, you she-will-give as food to \\nher dragons.\\n------------------------------------------------------------------\\n(DANY_506_22.mp3)\\nDANY\\nI’m sure she wouldn’t... She’d never...\\nTRANSLATION\\nSwaghij tha komozliwash... Thol komoz...\\nPHONETIC\\nswa-GHIJ tha ko-moz-LI-wash... THOL ko-moz...\\nCertainly not she-would... Never she’d...\\n------------------------------------------------------------------\\nEXT. MEEREEN - ALLEY - DAY6.5 6.5\\n(PEDDLER_506_28.mp3)\\nPEDDLER\\nWe’ve already paid, now leave us in peace.\\nTRANSLATION\\nShil khorej, shil yel tevash ye lighawa.\\nPHONETIC\\nSHIL KHO-rej, shil yel te-VASH ye li-GHA-wa.\\nAlready we-have-paid, now us give the peace.\\n------------------------------------------------------------------\\n(THUG1_506_24.mp3)\\nCONTINUED: (4)6.6 6.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     91.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 91, 'page_label': '92'}, page_content='CONTINUED:6.5 6.5\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     92.\\nTHUG 1\\nThe price just went up, old man.\\nTRANSLATION\\nY’odhra shil onyeshkh shimonta, wev.\\nPHONETIC\\nYODH-ra shil ON-yeshkh shi-mon-TA, WEV.\\nThe-price now has risen, old-man.\\n------------------------------------------------------------------\\n(PEDDLER_506_25.mp3)\\nPEDDLER\\nYou can’t keep pushing us...\\nTRANSLATION\\nTha kotha khima yel rua...\\nPHONETIC\\nTHA KO-tha KHI-ma yel RU-a...\\nNot you-can keep us pushing...\\n------------------------------------------------------------------\\n(PEDDLERS_WIFE_506_26.mp3)\\nPEDDLER’S WIFE\\nStop, no. No!\\nTRANSLATION\\nKlimash, tha. Tha!\\nPHONETIC\\nkli-MASH, THA. THA!\\nStop, no. No!\\n------------------------------------------------------------------\\nGAME OF THRONES #507\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/15/14\\nCONTINUED:6.5 6.5\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     92.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 92, 'page_label': '93'}, page_content='CONTINUED: (2)6.5 6.5\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     93.\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. NORTH KINGSROAD - STANNIS’ ENCAMPMENT - DAY7.13 7.13\\n(WORKER_507_1.mp3)\\nWORKER\\nHand me that spade.\\nTRANSLATION\\nYn tebá kuna amningy.\\nPHONETIC\\nin te-BA KU-na am-NIN-gi.\\nMe hand that spade.\\n-------------------------------------------------------------------\\n(WORKER_507_2.mp3)\\nWORKER\\nCome over here!\\nTRANSLATION\\nMají va kizir!\\nPHONETIC\\nma-JI va KI-zir!\\nCome to here!\\n-------------------------------------------------------------------\\n(WORKER_507_3.mp3)\\nCONTINUED: (2)6.5 6.5\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     93.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 93, 'page_label': '94'}, page_content='CONTINUED:7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     94.\\nWORKER\\nDon’t touch that!\\nTRANSLATION\\nTha riny kuny!\\nPHONETIC\\nTHA RINY kuny!\\nDon’t touch that!\\n-------------------------------------------------------------------\\n(WORKER_507_4.mp3)\\nWORKER\\nYour mother didn’t think so.\\nTRANSLATION\\nA mhysa tha odhaftash wagij.\\nPHONETIC\\na MI-sa tha o-DHAF-tash wa-GIJ.\\nYour mother didn’t think like-that.\\n-------------------------------------------------------------------\\n(WORKER_507_5.mp3)\\nWORKER\\nShut your mouth!\\nTRANSLATION\\nAcchakas hosk!\\nPHONETIC\\nach-cha-kas HOSK!\\nShut dog-mouth!\\n-------------------------------------------------------------------\\n(WORKER_507_6.mp3)\\nWORKER\\nI need water.\\nTRANSLATION\\nIman jini va jedhor.\\nCONTINUED:7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     94.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 94, 'page_label': '95'}, page_content='CONTINUED: (2)7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     95.\\nPHONETIC\\nI-man JI-ni va JE-dhor.\\nI-have need to water.\\n-------------------------------------------------------------------\\n(WORKER_507_7.mp3)\\nWORKER\\nI need food.\\nTRANSLATION\\nIman jini va havor.\\nPHONETIC\\nI-man JI-ni va HA-vor.\\nI-have need to food.\\n-------------------------------------------------------------------\\n(WORKER_507_8.mp3)\\nWORKER\\nWhere do I go?\\nTRANSLATION\\nSkure jan?\\nPHONETIC\\nSKU-re JAN?\\nWhere I-go?\\n-------------------------------------------------------------------\\n(WORKER_507_9.mp3)\\nWORKER\\nGo talk to him.\\nTRANSLATION\\nJa si ji kimivá.\\nPHONETIC\\nJA si JI ki-mi-VA.\\nGo and him talk.\\n-------------------------------------------------------------------\\nCONTINUED: (2)7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     95.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 95, 'page_label': '96'}, page_content='CONTINUED: (3)7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     96.\\n(WORKER_507_10.mp3)\\nWORKER\\nI’m finished with this part.\\nTRANSLATION\\nTadhathang kiz ejim.\\nPHONETIC\\nta-DHA-thang KIZ e-JIM.\\nI-finished-with this part.\\n-------------------------------------------------------------------\\n(WORKER_507_11.mp3)\\nWORKER\\nGood. Now go over there.\\nTRANSLATION\\nSyz. Sir ja va kunir.\\nPHONETIC\\nSIZ. sir JA va KU-nir.\\nGood. Now go over there.\\n-------------------------------------------------------------------\\n(WORKER_507_12.mp3)\\nWORKER\\nI can’t dig here.\\nTRANSLATION\\nAnha laz vo hilok jinne.\\nPHONETIC\\nAN-ha-laz vo hi-LOK JIN-ne.\\nI-can not dig here.\\n------------------------------------------------------------------\\nGAME OF THRONES #508\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/25/14\\nCONTINUED: (3)7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     96.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 96, 'page_label': '97'}, page_content='CONTINUED: (4)7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     97.\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. HARDHOME LONG HALL - DAY8.20 8.20\\n(WUN_WUN_508_1.mp3)\\nWUN WUN\\nThe fuck you looking at?\\nTRANSLATION\\nLokh doys bar thol kif rukh?\\nPHONETIC\\nlokh DOYS bar thol KIF rukh? \\nWhat fuck you be looking-at it?\\n------------------------------------------------------------------\\nGAME OF THRONES #509\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/14/14\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nCONTINUED: (4)7.13 7.13\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     97.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 97, 'page_label': '98'}, page_content='CONTINUED:8.20 8.20\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     98.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. ARENA - THE FIGHTING PIT OF DAZNAK - CONTINUOUS9.36 9.36\\n(ANNOUNCER_509_1.mp3)\\nANNOUNCER\\nFree citizens of Meereen! By the blessings of the Graces, and her \\nmajesty the Queen, welcome to the great games!\\nTRANSLATION\\nDaeri pastyssy ez Mirin! Po mandaves dos espo Tebillazmi, si zya \\neghlive ji Dare, jerásk va po krazi ghavaji!\\nPHONETIC\\nDAI-ri pas-TIS-si ez mi-RIN! po man-DA-ves-dos es-po te-bil-LAZ-mi, \\nsi ZI-a egh-LI-ve ji DA-re, je-RASK va po KRA-zi gha-VA-ji!\\nFree citizens of Meereen! The blessings-by of-the Graces, and her \\nmajesty the Queen, welcome to the great games!\\n-------------------------------------------------------------------\\n(ANNOUNCER_509_2.mp3)\\nANNOUNCER\\nMy Queen, our first contest. Who will triumph: the strong, or the \\nquick?\\nTRANSLATION\\nNya Dare, ilvo ile vettílaskir. Sparo maneris: ji kotova ja \\nj’adhirve? \\nPHONETIC\\nNI-a DA-re, IL-vo I-le vet-TI-las-kir. SPA-ro ma-NE-ris: ji ko-TO-va \\nja ja-DHIR-ve?\\nMy queen, our first contest. Who wins: the strong or the quick?\\n-------------------------------------------------------------------\\nCONTINUED:8.20 8.20\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     98.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 98, 'page_label': '99'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     99.\\nEXT. ROYAL BOX - THE FIGHTING PIT OF DAZNAK - CONTINUOUS9.37 9.37\\n(BEHEMOTH_FIGHTER_509_3.mp3)\\nBEHEMOTH FIGHTER\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAohor jaqiarzir ivilibin imorghulin, jaqiarzus Darys.\\nPHONETIC\\na-O-hor ja-ki-AR-zir i-vi-LI-bin i-mor-ghu-LIN, ja-ki-AR-zus DA-ris.\\nYour glory I-fight-for and-I-die-for, glorious Queen.\\n-------------------------------------------------------------------\\n(WIRY_FIGHTER_509_4.mp3)\\nWIRY FIGHTER\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAohor jaqiarzir ivilibin imorghulin, jaqiarzus Darys.\\nPHONETIC\\na-O-hor ja-ki-AR-zir i-vi-LI-bin i-mor-ghu-LIN, ja-ki-AR-zus DA-ris.\\nYour glory I-fight-for and-I-die-for, glorious Queen.\\n-------------------------------------------------------------------\\n(ANNOUNCER_509_5.mp3)\\nANNOUNCER\\nFirst there were two. Now there are four. Who will triumph: \\nattackers or defenders?\\nTRANSLATION\\nNesko hunilesk lanta. Sir honesk izola. Sparo maneris: po idhaguges \\nja po mizes? \\nPHONETIC\\nNES-ko hu-ni-lesk LAN-ta. SIR ho-nesk i-ZO-la. SPA-ro ma-NE-ris: po \\ni-dha-GU-ges ja po MI-zes?\\nBefore there-were two. Now there-are four. Who wins: the attackers \\nor the defenders?\\n-------------------------------------------------------------------\\n(ANNOUNCER_509_6.mp3)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     99.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 99, 'page_label': '100'}, page_content='CONTINUED:9.37 9.37\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     100.\\nANNOUNCER\\nWe ask again: Who will triumph?\\nTRANSLATION\\nPindi tuli: Sparo maneris?\\nPHONETIC\\nPIN-di TU-li: SPA-ro ma-NE-ris?\\nWe-ask again: Who wins?\\n-------------------------------------------------------------------\\nEXT. ARENA - THE FIGHTING PIT OF DAZNAK - CONTINUOUS9.41 9.41\\n(ANNOUNCER_509_7.mp3)\\nANNOUNCER\\nA Dothraki warrior?\\nTRANSLATION\\nMe Dothraki ladjak?\\nPHONETIC\\nme DOTH-ra-ki la-DJAK?\\nA Dothraki warrior?\\n-------------------------------------------------------------------\\n(DOTHRAKI_WARRIOR_509_8.mp3)\\nDOTHRAKI WARRIOR\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAohor jaqiarzir ivilivin imorkhulin, jaqiarzos Daris.\\nPHONETIC\\na-o-HOR ja-ki-ar-ZIR i-vi-li-VIN i-mor-khu-LIN, ja-ki-ar-ZOS da-RIS.\\nYour glory I-fight-for and-I-die-for, glorious Queen. \\n-------------------------------------------------------------------\\n(ANNOUNCER_509_9.mp3)\\nANNOUNCER\\nA Meereenese Champion?\\nCONTINUED:9.37 9.37\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     100.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 100, 'page_label': '101'}, page_content='CONTINUED:9.41 9.41\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     101.\\nTRANSLATION\\nMe Mirinía Kote?\\nPHONETIC\\nme mi-ri-NI-a KO-te?\\nA Meereenese Champion?\\n-------------------------------------------------------------------\\n(MEEREENESE_CHAMPION_509_10.mp3)\\nMEEREENESE CHAMPION\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAor yaqiarjil yewiliving yemoruling, yaqiarzush Tharish.\\nPHONETIC\\nA-or ya-ki-AR-jil ye-wi-LI-ving ye-mo-ru-LING ya-ki-AR-zush THA-\\nrish.\\nYour glory I-fight-for and-I-die-for, glorious Queen.\\n-------------------------------------------------------------------\\n(ANNOUNCER_509_11.mp3)\\nANNOUNCER\\nA Braavosi Braavo?\\nTRANSLATION\\nMe Bravozía Bravo?\\nPHONETIC\\nme bra-vo-ZI-a BRA-vo?\\nA Braavosi Braavo?\\n-------------------------------------------------------------------\\n(BRAAVO_509_12.mp3)\\nBRAAVO\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAohhor jaqiarzir ivilibbin imorghulin, jaqiarzus Darrys.\\nPHONETIC\\na-OH-hor ja-ki-AR-zir i-vi-LIB-bin i-MOR-ghu-LIN, ja-ki-AR-zus DAR-\\nris.\\nCONTINUED:9.41 9.41\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     101.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 101, 'page_label': '102'}, page_content=\"CONTINUED: (2)9.41 9.41\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     102.\\nYour glory I-fight-for and-I-die-for, glorious Queen.\\n-------------------------------------------------------------------\\n(ANNOUNCER_509_13.mp3)\\nANNOUNCER\\nA Summer Islander?\\nTRANSLATION\\nMe Jedhríy?\\nPHONETIC\\nme je-DHRI-i?\\nA Summer-Islander?\\n-------------------------------------------------------------------\\n(SUMMER_ISLANDER_509_14.mp3)\\nSUMMER ISLANDER\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAohor jaqiarzir ivilibin imorghulin, jaqiarzus Darys.\\nPHONETIC\\na-O-hor ja-ki-AR-zir i-vi-LI-bin i-mor-ghu-LIN, ja-ki-AR-zus DA-ris.\\nYour glory I-fight-for and-I-die-for, glorious Queen.\\n-------------------------------------------------------------------\\n(ANNOUNCER_509_15.mp3)\\nANNOUNCER\\nA Northern wildling?\\nTRANSLATION\\nMe Jelmoni dyni vala?\\nPHONETIC\\nme jel-MO-ni DI-ni VA-la?\\nA Northern beast man?\\n-------------------------------------------------------------------\\n(NORTHERN_WILDLING_509_16.mp3)\\nCONTINUED: (2)9.41 9.41\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     102.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 102, 'page_label': '103'}, page_content='CONTINUED: (3)9.41 9.41\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     103.\\nNORTHEN WILDLING\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAohor jaqiarzir ivilibin imorghulin, jaqiarzus Darys.\\nPHONETIC\\na-O-hor ja-ki-AR-zir i-vi-LI-bin i-mor-ghu-LIN, ja-ki-AR-zus DA-ris.\\nYour glory I-fight-for and-I-die-for, glorious Queen.\\n-------------------------------------------------------------------\\n(ANNOUNCER_509_17.mp3)\\nANNOUNCER\\nOr a Westerosi knight?\\nTRANSLATION\\nJa me Vesterozía azanty?\\nPHONETIC\\nja me ves-te-ro-ZI-a a-ZAN-ti?\\nOr a Westerosi knight?\\n-------------------------------------------------------------------\\n(JORAH_509_18.mp3)\\nJORAH\\nI fight and die for your glory, O glorious Queen.\\nTRANSLATION\\nAohor jaqiarzir ivilibin imorghulin, jaqiarzus Darys.\\nPHONETIC\\na-O-hor ja-ki-AR-zir i-vi-LI-bin i-mor-ghu-LIN, ja-ki-AR-zus DA-ris.\\nYour glory I-fight-for and-I-die-for, glorious Queen.\\n-------------------------------------------------------------------\\nEXT. FIGHTING PIT - DROGON ARRIVES - CONTINUOUS9.57 9.57\\n(DANY_509_19.mp3)\\nDANY\\nFly!\\nCONTINUED: (3)9.41 9.41\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     103.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 103, 'page_label': '104'}, page_content='CONTINUED:9.57 9.57\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     104.\\nTRANSLATION\\nSoves!\\nPHONETIC\\nso-VES!\\nFly!\\n------------------------------------------------------------------\\nGAME OF THRONES #510\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/12/14\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. AUDIENCE CHAMBER - DAY10.29 10.29\\n(JORAH_510_1.mp3)\\nJORAH\\nTorgo Nudho.\\nTRANSLATION\\nTorgo Nudho.\\nPHONETIC\\nTOR-go NU-dho.\\nGrey Worm.\\n-------------------------------------------------------------------\\nCONTINUED:9.57 9.57\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     104.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 104, 'page_label': '105'}, page_content='CONTINUED:10.29 10.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     105.\\n(MISSANDEI_510_2.mp3)\\nMISSANDEI\\nIt’s true. And I would be dead if not for the... ...little man.\\nTRANSLATION\\nSa dreji. Si eskin murghi ynones ji... ...byka vala.\\nPHONETIC\\nsa DRE-ji. si ES-kin MUR-ghi i-NO-nes ji... ...BI-ka VA-la.\\nIt’s true. And I-would-be dead if-not-for the... ...little man.\\n-------------------------------------------------------------------\\n(TYRION_510_3.mp3)\\nTYRION\\nDwarf. I believe that’s the word? Apologies, my Valyrian is a bit \\nnostril.\\nTRANSLATION\\nKrubo. Nyke pasan kesor udir drejor issa? Munna, nya Valyrio mirri \\npungilla issa.\\nPHONETIC\\nKRU-bo. NI-ke PA-san KE-sor U-dir DRE-jor IS-sa? MUN-na, NI-a va-LI-\\nri-o MIR-ri pun-GIL-la IS-sa.\\nDwarf. I believe this word correct is? Sorrows, my Valyrian slightly \\nnostril is.\\n-------------------------------------------------------------------\\n(MISSANDEI_510_4.mp3)\\nMISSANDEI\\n“A bit rusty.”\\nTRANSLATION\\n“Mirri puñila.”\\nPHONETIC\\nMIR-ri pun-YI-la.\\nSlightly rusty.\\n-------------------------------------------------------------------\\n(TYRION_510_5.mp3)\\nCONTINUED:10.29 10.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     105.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 105, 'page_label': '106'}, page_content='CONTINUED: (2)10.29 10.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     106.\\nTYRION\\n“A bit rusty.”\\nTRANSLATION\\n“Mirri puñila.”\\nPHONETIC\\nMIR-ri pun-YI-la.\\nSlightly rusty.\\n------------------------------------------------------------------\\nGAME OF THRONES #601\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/03/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. MEEREEN STREET - DAY1.18 1.18\\n(TYRION_601_1.mp3)\\nTYRION\\nFor your baby. To eat.\\nTRANSLATION\\nAoha ruho syt. Ipradon.\\nPHONETIC\\na-O-ha ru-HO-sit. i-PRA-don.\\nCONTINUED: (2)10.29 10.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     106.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 106, 'page_label': '107'}, page_content='CONTINUED:1.18 1.18\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     107.\\nYour baby-in-exchange-for. To-eat.\\n-------------------------------------------------------------------\\n(VARYS_601_2.mp3)\\nVARYS\\nHis Valyrian is terrible. He only wants to give you money, so your \\nbaby can eat.\\nTRANSLATION\\nZyha Valyria qupegrie issa. Gelyri aot tepagon jaelas, hegnir aohys \\nrus ipradagon kostos.\\nPHONETIC\\nZI-ha va-LI-ri-a ku-PE-gri-e IS-sa. ge-LI-ri A-ot te-PA-gon JAI-las, \\nHEG-nir a-O-his RUS i-pra-DA-gon KOS-tos.\\nHis Valyrian terrible is. Money to-you to-give he-wants, so-that \\nyour baby eat can.\\n-------------------------------------------------------------------\\nEXT. COVERED PASSAGEWAY - DAY1.22 1.22\\n(ZANRUSH_601_3.mp3)\\nZANRUSH\\nThe Lord of Light sent the Mother of Dragons to you -- and those who \\nlove the darkness chased her away.\\nTRANSLATION\\nAeksio Oño jemot Muñe Zaldrizoti jittas -- se syndrori jorraelis lyr \\nozdakanot ziry dintis.\\nPHONETIC\\nAIK-si-o ON-yo je-mot MUN-ye zal-DI-zo-ti JIT-tas -- se SIN-dro-ri \\njor-RAI-lis lir oz-da-KA-not ZI-ri DIN-tis.\\nLord of-Light to-you Mother of-Dragons sent -- and darkness love \\nthose-who to-flight her put.\\n-------------------------------------------------------------------\\n(ZANRUSH_601_4.mp3)\\nCONTINUED:1.18 1.18\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     107.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 107, 'page_label': '108'}, page_content='CONTINUED:1.22 1.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     108.\\nZANRUSH\\nHow will you respond? Will you let them drag you back into the long \\nnight of bondage? Will you wring your hands while you wait idly for \\nthe Mother of Dragons to return?\\nTRANSLATION\\nSkorkydoso udlilat? Belmondo bantazma bosa jemi qladilusy botilat? \\nOndossa jorhakelat, lykapsiri Zaldrizoti Muño amazinon jumbari?\\nPHONETIC\\nskor-ki-DO-so ud-LI-lat? bel-MON-do ban-TAZ-ma BO-sa je-mi kla-DI-lu-\\nsi bo-TI-lat? on-DOS-sa jo-rha-KE-lat, li-kap-SI-ri zal-DRI-zo-ti \\nMUN-yo a-MA-zi-non JUM-ba-ri?\\nHow will-you-respond? Bondage’s to-midnight long you they-drag will-\\nyou-allow? Hands will-you-worry, idly Dragons’ Mother’s return \\nawaiting?\\n-------------------------------------------------------------------\\n(ZANRUSH_601_5.mp3)\\nZANRUSH\\nOr will you take up her flames yourselves? Will you burn away the \\nchains and the nonbelievers who make them?\\nTRANSLATION\\nIa jemela zyhys perzi ondurilat? Belma se ponte setessis lyri \\nnapastyri ozzalilat?\\nPHONETIC\\nya je-ME-la ZI-his PER-zi on-du-RI-lat? BEL-ma se PON-te se-TES-sis \\nLI-ri na-PAS-ti-ri oz-ZA-li-lat?\\nOr you her flames will-you-take-up? Chains and them make who \\nnonbelievers will-you-burn-away?\\n-------------------------------------------------------------------\\n(ZANRUSH_601_6.mp3)\\nZANRUSH\\nWill you fight for your own salvation, now that Queen Daenerys is \\nnot here to fight for you? Will you kill the--\\nTRANSLATION\\nJemelo kaerinnon ivilibilat, lo sir Daria Daenerys jemi ivilibagon \\nkesir ilos daor? Aeksia ossenilat se--\\nCONTINUED:1.22 1.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     108.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 108, 'page_label': '109'}, page_content='CONTINUED: (2)1.22 1.22\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     109.\\nPHONETIC\\nje-ME-lo kai-RIN-non i-vi-LI-bi-lat, lo sir DA-ri-a DAI-ne-ris je-mi \\ni-vi-LI-ba-gon KE-sir I-los DAOR? AIK-si-a os-SE-ni-lat se--\\nYour-own salvation will-you-fight-for, when now Queen Daenerys you \\nto-fight-for here is not? Will you kill the--\\n-------------------------------------------------------------------\\nEXT. DOTHRAKI SEA - DAY1.28 1.28\\n(QHONO_601_7.mp3)\\nQHONO\\nMaybe she saw a ghost. My sister’s friend’s mother saw a ghost and \\nher hair turned white.\\nTRANSLATION\\nIshish me tih leyes. Mai okeosi inavvasi anni tih leyes majin noreth \\nmoon zasqaso.\\nPHONETIC\\ni-SHISH me tih le-YES. MA-i O-ke-o-si I-nav-va-si an-ni tih le-YES \\nma-jin no-RETH mo-on ZAS-qa-so.\\nMaybe she saw ghost. Mother of-friend of-sister my saw ghost and \\nhair from-her turned-white.\\n-------------------------------------------------------------------\\n(AKHO_601_8.mp3)\\nAKHO\\nPink people are afraid of the sun. It burns their skin. So this pink \\ngirl, she probably stands too long in the sun and her hair goes \\nwhite.\\nTRANSLATION\\nHannavenaki rokhi shekhes. Me avvirsae ilek moroa. Majin jin \\nhannaveneesi, ishish me kovara torga shekhi k’athneakari sekke majin \\nnoreth zasqasoe.\\nPHONETIC\\nHAN-na-ve-na-ki RO-khi she-KHES. me AV-vir-sa-e i-LEK mo-ro-a. ma-\\nJIN jin HAN-na-ve-ne-e-si, i-SHISH me KO-va-ra tor-ga SHE-khi KATH-\\nne-a-ka-ri SEK-ke ma-jin no-RETH ZAS-qa-so-e.\\nPink-people fear sun. It burns skin from-them. So this pink-girl, \\nCONTINUED: (2)1.22 1.22\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     109.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 109, 'page_label': '110'}, page_content=\"CONTINUED:1.28 1.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     110.\\nmaybe she stands under sun long-time too-much and (the) hair turns-\\nwhite.\\n-------------------------------------------------------------------\\n(QHONO_601_9.mp3)\\nQHONO\\nYou think she’s got white pussy hair, too? You ever been with a girl \\nwith white pussy hair?\\nTRANSLATION\\nHash yer dirgi megech mae hemee ma norethoon zasqa akka? Hash yer \\nray chilo ma nayatoon ma qeviroon lajaki zasqa?\\nPHONETIC\\nHASH yer DIR-gi me-GECH ma-e HE-me-e ma no-re-tho-ON ZAS-qa AK-ka? \\nHASH yer ray CHI-lo ma na-ya-to-ON ma qe-vi-ro-ON LA-ja-ki ZAS-qa?\\nDo you think that-pussy her is-fur-covered with hair white also? Did \\nyou ever sleep with girl with pubic bush white?\\n-------------------------------------------------------------------\\n(AKHO_601_10.mp3)\\nAKHO\\nOnly when I was fucking your grandma.\\nTRANSLATION\\nKash anha hile kristasof yeri disse.\\nPHONETIC\\nkash AN-ha HI-le kris-ta-SOF ye-ri DIS-se.\\nWhen I was-fucking grandmother your only.\\n-------------------------------------------------------------------\\n(QHONO_601_11.mp3)\\nQHONO\\nI’ll ask Khal Moro for a night with you. What do you think?\\nTRANSLATION\\nAnha aqafak zhey Khaloon Moro ajjalanes ma yeroon. Fin yer dirgi?\\nPHONETIC\\nAN-ha a-qa-FAK zhey kha-lo-ON MO-ro aj-ja-la-NES ma ye-ro-ON. FIN \\nyer dir-gi?\\nCONTINUED:1.28 1.28\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     110.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 110, 'page_label': '111'}, page_content='CONTINUED: (2)1.28 1.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     111.\\nI will-ask Khal Moro a-night with you. What you think?\\n-------------------------------------------------------------------\\n(AKHO_601_12.mp3)\\nAKHO\\nPretty eyes, but she’s an idiot.\\nTRANSLATION\\nTihi zheana, vosma me tokik.\\nPHONETIC\\nTI-hi ZHE-a-na, vos-ma me to-KIK.\\nEyes pretty, but she (is) idiot.\\n-------------------------------------------------------------------\\n(QHONO_601_13.mp3)\\nQHONO\\nShe doesn’t have to be smart to get fucked in the ass.\\nTRANSLATION\\nAnha vo zigerok memé deva ahilek mae vi choyokh.\\nPHONETIC\\nAN-ha vo zi-ge-ROK me-ME DE-va a-hi-LEK ma-e vi cho-YOKH.\\nI don’t need that-she be-smart to-fuck her between (the) asscheeks.\\n-------------------------------------------------------------------\\n(AKHO_601_14.mp3)\\nAKHO\\nI like to talk when I’m finished. Otherwise, we might as well be \\ndogs.\\nTRANSLATION\\nMe allayafa anna vasterat irge me nakhoe. Hash vos, hash kisha \\njanaan.\\nPHONETIC\\nme AL-la-ya-fa an-na vas-te-RAT ir-ge me NA-kho-e. hash VOS, hash KI-\\nsha ja-na-AN.\\nCONTINUED: (2)1.28 1.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     111.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 111, 'page_label': '112'}, page_content='CONTINUED: (3)1.28 1.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     112.\\nIt pleases me to-talk after it finishes. If not, then we would-be-\\ndogs.\\n-------------------------------------------------------------------\\nEXT. DOTHRAKI CAMP - DAY1.29 1.29\\n(AKHO_601_15.mp3)\\nAKHO\\nFor you, my Khal. The white-haired girl we found in the hills.\\nTRANSLATION\\nHa shafkea, zhey Khal anni. Nayat nharesi vizhada mekisha ezish \\nsh’olta.\\nPHONETIC\\nha SHAF-ke-a, zhey KHAL AN-ni. na-YAT na-HA-re-si VI-zha-da me-KI-\\nsha e-ZISH SHOL-ta.\\nFor you, O Khal of-mine. Girl with-head silver-colored that-we found \\nin-the-hills.\\n-------------------------------------------------------------------\\n(BLOODRIDER1_601_16.mp3)\\nBLOODRIDER #1\\nLook at those lips, blood of my blood.\\nTRANSLATION\\nTihis jin hethis, zhey qoy qoyi.\\nPHONETIC\\nti-HIS jin he-THIS, zhey QOY QO-yi.\\nLook-at those lips, O blood of-blood (of mine).\\n-------------------------------------------------------------------\\n(WIFE1_601_17.mp3)\\nWIFE #1\\nBlue-eyed women are witches.\\nTRANSLATION\\nChiorisi tihi chandri maegi.\\nCONTINUED: (3)1.28 1.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     112.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 112, 'page_label': '113'}, page_content='CONTINUED:1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     113.\\nPHONETIC\\nCHI-o-ri-si TI-hi CHAN-dri MA-e-gi.\\nWomen of-eyes blue (are) witches.\\n-------------------------------------------------------------------\\n(WIFE2_601_18.mp3)\\nWIFE #2\\nIt is known.\\nTRANSLATION\\nMe nem nesa.\\nPHONETIC\\nME nem NE-sa.\\nIt is known.\\n-------------------------------------------------------------------\\n(WIFE1_601_19.mp3)\\nWIFE #1\\nCut off her head before she casts a spell on you.\\nTRANSLATION\\nZirisses nhare moon hatif me ta movekh yeraan.\\nPHONETIC\\nzi-ris-SES NHA-re mo-ON ha-tif me ta mo-VEKH ye-ra-AN.\\nCut (the) head from-her before she does (black) magic to-you.\\n-------------------------------------------------------------------\\n(MORO_601_20.mp3)\\nKHAL MORO\\nEven if I was blind, I’d hear my wives say, “Cut off her head,” and \\nI’d know this woman is beautiful.\\nTRANSLATION\\nHash anha azisirek, hash anha acharak mechiorikemis anni asti ki, \\n“Zirisses nhare moon”, majin anha anesak sekosshi mejin chiori \\nzheanae.\\nCONTINUED:1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     113.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 113, 'page_label': '114'}, page_content='CONTINUED: (2)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     114.\\nPHONETIC\\nhash an-ha a-zi-si-REK, hash an-ha a-cha-RAK me-chi-o-ri-ke-MIS an-\\nni as-ti ki, “zi-ris-SES NHA-re mo-on”, ma-JIN an-ha a-ne-SAK se-\\nKOSH-shi me-jin CHI-o-ri ZHE-a-na-e.\\nIf I would-be-blind, then I would-hear that-wives my say by, “Cut \\n(the) head from-her”, and-then I would-know for-sure that-this woman \\nis-beautiful.\\n-------------------------------------------------------------------\\n(MORO_601_21.mp3)\\nKHAL MORO\\nI’m glad I’m not blind. Seeing a beautiful woman naked for the first \\ntime -- what is better than that?\\nTRANSLATION\\nMe allayafa anna m’anha vo zisirok. Tihat chiories zheana \\nk’athzhonathari hatif eyaki -- fin adavrana?\\nPHONETIC\\nme AL-la-ya-fa an-na man-ha VO zi-si-ROK. ti-HAT chi-o-ri-ES ZHE-a-\\nna KATH-zho-na-tha-ri ha-TIF E-ya-ki -- FIN A-da-vra-na?\\nIt gladdens me that-I not am-blind. To-see woman beautiful nakedly \\nbefore all-else -- what is-better?\\n-------------------------------------------------------------------\\n(BLOODRIDER1_601_22.mp3)\\nBLOODRIDER #1\\nKilling another Khal.\\nTRANSLATION\\nAtthasat eshna khales.\\nPHONETIC\\nath-tha-SAT ESH-na kha-LES.\\nTo-kill another khal.\\n-------------------------------------------------------------------\\n(MORO_601_23.mp3)\\nKHAL MORO\\nYes, killing another Khal.\\nCONTINUED: (2)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     114.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 114, 'page_label': '115'}, page_content='CONTINUED: (3)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     115.\\nTRANSLATION\\nSek, atthasat eshna khales.\\nPHONETIC\\nSEK, ath-tha-SAT ESH-na kha-LES.\\nYes, to-kill another khal.\\n-------------------------------------------------------------------\\n(BLOODRIDER2_601_24.mp3)\\nBLOODRIDER #2\\nConquering a city and taking her people as slaves and taking her \\nidols back to Vaes Dothrak.\\nTRANSLATION\\nAssilat vaes majin azzafrolat gimisires mae majin yanqolat jor mae \\nVaesaan Dothrak. \\nPHONETIC\\nas-si-LAT va-ES ma-jin az-za-fro-LAT gi-mi-si-RES ma-e ma-jin yan-qo-\\nLAT JOR ma-e va-e-sa-AN do-THRAK.\\nTo-conquer city and-then enslave people its and-then take idols its \\nto-Vaes Dothrak.\\n-------------------------------------------------------------------\\n(BLOODRIDER1_601_25.mp3)\\nBLOODRIDER #1\\nBreaking a wild horse, forcing a half ton of muscle to submit to \\nyour will.\\nTRANSLATION\\nVishaferat hrazef chafi; iffat krazaaj mesi k’oakahi.\\nPHONETIC\\nvi-sha-fe-RAT hra-ZEF CHA-fi; if-FAT kra-za-AJ ME-si KO-a-ka-hi.\\nTo-break horse wild; to-force-to-submit half-ton of-muscle by-your-\\nwill.\\n-------------------------------------------------------------------\\n(MORO_601_26.mp3)\\nCONTINUED: (3)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     115.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 115, 'page_label': '116'}, page_content='CONTINUED: (4)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     116.\\nKHAL MORO\\nSeeing a beautiful woman naked for the first time -- it is among the \\nfive best things in life.\\nTRANSLATION\\nTihat chiories zheana k’athzhonathari hatif eyaki -- me vi mek \\nathnakhar adavranaz atthiraroon.\\nPHONETIC\\nti-HAT chi-o-ri-ES ZHE-a-na KATH-zho-na-tha-ri ha-TIF E-ya-ki -- me \\nvi MEK ath-na-KHAR a-da-vra-NAZ ath-thi-ra-ro-ON.\\nTo-see woman beautiful nakedly before all-else -- it (is) among five \\nthings best from-life.\\n-------------------------------------------------------------------\\n(DANY_601_27.mp3)\\nDANY\\nDo not touch me.\\nTRANSLATION\\nVo frakho anna vosecchi.\\nPHONETIC\\nVO FRA-kho an-na vo-SECH-chi.\\nDon’t touch me ever.\\n-------------------------------------------------------------------\\n(DANY_601_28.mp3)\\nDANY\\nI am Daenerys Stormborn of the House Targaryen, the First of Her \\nName, the Unburnt, Queen of Meereen, Queen of the Andals and the \\nRhoynar and the First Men, Khaleesi of the Great Grass Sea, Breaker \\nof Chains and Mother of Dragons.\\nTRANSLATION\\nAnha Daenerys Vazyol h’Okreseroon Targeryen, Atak ma Hakesoon Mae, \\nOsavvirsak, Khaleesi Mirini, Khaleesi m’Andahli ma Roynari m’Ataki, \\nKhaleesi Havazhofi Hranni, ma Haggey-Assamvak ma Mai Zhavorsi.\\nPHONETIC\\nAN-ha de-NE-ris vaz-YOL ho-kre-se-ro-ON tar-GER-yen, a-TAK ma ha-ke-\\nso-ON ma-e, o-sav-vir-SAK, KHA-le-e-si MI-ri-ni, KHA-le-e-si man-DAH-\\nli ma ROY-na-ri MA-ta-ki, KHA-le-e-si HA-va-zho-fi HRAN-ni, ma hag-\\nGEY as-sam-VAK ma MA-i zha-VOR-si.\\nCONTINUED: (4)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     116.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 116, 'page_label': '117'}, page_content='CONTINUED: (5)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     117.\\nI (am) Daenerys Stormborn from-house Targaryen, first with name her, \\nUburnt, Queen of-Meereen, Queen and-of-Andals and Rhoynars and First-\\nOnes, Khaleesi of-Great-Sea of-grass, and chain-breaker and Mother \\nof-Dragons.\\n-------------------------------------------------------------------\\n(MORO_601_29.mp3)\\nKHAL MORO\\nYou are nobody, the millionth of your name, Queen of Nothing, slave \\nof Khal Moro.\\nTRANSLATION\\nYer vosak, yorak ma hakesoon yeri, Khaleesi Vosi, zafra Khali Moro.\\nPHONETIC\\nYER vo-SAK, yo-RAK ma ha-ke-so-ON ye-ri, KHA-le-e-si VO-si, ZAF-ra \\nKHA-li MO-ro.\\nYou (are) nobody, millionth-one with name your, Queen of-nothing, \\nslave of-Khal Moro.\\n-------------------------------------------------------------------\\n(MORO_601_30.mp3)\\nKHAL MORO\\nTonight I will lie with you, and if the Great Stallion is kind you \\nwill give me a son. Do you understand?\\nTRANSLATION\\nAjjalan anha achilok ma yeroon, ma hash Vezhof erina, hash yer \\nvayyoe anhaan rizhes. Hash yer tihoe?\\nPHONETIC\\naj-ja-LAN an-ha a-chi-LOK ma ye-ro-ON, ma hash ve-ZHOF E-ri-na, hash \\nyer VAY-yo-e an-ha-AN ri-ZHES. HASH yer ti-ho-e?\\nTonight I will-lie with you, and if (the) Great-Stallion is-kin, \\nthen you will-bear me (a) son. Do you understand?\\n-------------------------------------------------------------------\\n(DANY_601_31.mp3)\\nCONTINUED: (5)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     117.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 117, 'page_label': '118'}, page_content='CONTINUED: (6)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     118.\\nDANY\\nI will not lie with you. And I will bear no children, for you or \\nanyone else. Not until the sun rises in the west and sets in the \\neast.\\nTRANSLATION\\nAnha vos ochilok ma shafkoa vosecchi. M’anha vo vayyok vo yal che ha \\nshafkea che h’eshnakaan. Avvos vosma shekh yola she jimma ma drivoe \\nshe titha.\\nPHONETIC\\nAN-ha VOS o-chi-LOK ma shaf-ko-a vo-SECH-chi. MAN-ha VO vai-YOK VO \\nyal che ha SHAF-ke-a che hesh-na-ka-AN. av-VOS vos-ma SHEKH YO-la \\nshe JIM-ma ma DRI-vo-e she TI-tha.\\nI not will-lie with you never. And-I not will-bear no children or \\nfor you or for-another. Never but (the) sun rises in (the) west and \\nsets in (the) east.\\n-------------------------------------------------------------------\\n(WIFE1_601_32.mp3)\\nWIFE #1\\nI told you she is a witch. Cut off her head.\\nTRANSLATION\\nAnha ast yeraan, me maegi. Zirisses nhare moon.\\nPHONETIC\\nAN-ha AST ye-ra-AN, me MA-e-gi. zi-ris-SES NHA-re mo-ON.\\nI told you, she is-witch. Cut-off head from-her.\\n-------------------------------------------------------------------\\n(MORO_601_33.mp3)\\nKHAL MORO\\nI like her. She has spirit.\\nTRANSLATION\\nMe allayafa anna. Athvadar mra qora.\\nPHONETIC\\nme AL-la-ya-fa an-na. ath-va-DAR mra QO-ra.\\nShe pleases me. Spirit (is) in her.\\nCONTINUED: (6)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     118.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 118, 'page_label': '119'}, page_content='CONTINUED: (7)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     119.\\n-------------------------------------------------------------------\\n(DANY_601_34.mp3)\\nDANY\\nI was wife to Khal Drogo, son of Khal Bharbo.\\nTRANSLATION\\nAnha chiorikemoon ha Khalaan Drogo ki Bharbosi.\\nPHONETIC\\nAN-ha chi-o-ri-ke-mo-ON ha kha-la-AN DRO-go ki BAR-bo-si.\\nI was-wife for Khal Drogo son-of Bharbo.\\n-------------------------------------------------------------------\\n(MORO_601_35.mp3)\\nKHAL MORO\\nKhal Drogo is dead.\\nTRANSLATION\\nKhal Drogo driva.\\nPHONETIC\\nKHAL DRO-go DRI-va.\\nKhal Drogo is-dead.\\n-------------------------------------------------------------------\\n(DANY_601_36.mp3)\\nDANY\\nI know. I burnt his body.\\nTRANSLATION\\nAnha nesak. Anha avvirsa khadokh moon.\\nPHONETIC\\nAN-ha ne-SAK. AN-ha AV-vir-sa kha-DOKH mo-ON.\\nI know. I burned body his.\\n-------------------------------------------------------------------\\n(MORO_601_37.mp3)\\nCONTINUED: (7)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     119.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 119, 'page_label': '120'}, page_content='CONTINUED: (8)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     120.\\nKHAL MORO\\nForgive me. I did not know. It is forbidden to lie with a Khal’s \\nwidow. No one will touch you, you have my word.\\nTRANSLATION\\nAnha nemo echomosak. Anha vo neso. Me izvena, jin athchilozar ma \\nkhaleenisoon. Vosak ofrakha yera vosecchi, anha astak yeraan asqoy.\\nPHONETIC\\nAN-ha ne-mo e-cho-ma-SAK. AN-ha VO NE-so. me IZ-ve-na, jin ath-chi-\\nlo-ZAR ma KHA-le-e-ni-so-ON. vo-SAK O-fra-kha ye-ra vo-SECH-chi, an-\\nha as-TAK ye-ra-an as-QOY.\\nI myself dishonor. I not knew. It is-forbidden, this lying with \\nkhal’s-widow. Nobody will-not-touch you never, I say to-you (my) \\noath.\\n-------------------------------------------------------------------\\n(DANY_601_38.mp3)\\nDANY\\nIf you will escort me back to Meereen, I will see that your khalasar \\nis given a thousand horses as a sign of my gratitude.\\nTRANSLATION\\nHash shafka vidrisofi anna Mirinaan, hash anha vammelisok mekhalasar \\nshafki nem vazha dalen hrazef k’azhi anhoon.\\nPHONETIC\\nhash SHAF-ka VI-dri-so-fi an-na mi-ri-na-AN, hash an-ha vam-me-li-\\nSOK me-kha-la-SAR shaf-ki nem VA-zha da-LEN hra-ZEF KA-zhi an-ho-ON.\\nIf you escort me (back) to-Meereen, then I will-ensure that-khalasar \\nyour is given a-thousand horses by-gift from-me.\\n-------------------------------------------------------------------\\n(MORO_601_39.mp3)\\nKHAL MORO\\nWhen a khal dies, there is only place for his khaleesi.\\nTRANSLATION\\nHash khal drivoe, hash at gachi disse vekha ha khaleesisaan mae.\\nPHONETIC\\nhash KHAL DRI-vo-e, hash AT GA-chi DIS-se ve-kha ha kha-le-e-si-sa-\\nAN ma-e.\\nCONTINUED: (8)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     120.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 120, 'page_label': '121'}, page_content='CONTINUED: (9)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     121.\\nWhen khal dies, then one place only is-there for Khaleesi his.\\n-------------------------------------------------------------------\\n(WIFE1_601_40A.mp3)\\nWIFE #1\\nVaes Dothrak. The Temple of the Dosh Khaleen.\\nTRANSLATION\\nVaes Dothrak. Vaesof Doshi Khaleen.\\nPHONETIC\\nva-ES doth-RAK. va-e-SOF DO-shi kha-le-EN.\\nVaes Dothrak. Temple of-Dosh Khaleen.\\n-------------------------------------------------------------------\\n(WIFE2_601_41A.mp3)\\nWIFE #2\\nTo live out her days with the widows of dead khals.\\nTRANSLATION\\nAthira asshekhis mae ma khaleenisoa khali drivi.\\nPHONETIC\\nA-thi-ra ash-she-KHIS ma-e ma KHA-le-e-ni-so-a KHA-li DRI-vi.\\nTo-live days her with widows of-khals dead.\\n-------------------------------------------------------------------\\n(WIFE1_601_42.mp3)\\nWIFE #1\\nIt is known.\\nTRANSLATION\\nMe nem nesa.\\nPHONETIC\\nME nem NE-sa.\\nIt is known.\\n-------------------------------------------------------------------\\nCONTINUED: (9)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     121.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 121, 'page_label': '122'}, page_content='CONTINUED: (10)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     122.\\nGAME OF THRONES #602\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 07/12/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nINT. ????????????????????????????????????2.29 2.29\\n(PRIESTESS_602_1.mp3)\\nPRIESTESS\\nWe ask the Lord to shine his light, and lead a soul out of darkness.\\nTRANSLATION\\nZhyhys oñoso jehikagon Aeksiot epi, se gis hen syndrorro jemagon.\\nPHONETIC\\nZI-his O-nyo-so je-hi-KA-gon AIK-si-ot e-pi, se GIS hen sin-DROR-ro \\nje-MA-gon.\\nHis light to-shine (the) lord we-ask, and (a) soul out-of darkness \\nlead.\\n------------------------------------------------------------------\\n(PRIESTESS_602_2.mp3)\\nPRIESTESS\\nWe beg the Lord to share his fire, and light a candle that has gone \\nout.\\nCONTINUED: (10)1.29 1.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     122.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 122, 'page_label': '123'}, page_content='CONTINUED:2.29 2.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     123.\\nTRANSLATION\\nZyhys perzys stepagon Aeksio Oño jorepi, se morghultas lys qelitsos \\nsikagon.\\nPHONETIC\\nZI-his PER-zis ste-PA-gon AIK-si-o O-nyo jo-RE-pi, se mor-GHUL-tas \\nlis ke-LIT-sos si-KA-gon.\\nHis fire to-share (the) Lord of Light to-share, and has-gone-out \\nthat candle to-light.\\n------------------------------------------------------------------\\n(PRIESTESS_602_3.mp3)\\nPRIESTESS\\nFrom darkness, light. From ashes, fire. From death, life.\\nTRANSLATION\\nHen syndrorro, oños. Hen ñuqir, perzys. Hen morghot, glaeson.\\nPHONETIC\\nhen sin-DROR-ro, O-nyos. hen NYU-kir, PER-zis. Hen MOR-ghot, GLAI-\\nson.\\nFrom darkness, light. From ashes, fire. From death, life.\\n-------------------------------------------------------------------\\nGAME OF THRONES #603\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 07/23/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nCONTINUED:2.29 2.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     123.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 123, 'page_label': '124'}, page_content='CONTINUED: (2)2.29 2.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     124.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. GATES OF VAES DOTHRAK - DAY3.4 3.4\\n(QHONO_603_1.mp3)\\nQHONO\\nHey, Great Khaleesi. Move your ass.\\nTRANSLATION\\nHale, zhey Khaleesi Vezhven. Eyes choy.\\nPHONETIC\\nHA-le, zhey KHA-le-e-si vezh-VEN. e-YES CHOY.\\nHey, O Khaleesi Great. Move ass (yours).\\n-------------------------------------------------------------------\\nEXT. TEMPLE OF THE DOSH KHALEEN - DAY3.6 3.6\\n(MORO_603_2.mp3)\\nKHAL MORO\\nWelcome home, khaleesi.\\nTRANSLATION\\nAnha asshik yera vaesishoon, zhey Khaleesi.\\nPHONETIC\\nAN-ha ash-SHIK ye-ra va-e-si-sho-ON, zhey KHA-le-e-si.\\nI introduce you to-home (your), O Khaleesi.\\n-------------------------------------------------------------------\\nINT. TEMPLE OF THE DOSH KHALEEN - CONTINUOUS3.7 3.7\\n(DKPRIESTESS_603_3.mp3)\\nDOSH KHALEEN PRIESTESS\\nGo.\\nCONTINUED: (2)2.29 2.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     124.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 124, 'page_label': '125'}, page_content='CONTINUED:3.7 3.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     125.\\nTRANSLATION\\nEs.\\nPHONETIC\\nES.\\nGo.\\n-------------------------------------------------------------------\\n(DANY_603_4.mp3)\\nDANY\\nWhat are you doing?! Take your hands off me! I will have your heads! \\nI am the Mother of Dragons! My son was the Stallion Who Mounts the \\nWorld! I am wife of Khal Drogo! You beasts!\\nTRANSLATION\\nFin yeri ti?! Eqorasas anna! Anha afichak anhaan nharees yeroa! Anha \\nMai Zhavorsi! Rizh anni Vezhoon Fin Saja Rhaesheseres! Anha \\nchiorikem Khali Drogo! Zhey ivezho!\\nPHONETIC\\nFIN YE-ri TI?! e-qo-ra-SAS an-na! AN-ha a-fi-CHAK an-ha-AN nha-re-ES \\nye-ro-a! AN-ha MAI zha-VOR-si! RIZH an-ni ve-zho-ON fin SA-ja rha-e-\\nshe-se-RES! AN-ha chi-o-ri-KEM KHA-li DRO-go! zhey I-ve-zho!\\nWhat you-all do?! Unhand me! I will-take to-me heads yours! I (am \\nthe) Mother of-Dragons! Son my was-Stallion Who Mounts (the) World! \\nI (am) wife of-Khal Drogo! O beasts!\\n-------------------------------------------------------------------\\n(DANY_603_5.mp3)\\nDANY\\nYou have made a mistake. One you will regret. I am the wife of the \\nGreat Khal.\\nTRANSLATION\\nYeri ray esh osoon. Ki jini yeri akhezhi. Anha chiorikem Khali \\nVezhvena.\\nPHONETIC\\nYE-ri RAY ESH o-so-ON. ki JI-ni ye-ri A-khe-zhi. AN-ha chi-o-ri-KEM \\nKHA-li VEZH-ve-na.\\nYou-all have left (the) path. (Expression.) By this you-all will-be-\\nsorry. I (am) wife of-Khal Great.\\nCONTINUED:3.7 3.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     125.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 125, 'page_label': '126'}, page_content='CONTINUED: (2)3.7 3.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     126.\\n-------------------------------------------------------------------\\n(DKPRIESTESS_603_6.mp3)\\nDOSH KHALEEN PRIESTESS\\nWe know who you are. I remember you eating the stallion’s heart. \\n(beat) Why didn’t you come to us after Khal Drogo died?\\nTRANSLATION\\nKisha shilaki yera. Anha vineserak meyer adakh zhores vezhoon. \\n(beat) Kifindirgi yer vos esso kishaan irge Khal Drogo drivo?\\nPHONETIC\\nKI-sha SHI-la-ki ye-ra. AN-ha vi-ne-se-RAK me-yer a-DAKH zho-RES ve-\\nzho-ON. (beat) KI-fin-dir-gi yer vos ES-so ki-sha-AN ir-ge KHAL DRO-\\ngo DRI-vo?\\nWe know you. I remember that-you ate (the) heart from-the-stallion. \\n(beat) Why you not came to-us after Khal Drogo died?\\n-------------------------------------------------------------------\\n(DANY_603_7.mp3)\\nDANY\\nBecause I am Daenerys Stormborn, the Breaker of Chains, the Queen of \\nMeereen and the Mother of Dragons. My place is not here with you.\\nTRANSLATION\\nHajinaan m’anha Deneris Vazyol, Haggey-Assamvak, Khaleesi Mirini ma \\nMai Zhavorsi. Vaes anni vos jinne ma shafkoa.\\nPHONETIC\\nha-ji-na-AN MAN-ha de-NE-ris vaz-YOL, hag-GEY-as-sam-VAK, KHA-le-e-\\nsi MI-ri-ni ma MAI zha-VOR-si. va-ES an-ni VOS JIN-ne ma shaf-ko-a.\\nBecause that-I (am) Daenerys Stormborn, Chain-Breaker, Queen of-\\nMeereen, and Mother of-Dragons. Place my (is) not here with you-all.\\n-------------------------------------------------------------------\\n(DKPRIESTESS_603_8.mp3)\\nDOSH KHALEEN PRIESTESS\\nMaybe you’re right. (beat) You were the wife of the Great Khal. You \\nthought he would conquer the world with you at his side. You thought \\nyou would give birth to the Stallion Who Mounts the World. (beat) He \\ndidn’t. You didn’t. (beat) I was the wife of the Great Khal.\\nCONTINUED: (2)3.7 3.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     126.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 126, 'page_label': '127'}, page_content='CONTINUED: (3)3.7 3.7\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     127.\\nTRANSLATION\\nIshish yer jili. (beat) Yer chiorikemoon Khali Vezhvena. Yer shillo \\nmemé vassila rhaesheseres ma yeroon qisi. Yer shillo meyer vayyoli \\nVezhes Fin Saja Rhaesheseres. (beat) Me vo to. Yer vo to. (beat) \\nAnha chiorikemoon Khali Vezhvena.\\nPHONETIC\\ni-SHISH yer JI-li. (beat) yer chi-o-ri-ke-mo-ON KHA-li VEZH-ve-na. \\nyer SHIL-lo me-ME VAS-si-la rha-e-she-se-RES ma ye-ro-ON QE-si. yer \\nSHIL-lo me-YER VAY-yo-li ve-ZHES fin SA-ja rha-e-she-se-RES. (beat) \\nme vo TO. yer vo TO. (beat) AN-ha chi-o-ri-ke-mo-ON KHA-li VEZH-ve-\\nna.\\nMaybe you are-right. (beat) You were-wife of-Khal Great. You \\nbelieved that-he would-conquer (the) world with you near-at-hand. \\nYou believed that-you would-birth (the) Stallion Who Mounts (the) \\nWorld. (beat) He not didn’t. You not didn’t. (beat) I was-wife of-\\nKhal Great.\\n-------------------------------------------------------------------\\n(DKPRIESTESS_603_9.mp3)\\nDOSH KHALEEN PRIESTESS (CONT’D)\\nKhal Savo. I thought he would conquer the world with me at his side. \\nHave you heard the stories about him? No. You haven’t. (beat) You’re \\nyoung. We were all young, once. Some of us still are. But we all \\nunderstand the way things are. You will learn as well, if you are \\nfortunate enough to stay with us.\\nTRANSLATION\\nKhal Savo. Anha shillo memé vassila rhaesheseres m’anhoon qisi. Hash \\nyer ray char astosoris mae? Vos. Yer vo charo mora vosecchi. (beat) \\nYer imeshi. Ei kisha imeshish, kash kashi. Loy kishi zin imeshaki. \\nVosma ei kisha ray tihosh os fin onqotha enossho. Yer atihoe akka, \\nhash shieraki gori ha yeraan ma yer avikovareri ma kishoon.\\nPHONETIC\\nKHAL SA-vo. AN-ha SHIL-lo me-ME VAS-si-la rha-e-she-se-RES man-ho-ON \\nQE-si. HASH yer ray CHAR as-to-RIS ma-e? VOS. yer VO CHA-ro mo-ra vo-\\nSECH-chi. (beat) YER i-me-SHISH. E-i ki-sha I-me-shi, kash KA-shi. \\nLOY ki-shi ZIN I-me-sha-ki. vos-ma E-i ki-sha ray ti-HOSH OS fin ON-\\nqo-tha e-NOSH-sho. yer A-ti-ho-e AK-ka, hash SHI-e-ra-ki GO-ri ha ye-\\nra-AN ma yer A-vi-ko-va-re-ri ma ki-sho-ON.\\nKhal Savo. I believed that-he would-conquer (the) world with-me near-\\nat-hand. Have you ever heard (the) stories of-him? No. You not heard \\nthem never. (beat) You are-young. All we were-young, during a-time. \\nSome of-us still are-young. But all we already understand (the) path \\nCONTINUED: (3)3.7 3.7\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     127.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 127, 'page_label': '128'}, page_content=\"CONTINUED: (4)3.7 3.7\\nPHONETIC (CONT'D)\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     128.\\nthat walks (the) mule. (Expression.) You will-understand as-well, if \\n(the) stars are-charging for you (expression) and you stay with us.\\n-------------------------------------------------------------------\\n(DANY_603_10.mp3)\\nDANY\\nWhere else would I go? Every khaleesi becomes Dosh Khaleen.\\nTRANSLATION\\nFinnaan anha vek? Ei khaleesi Doshaan Khaleen.\\nPHONETIC\\nfin-na-AN an-ha VEK? E-i kha-le-e-si do-sha-AN kha-le-EN.\\nWhere-else I would-go? Every khaleesi become-Dosh Khaleen.\\n-------------------------------------------------------------------\\n(DKPRIESTESS_603_11.mp3)\\nDOSH KHALEEN PRIESTESS\\nYes. Immediately after the death of their khal. But you did not. You \\nwent out into the world. That is forbidden. (beat) All the khalasars \\nhave returned to the ancient city for the Khalar Vezhven, where they \\nwill meet to discuss their plans for the coming winter. They will \\ndecide which cities will be sacked, which tribes will be enslaved, \\nand which ones destroyed. And now they must decide what to do with \\nKhal Drogo’s silver-haired widow. (beat) With luck, your place will \\nbe here with us, Mother of Dragons. It is the best you can hope for, \\nnow.\\nTRANSLATION\\nSek. Irge leshiti athdrivari khali mae. Vosma yer vos. Yer ver yomme \\nrhaesheser. Reki izvena. (beat) Ei khalasari ray essash vaesaan \\nershe haji Khalaroon Vezhvena, fini ashiloe mori ajerie ostirge mori \\nhaji aheshkoon sila meshes. Mori avokkeri ma fin vaes nem vemrasoe, \\nma fin yanqosori nem vazzafroe, ma fini nem vohhari. Ma ajjin \\nmori’th vokkeri fasqoy khaleeni ma norethoon vizhada Khali Drogo. \\n(beat) Hash shieraki gori, hash vaes yeri jinnaan ma kishoon, zhey \\nMai Zhavorsi. Me fasqoyi avezhvenanaz fin laz zali yer, ajjinoon.\\nPHONETIC\\nSEK. IR-ge LE-shi-ti ATH-dri-va-ri KHA-li ma-e. vos-ma YER VOS. yer \\nVER yom-me rha-e-she-SER. RE-ki IZ-ve-na. (beat) E-i KHA-la-sa-ri \\nray es-SASH va-e-sa-AN ER-she ha-ji kha-la-ro-ON VEZH-ve-na, fi-ni A-\\nshi-lo-e mo-ri A-je-ri-e OS-tir-ge mo-ri ha-ji a-hesh-ko-ON SI-la me-\\nSHES. MO-ri A-vok-ke-ri ma FIN va-ES nem VEM-ra-so-e, ma FIN YAN-qo-\\nso-ri nem VAZ-za-fro-e, ma FI-ni nem VOH-ha-ri. ma aj-JIN mo-rith \\nVOK-ke-ri fas-QOY KHA-le-e-ni ma no-re-tho-ON VI-zha-da KHA-li DRO-\\nCONTINUED: (4)3.7 3.7\\nPHONETIC (CONT'D)\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     128.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 128, 'page_label': '129'}, page_content=\"CONTINUED: (5)3.7 3.7\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     129.\\ngo. (beat) HASH SHI-e-ra-ki GO-ri, hash va-ES ye-ri jin-na-AN ma ki-\\nsho-ON, zhey MAI zha-VOR-si. me FAS-qo-yi a-vezh-ve-na-NAZ fin laz \\nZA-li yer, aj-ji-no-ON.\\nYes. After (the) second of-death of-Khal of-hers. But you not. You \\ntraveled across (the) world. That is-forbidden. (beat) All khalasars \\nalready returned to-city ancient because-of Khalar Vezhven, that \\nmeet they and-will-discuss plans their for winter next thereat. They \\nwill-decide and which cities will-be sacked, and which tribes will-\\nbe enslaved, and which will-be destroyed. And now they-must decide \\n(the) fate of-widow with hair silver of-Khal Drogo. (beat) If (the) \\nstars are-charging (expression), then place your will-be-here with \\nus, O Mother of-Dragons. It (is) fate best that can hope-for you, \\nnow.\\n-------------------------------------------------------------------\\nGAME OF THRONES #604\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/03/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. VAES DOTHRAK - NIGHT4.6 4.6\\n(IGGO_604_8.mp3)\\nIGGO\\nYou bring any girls home from Saath?\\nCONTINUED: (5)3.7 3.7\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     129.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 129, 'page_label': '130'}, page_content='CONTINUED:4.6 4.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     130.\\nTRANSLATION\\nYer fich jinnaan loy nayati Saathoon?\\nPHONETIC\\nyer FICH jin-na-AN loy NA-ya-ti sa-a-tho-ON?\\nYou bring to-here any girls from-Saath?\\n-------------------------------------------------------------------\\n(AKHO_604_9.mp3)\\nAKHO\\nMm, a little beauty with red hair.\\nTRANSLATION\\nMm, zheanish norethqoyi.\\nPHONETIC\\nMM, zhe-a-NISH NO-reth-qo-yi.\\nMm, little-beauty of-red-hair.\\n-------------------------------------------------------------------\\n(IGGO_604_10.mp3)\\nIGGO\\nWhat if you have a son with red hair?\\nTRANSLATION\\nMa hash rizh yeri norethqoyik?\\nPHONETIC\\nma HASH RIZH ye-ri no-reth-qo-YIK?\\nAnd if son your (is) redhead?\\n-------------------------------------------------------------------\\n(AKHO_604_11.mp3)\\nAKHO\\nI’ll throw him in the river.\\nTRANSLATION\\nAnha vovvethak mae ashefasaan.\\nCONTINUED:4.6 4.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     130.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 130, 'page_label': '131'}, page_content='CONTINUED: (2)4.6 4.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     131.\\nPHONETIC\\nAN-ha vov-ve-THAK ma-e a-she-fa-sa-AN.\\nI will-throw him in-the-river.\\n-------------------------------------------------------------------\\n(JORAH_604_12.mp3)\\nJORAH\\nMy friends... We wandered off from the Western Market and got lost. \\nCould you show us the way back?\\nTRANSLATION\\nZhey okeosi anni... Kisha nemo silish Jereseroon Jima majin leisosh. \\nHash shafka laz idrie kisha rekkaan akka?\\nPHONETIC\\nZHEY O-ke-o-si an-ni... KI-sha ne-mo si-LISH je-re-se-ro-ON JI-ma ma-\\njin le-i-SOSH. HASH shaf-ka laz I-dri-e ki-sha rek-ka-AN AK-ka?\\nO friends my... We ourselves followed from-Market Western and got-\\nlost. Is-it you can guide us to-there again?\\n-------------------------------------------------------------------\\n(AKHO_604_13.mp3)\\nAKHO\\nWhat do you sell?\\nTRANSLATION\\nFin yer vijereri?\\nPHONETIC\\nFIN yer VI-je-re-ri?\\nWhat you sell?\\n-------------------------------------------------------------------\\n(JORAH_604_14.mp3)\\nJORAH\\nWine. Come down to my stall tomorrow, I’ll give a cask of the \\nArbor’s finest.\\nCONTINUED: (2)4.6 4.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     131.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 131, 'page_label': '132'}, page_content='CONTINUED: (3)4.6 4.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     132.\\nTRANSLATION\\nSewafikh. Jadi vaesaan anni asshekh, anha vazhak shafkea khogare  \\navezhvenanaza Halahisiroon.\\nPHONETIC\\nse-wa-FIKH. JA-di va-e-sa-AN an-ni ash-SHEKH, an-ha va-ZHAK shaf-ke-\\na KHO-ga-re A-vezh-ve-na-na-za ha-la-hi-si-ro-ON.\\nWine. Come to-stall my tomorrow, I will-give to-you cask finest from-\\nthe-Arbor.\\n-------------------------------------------------------------------\\n(AKHO_604_15.mp3)\\nAKHO\\nYou’re not merchants. (to Iggo) Get the others.\\nTRANSLATION\\nYeri vos jeraki. (to Iggo) Hales eshnakis.\\nPHONETIC\\nYE-ri VOS JE-ra-ki. (to Iggo) ha-LES esh-na-KIS.\\nYou-all (are) not merchants. (to Iggo) Get (the) others.\\n-------------------------------------------------------------------\\n(IGGO_604_16.mp3)\\nIGGO\\nHelp! Help!\\nTRANSLATION\\nRhelas! Rhelas!\\nPHONETIC\\nrhe-LAS! rhe-LAS!\\nHelp! Help!\\n-------------------------------------------------------------------\\nINT. TEMPLE OF THE DOSH KHALEEN - NIGHT4.7 4.7\\n(HPRIESTESS_604_17.mp3)\\nCONTINUED: (3)4.6 4.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     132.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 132, 'page_label': '133'}, page_content='CONTINUED:4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     133.\\nHIGH PRIESTESS\\nSome of them don’t think Dothraki should breed with foreigners. They \\ndon’t think the blood should be diluted.\\nTRANSLATION\\nLoy mori vo dirgi me-Dothraki jif vigoveri ifakis. Mori vo dirgi \\nmeqoy jifim ayyoza.\\nPHONETIC\\nLOY mo-ri VO DIR-gi me-DOTH-ra-ki jif VI-go-ve-ri i-fa-KIS. MO-ri vo \\nDIR-gi me-QOY ji-fim AY-yo-za.\\nSome of-them don’t think that-Dothraki should breed-with foreigners. \\nThey don’t think that-blood should-be diluted.\\n-------------------------------------------------------------------\\n(HPRIESTESS_604_18.mp3)\\nHIGH PRIESTESS\\nThey are stupid old women. They don’t realize that we have always \\ndiluted our blood. We conquer a people, we take the best women, they \\nbear us children. That’s how we stay strong.\\nTRANSLATION\\nMori vikeesisi toki. Mori vo tihoo mekisha ray ayyoz qoy kishoon \\nayyeyoon. Kisha assilaki yanqosores, kisha qoraki chiories \\nadavranazi, mori ayyoe yal kishaan. Kijinosi kisha zin hajaki.\\nPHONETIC\\nMO-ri VI-ke-e-si-si TO-ki. MO-ri VO TI-ho-o me-KI-sha ray ay-YOZ QOY \\nki-sho-ON ay-ye-yo-ON. KI-sha AS-si-la-ki yan-qo-so-RES, KI-sha QO-\\nra-ki chi-o-ri-ES A-dav-ra-na-zi, MO-ri AY-yo-e YAL ki-sha-AN. KI-ji-\\nno-si KI-sha ZIN HA-ja-ki.\\nThey (are) nags stupid. They don’t realize that-we have diluted \\nblood our from-forever. We conquer people, we claim women best, they \\nbear children to-us. By-this we remain strong.\\n-------------------------------------------------------------------\\n(HPRIESTESS_604_19.mp3)\\nHIGH PRIESTESS\\nThis one is Lhazareen. Her Khal found her hiding in a well after he \\nburned her village. How old were you?\\nTRANSLATION\\nJinak Lazari. Khal mae ez mae aresaya she dirke irge memé avvirsa \\nvaesish mae nakhaan. Fini thirisir yeri arrek?\\nCONTINUED:4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     133.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 133, 'page_label': '134'}, page_content='CONTINUED: (2)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     134.\\nPHONETIC\\nji-NAK LA-za-ri. KHAL ma-e EZ ma-e A-re-sa-ya she DIR-ke ir-ge me-ME \\nAV-vir-sa va-e-SISH ma-e na-kha-AN. FI-ni thi-ri-SIR ye-ri ar-REK?\\nThis-one Lhazareen. Khal her found her hiding in well after that-he \\nburned village her to-the-ground. What (was) age your then?\\n-------------------------------------------------------------------\\n(ORNELA_604_20.mp3)\\nORNELA\\n12.\\nTRANSLATION\\nAkatthi.\\nPHONETIC\\na-KATH-thi.\\nTwelve.\\n-------------------------------------------------------------------\\n(HPRIESTESS_604_21.mp3)\\nHIGH PRIESTESS\\nA year later, she bore her Khal a daughter. He must have been so \\nhappy. How did he show his happiness?\\nTRANSLATION\\nIrge firesofi, me ayyo khalaan mae ohar. Me’th allayaf mae sekosshi. \\nKifinosi me qach athlayafar mae?\\nPHONETIC\\nIR-ge FI-re-so-fi, me AY-yo kha-la-AN ma-e o-HAR. METH al-la-YAF ma-\\ne se-KOSH-shi. KI-fi-no-si me qach ath-la-ya-FAR ma-e?\\nAfter (a) year, she bore khal her (a) daughter. It-must have-pleased \\nhim indeed. How he displayed happiness his?\\n-------------------------------------------------------------------\\n(ORNELA_604_22.mp3)\\nORNELA\\nHe broke my ribs.\\nCONTINUED: (2)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     134.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 134, 'page_label': '135'}, page_content='CONTINUED: (3)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     135.\\nTRANSLATION\\nMe assamve serje.\\nPHONETIC\\nme as-SAM-ve SER-je.\\nHe broke ribs (of me).\\n-------------------------------------------------------------------\\n(HPRIESTESS_604_23.mp3)\\nHIGH PRIESTESS\\nWe are not queens, here. We do not conquer cities or ride dragons. \\nBut do not despair. The khals depend on us for our wisdom. Our lives \\nhave meaning. And we have each other.\\nTRANSLATION\\nKisha vos khaleesisi jinne. Kisha vos ch’assiloki vaes che dothroki \\nzhavors. Vosma vo khezhos. Khali jadi kishaan haji athvillaroon. \\nAtthirar kishi annevae shorhae. Ma kisha ma kishoon akka.\\nPHONETIC\\nKI-sha VOS KHA-le-e-si-si jin-ne. KI-sha VOS CHAS-si-lo-ki va-ES che \\nDO-thro-ki zha-VORS. vos-ma VO khe-ZHOS. KHA-li JA-di ki-sha-AN ha-\\nji ath-vil-la-ro-ON. ath-thi-RAR ki-shi AN-ne-va-e SHO-rha-e. ma KI-\\nsha ma ki-sho-ON AK-ka.\\nWe not queens here. We not or-conquer cities or ride dragons. But \\ndon’t despair. Khals come to-us for wisdom. Lives our leave \\nfootprints. (Expression) And we (are) with ourselves also.\\n-------------------------------------------------------------------\\n(DANY_604_24.mp3)\\nDANY\\nThat is more than most have.\\nTRANSLATION\\nHazi ale khadosoon.\\nPHONETIC\\nHA-zi A-le kha-do-so-ON.\\nThat (is) more than-the-bulk (of people).\\n-------------------------------------------------------------------\\nCONTINUED: (3)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     135.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 135, 'page_label': '136'}, page_content='CONTINUED: (4)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     136.\\n(HPRIESTESS_604_25A.mp3)\\nHIGH PRIESTESS\\nWhen the Khals meet for the Khalar Vezhven, I hope they let you live \\nout your days with us. The other possibilities are not so pleasant.\\nTRANSLATION\\nKash Khali jadoe haji Khalaroon Vezhvena, kash anha zalak memori \\nazhi yeraan thirat asshekhis yeri nakhaan ma kishoon. Eshna osi vo \\nlaini vosso.\\nPHONETIC\\nkash KHA-li JA-do-e ha-ji kha-la-ro-ON VEZH-ve-na, KASH an-ha za-LAK \\nme-MO-ri A-zhi ye-ra-AN thi-RAT ash-she-KHIS ye-ri na-kha-AN ma ki-\\nsho-ON. ESH-na O-si VO LA-i-ni VOS-so.\\nWhen Khals meet for Khalar Vezhven, then I hope that-they give to-\\nyou to-live days your to-the-end with us. Other possibilities not \\nnice very.\\n-------------------------------------------------------------------\\n(DANY_604_26A.mp3)\\nDANY\\nAnd the Khalar Vezhven is tomorrow night? (off the Priestess’ nod) I \\nneed to make water.\\nTRANSLATION\\nMa Khalar Vezhven silokh sh’ajjalan? (off the Priestess’ nod) Anha \\nzigerek athnavaroon.\\nPHONETIC\\nma kha-LAR vezh-VEN si-LOKH shaj-ja-LAN? (off the Priestess’s nod) \\nAN-ha zi-ge-REK ath-na-va-ro-ON.\\nAnd Khalar Vezhven (is) tomorrow at-night? (off the Priestess’ nod) \\nI need to-make-water.\\n-------------------------------------------------------------------\\n(HPRIESTESS_604_27.mp3)\\nHIGH PRIESTESS\\nYou can’t run from Dothraki. You know this.\\nTRANSLATION\\nYer laz vo choqi Dothrakoa vosecchi. Yer nesi jin.\\nCONTINUED: (4)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     136.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 136, 'page_label': '137'}, page_content='CONTINUED: (5)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     137.\\nPHONETIC\\nYER laz VO CHO-qe DO-thra-ko-a vo-SECH-chi. yer NE-si jin.\\nYou can not flee from-Dothraki never. You know this.\\n-------------------------------------------------------------------\\n(DANY_604_28.mp3)\\nDANY\\nI will never run from Dothraki.\\nTRANSLATION\\nAnha vos ochoqak Dothrakoa vosecchi.\\nPHONETIC\\nAN-ha VOS o-cho-QAK DO-thra-ko-a vo-SECH-chi.\\nI not will-flee from-Dothraki never.\\n-------------------------------------------------------------------\\n(HPRIESTESS_604_29.mp3)\\nHIGH PRIESTESS\\nGo, show her.\\nTRANSLATION\\nEs, idris mae.\\nPHONETIC\\nES, i-DRIS ma-e.\\nGo, show her.\\n-------------------------------------------------------------------\\nEXT. TEMPLE OF THE DOSH KHALEEN - NIGHT4.8 4.8\\n(HPRIESTESS_604_30.mp3)\\nDANY\\nI needed fresh air.\\nTRANSLATION\\nAnha zigere yash chosha.\\nCONTINUED: (5)4.7 4.7\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     137.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 137, 'page_label': '138'}, page_content='CONTINUED:4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     138.\\nPHONETIC\\nAN-ha ZI-ge-re YASH CHO-sha.\\nI needed air fresh.\\n-------------------------------------------------------------------\\n(DANY_604_31.mp3)\\nDANY\\nThe old women stink.\\nTRANSLATION\\nYesisi vachrari.\\nPHONETIC\\nYES-si-si VACH-ra-ri.\\n(The) old-women stink.\\n-------------------------------------------------------------------\\n(ORNELA_604_32.mp3)\\nORNELA\\nThey do stink.\\nTRANSLATION\\nMori vachrari sekosshi.\\nPHONETIC\\nMO-ri VACH-ra-ri se-KOSH-shi.\\nThey stink do.\\n-------------------------------------------------------------------\\n(DANY_604_33.mp3)\\nDANY\\nYou must have been very young when your Khal died.\\nTRANSLATION\\nMori’th samvenosh kash Khal yeri drivo, jin firesof yeri.\\nCONTINUED:4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     138.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 138, 'page_label': '139'}, page_content='CONTINUED: (2)4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     139.\\nPHONETIC\\nMO-ri’th sam-ve-NOSH kash KHAL ye-ri DRI-vo, jin fi-re-SOF ye-ri.\\nThey-must not-have-been-many when khal your died, those years your.\\n-------------------------------------------------------------------\\n(ORNELA_604_34.mp3)\\nORNELA\\nSixteen.\\nTRANSLATION\\nZhindatthi.\\nPHONETIC\\nzhin-DATH-thi.\\nSixteen.\\n-------------------------------------------------------------------\\n(DANY_604_35A.mp3)\\nDANY\\nToo bad he didn’t die sooner.\\nTRANSLATION\\nMe ohazha memé vo drivo k’athaqisinari.\\nPHONETIC\\nme O-ha-zha me-ME VO DRI-vo KA-tha-ke-si-na-ri.\\nIt is-too-bad that-he not died sooner.\\n-------------------------------------------------------------------\\n(ORNELA_604_36A.mp3)\\nORNELA\\nYes, too bad.\\nTRANSLATION\\nSek, me ohazha.\\nCONTINUED: (2)4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     139.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 139, 'page_label': '140'}, page_content='CONTINUED: (3)4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     140.\\nPHONETIC\\nSEK, me O-ha-zha.\\nYes, it is-too-bad.\\n-------------------------------------------------------------------\\n(ORNELA_604_37.mp3)\\nORNELA\\nIs it true you have three dragons?\\nTRANSLATION\\nHash me jila, jin sen zhavorsi mra qora?\\nPHONETIC\\nHASH ME JI-la, jin SEN zha-VOR-si mra qo-ra?\\nIs it true, this three dragons (you’re) having?\\n-------------------------------------------------------------------\\n(ORNELA_604_38.mp3)\\nORNELA\\nAnd they breathe fire?\\nTRANSLATION\\nMa mori leshita ki vorsasi?\\nPHONETIC\\nma mo-ri LE-shi-ta ki VOR-sa-si?\\nAnd they breathe by fire?\\n-------------------------------------------------------------------\\n(DANY_604_39.mp3)\\nDANY\\nThey do. Would you like to see them one day?\\nTRANSLATION\\nMori ti kijinosi sekosshi. Hash me vallayafa yera tihat mora \\nhezhahhe?\\nPHONETIC\\nMO-ri TI ki-ji-no-si se-KOSH-shi. HASH me VAL-la-ya-fa ye-ra ti-HAT \\nmo-ra he-ZHAH-he?\\nCONTINUED: (3)4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     140.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 140, 'page_label': '141'}, page_content='CONTINUED: (4)4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     141.\\nThey do like-that indeed. Would it please you to-see them one-day?\\n-------------------------------------------------------------------\\n(ORNELA_604_40.mp3)\\nORNELA\\nI am Dosh Khaleen. I can never leave Vaes Dothrak, unless I rise as \\nsmoke from the pyre on the day I die.\\nTRANSLATION\\nAnha Dosh Khaleen. Anha laz vos odinak Vaesoon Dothrak vosecchi, \\nvosm’anha ayothak vorsqoyoon ven fih kash anha adrivok.\\nPHONETIC\\nAN-ha DOSH kha-le-EN. AN-ha laz VOS o-di-NAK va-e-so-ON do-THRAK vo-\\nSECH-chi, vos-MAN-ha a-yo-THAK vors-qo-yo-ON ven FIH kash AN-ha a-\\ndri-VOK.\\nI (am) Dosh Khaleen. I can not leave from-Vaes Dothrak never, unless-\\nI will-rise from-funeral-pyre as smoke when I will-die.\\n-------------------------------------------------------------------\\n(DANY_604_41.mp3)\\nDANY\\nAnd you... Have faith in me, khaleesi. Do not betray me.\\nTRANSLATION\\nMa yer... Qothas k’anni, zhey khaleesi. Vos yer nem holos anhoon.\\nPHONETIC\\nma YER... qo-THAS KAN-ni, zhey KHA-le-e-si. VOS yer nem ho-LOS an-ho-\\nON.\\nAnd you... Have-faith by-me, O khaleesi. Not you be blown from-me. \\n(Expression)\\n-------------------------------------------------------------------\\nINT. AUDIENCE CHAMBER - CONTINUOUS4.12 4.12\\n(ASH_604_1.mp3)\\nASH\\nThey shouldn’t have even been allowed to walk our streets. It’s an \\ninsult.\\nCONTINUED: (4)4.8 4.8\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     141.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 141, 'page_label': '142'}, page_content='CONTINUED:4.12 4.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     142.\\nTRANSLATION\\nTha yenka onya mazmedha rual fendha yelwa khil. Sa ánghowa.\\nPHONETIC\\nTHA yen-ka ON-ya maz-ME-dha ru-AL FEN-dha yel-wa KHIL. sa AN-gho-wa.\\nNot they-should have been allowed to-walk our streets. It-is (an) \\ninsult.\\n-------------------------------------------------------------------\\n(KESH_604_2.mp3)\\nKESH\\nI’d gladly have slit their throats before they made it through our \\ngates, but let’s hear [why they’re here first].\\nTRANSLATION\\nKrenyikhé unyishishk nyetodha poj irosh nyeshka majij ya yelwa rim, \\ni riwij [shkurja li kijil yelkhé].\\nPHONETIC\\nkren-yi-KHE un-yi-SHISHK nye-TO-dha poj I-rosh nyesh-ka ma-JIJ ya \\nYEL-wa RIM, i ri-WIJ [SHKUR-ja li KI-jil yel-KHE].\\nGlady I-would-have slit their throats before they-came through our \\ngates, but let’s-hear [why they-are here first].\\n-------------------------------------------------------------------\\n(TYRION_604_3.mp3)\\nTYRION\\nMy friends! To apologize for you to wait!\\nTRANSLATION\\nNya roqirossa! Usovegon jemo syt jumbagon!\\nPHONETIC\\nNI-a ro-ki-ROS-sa! u-SO-ve-gon JE-mo-sit JUM-ba-gon!\\nMy friens! To-apologize for-you to-wait!\\n-------------------------------------------------------------------\\n(ASH_604_4.mp3)\\nCONTINUED:4.12 4.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     142.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 142, 'page_label': '143'}, page_content='CONTINUED: (2)4.12 4.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     143.\\nASH\\nAnd you, Torgo Nudho? You want to drink wine with these men? The men \\nwho tore us from our mothers’ arms and sold us at auction, like \\ncattle?\\nTRANSLATION\\nShe a, Thorgha Nudha? Ev shka moz avrelya fej wal thosh? Pa wal yel \\nwazghesh shing pa nesh esh yelwa mish she yel lerch ej rovnya sha \\nnofel?\\nPHONETIC\\nshe A, THOR-gha NU-dha? EV shka MOZ av-REL-ya FEJ WAL thosh? pa WAL \\nyel waz-GHESH shing pa NESH esh YEL-wa MISH she yel LERCH ej ROV-nya \\nsha NO-fel?\\nAnd you, Grey Worm? You-want that you-drink wine these men with? The \\nmen us tore from the arms of our mothers and us sold at-the auction \\nlike cattle?\\n-------------------------------------------------------------------\\n(GREYWORM_604_5.mp3)\\nGREY WORM\\nI am a soldier, not a politician. But if there is a chance for peace \\n-- a just peace -- we should take it.\\nTRANSLATION\\nNyk skan minty, do jovenne. Y lu honesk ji kelnisto eji lysk -- me \\ndreji lysk -- inki zer jéragho.\\nPHONETIC\\nNIK skan MIN-ti, DO jo-VEN-ne. i lu HO-nesk ji kel-NIS-to e-ji LISK -\\n- me DRE-ji LISK -- IN-ki zer JE-ra-gho.\\nI am soldier, not politician. But if there’s the chance of-the peace \\n-- a true peace -- we-should it take.\\n-------------------------------------------------------------------\\n(KESH_604_6.mp3)\\nKESH\\nMissandei, you know what these men are. How can you trust them?\\nTRANSLATION\\nMishanje, khim shkul she fej wal. Shkokhé koth pong paza?\\nPHONETIC\\nmi-SHAN-je, KHIM shkul she FEJ WAL. shko-KHE koth pong PA-za?\\nCONTINUED: (2)4.12 4.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     143.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 143, 'page_label': '144'}, page_content='CONTINUED: (3)4.12 4.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     144.\\nMissandei, you-know what are these men. How can-you them trust?\\n-------------------------------------------------------------------\\n(MISSANDEI_604_7.mp3)\\nMISSANDEI\\nI do not trust them. I will never trust them. But as a wise man once \\nsaid, “We make peace with our enemies, not our friends.”\\nTRANSLATION\\nDo pon pazan. Dori pon pazozlivan. Y he sylvie vala mere ivetredas, \\n“Verdi ji lysk ilvi qrinuntys zy, do ilvi rageros zy.”\\nPHONETIC\\nDO pon PA-zan. DO-ri pon pa-zoz-LI-van. i he SIL-vi-e VA-la me-re i-\\nve-TRE-das, “VER-di ji LISK il-vi kri-NUN-tis-zi, DO il-vi ra-GE-ros-\\nzi.\\nNot them I-trust. Never them I-will-trust. But as wise man once \\nsaid, “We-make the peace our enemies for, not our friend for.”\\n-------------------------------------------------------------------\\nINT. TEMPLE OF THE DOSH KHALEEN - NIGHT4.29 4.29\\n(GREENKHAL_604_42.mp3)\\nGREEN KHAL\\nYour horses trampled my man Iggo. He was better at healing horses \\nthan any man in my khalasar.\\nTRANSLATION\\nHrazef shafki nokittish mahrazhes anni zhey Iggo. Me akkoal hrazef \\nk’athadavranazi ei mahrazhoa khalasari anni.\\nPHONETIC\\nhra-ZEF shaf-ki no-kit-TISH mah-ra-ZHES an-ni zhey IG-go. me ak-ko-\\nAL hra-ZEF K’A-tha-dav-ra-na-zi E-i MAH-ra-zho-a KHA-la-sa-ri an-ni.\\nHorses your trampled man my named Iggo. He healed horses better \\nevery than-man of-khalasar my.\\n-------------------------------------------------------------------\\n(MORO_604_43.mp3)\\nCONTINUED: (3)4.12 4.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     144.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 144, 'page_label': '145'}, page_content='CONTINUED:4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     145.\\nMORO\\nThey also trampled my man Akho. Even worse, I lost two mares and a \\nstallion. What’s worth more: two mares and a stallion, or two men \\ndumb enough to get trampled by horses? Fuck them both. You should \\nthank my horses.\\nTRANSLATION\\nMori nokittish akka mahrazhes anni zhey Akho. K’athodavranari, anha \\naran m’akat lamees ma vezhes. Fini adavrana: che m’akat lamesi ma \\nvezh che akat mahrazhi ven toki ven mori nem nokittish ki hrazefi? \\nHiles mora nakhaan. Shafka jif hoeri hrazef anni.\\nPHONETIC\\nMO-ri no-kit-TISH AK-ka mah-ra-ZHES an-ni zhey A-kho. KA-tho-dav-ra-\\nna-ri, an-ha a-RAN ma-KAT la-me-ES ma ve-ZHES. FI-ni A-dav-ra-na: \\nche ma-KAT LA-me-si ma VEZH che a-KAT MAH-ra-zhi ven TO-ki ven MO-ri \\nnem no-kit-TISH ki HRA-ze-fi? hi-LES mo-ra na-kha-AN. SHAF-ka jif HO-\\ne-ri hra-ZEF an-ni.\\nThey trampled also man my named Akho. By-worseness, I lost and-two \\nmares and stallion. What is-more-valuable: or and-two mares and \\nstallion or two men as stupid as they were trampled by horses? Fuck \\nthem to-the-ground. You should praise horses my.\\n-------------------------------------------------------------------\\n(FORZHO_604_70A.mp3)\\nFORZHO\\nIt is forbidden to spill blood in the sacred city.\\nTRANSLATION\\nMe izvena, jin athaqqiyazar she vaesof.\\nPHONETIC\\nme IZ-ve-na, jin a-thaq-qe-ya-ZAR she va-e-SOF.\\nIt is-forbidden, this spilling-blood in sacred-city.\\n-------------------------------------------------------------------\\n(MORO_604_78.mp3)\\nMORO\\nIt is forbidden to carry weapons in the sacred city.\\nTRANSLATION\\nMe izvena, jin athkessezar az she vaesof.\\nCONTINUED:4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     145.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 145, 'page_label': '146'}, page_content='CONTINUED: (2)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     146.\\nPHONETIC\\nme IZ-ve-na jin ath-kes-se-ZAR AZ she va-e-SOF.\\nIt is-forbidden, this carrying weapons in sacred-city.\\n-------------------------------------------------------------------\\n(FORZHO_604_79.mp3)\\nFORZHO\\nSo we don’t spill blood!\\nTRANSLATION\\nMajin kisha vos addrekoki qoy moon!\\nPHONETIC\\nma-JIN KI-sha VOS AD-dre-ko-ki QOY mo-ON!\\nSo we don’t spill blood from-him!\\n-------------------------------------------------------------------\\n(MORO_604_80.mp3)\\nMORO\\nWell... There’s always a little blood.\\nTRANSLATION\\nHazaan... Loyi qoyi avekha ayyey.\\nPHONETIC\\nha-za-AN... LO-yi qo-yi A-ve-kha ay-YEY.\\nWell... Some blood there-will-be always.\\n-------------------------------------------------------------------\\n(FORZHO_604_81.mp3)\\nFORZHO\\nSomeone crushed his head with a rock--\\nTRANSLATION\\nAto kaffe nharees moon ki negwini--\\nCONTINUED: (2)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     146.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 146, 'page_label': '147'}, page_content='CONTINUED: (3)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     147.\\nPHONETIC\\nA-to KAF-fe nha-re-ES mo-on ki NE-gwi-ni--\\nSomeone crushed head from-him by rock--\\n-------------------------------------------------------------------\\n(BROZHO_604_71.mp3)\\nBROZHO\\nAggo was not killed with a blade.\\nTRANSLATION\\nAggo nem vos addrivo k’azi vosecchi.\\nPHONETIC\\nAG-go nem VOS AD-dri-vo KA-zi vo-SECH-chi.\\nAggo was not killed by-blade never.\\n-------------------------------------------------------------------\\n(FORZHO_604_72.mp3)\\nFORZHO\\nBut his blood was spilled.\\nTRANSLATION\\nVosma qoy moon nem addrek.\\nPHONETIC\\nVOS-ma QOY mo-ON nem ad-DREK.\\nBut blood from-him was spilled.\\n-------------------------------------------------------------------\\n(BROZHO_604_73.mp3)\\nBROZHO\\nThere is always some blood.\\nTRANSLATION\\nLoyi qoyi avekha ayyey.\\nCONTINUED: (3)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     147.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 147, 'page_label': '148'}, page_content='CONTINUED: (4)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     148.\\nPHONETIC\\nLO-yi qo-yi A-ve-kha ay-YEY.\\nSome blood there-will-be always.\\n-------------------------------------------------------------------\\n(FORZHO_604_74.mp3)\\nFORZHO\\nNot when you strangle them.\\nTRANSLATION\\nVos hash me avarraggera mae.\\nPHONETIC\\nVOS hash me A-var-rag-ge-ra ma-e.\\nNot if one will-strangle them.\\n-------------------------------------------------------------------\\n(RHALKO_604_75.mp3)\\nRHALKO\\nOr break their neck.\\nTRANSLATION\\nChe hash me vassamva lent.\\nPHONETIC\\nCHE hash me vas-SAM-va LENT.\\nOr if one will-break neck.\\n-------------------------------------------------------------------\\n(FORZHO_604_76.mp3)\\nFORZHO\\nOr roll them in a rug and trample them with a horse. The law states \\nthat--\\nTRANSLATION\\nChe hash me vacchorka mae ki janeti majin me anokitta mae ki \\nhrazefi. Me nem nesa k’assokhosori me--\\nCONTINUED: (4)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     148.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 148, 'page_label': '149'}, page_content='CONTINUED: (5)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     149.\\nPHONETIC\\nCHE hash me vach-CHOR-ka ma-e ki JA-ne-ti ma-JIN me a-no-KIT-ta ma-e \\nki HRA-ze-fi. me nem NE-sa KAS-so-kho-so-ri me--\\nOr if one will-roll them by rug and-then one will-trample them by \\nhorse. It is known by-law that--\\n-------------------------------------------------------------------\\n(MORO_604_77A.mp3)\\nMORO\\nAggo belonged to my khalasar. He served me well. He got his head \\nsmashed in by a rock. Fuck Aggo.\\nTRANSLATION\\nAggo dothra ma khalasaroon anni. Me sili anna chek. Nhare moon nem \\nkaf ki negwini. Hiles Aggoes.\\nPHONETIC\\nAG-go DO-thra ma kha-la-sa-ro-ON an-ni. me SI-li an-na CHEK. NHA-re \\nmo-ON nem KAF ki NE-gwi-ni. hi-LES ag-go-ES.\\nAggo rode with khalasar my. He served me well. Head from-him got \\nsmashed by rock. Fuck Aggo.\\n-------------------------------------------------------------------\\n(MORO_604_44.mp3)\\nMORO\\nBring in Drogo’s widow.\\nTRANSLATION\\nFichi khaleenies Drogosi.\\nPHONETIC\\nFI-chi kha-le-e-ni-ES DRO-go-si.\\nBring widow of-drogo.\\n-------------------------------------------------------------------\\n(MORO_604_45.mp3)\\nBROZHO\\nWho cares about her? She’s a midget.\\nCONTINUED: (5)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     149.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 149, 'page_label': '150'}, page_content='CONTINUED: (6)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     150.\\nTRANSLATION\\nFin nem olda ki mae? Me lentashi.\\nPHONETIC\\nFIN nem ol-da ki ma-e? me LEN-ta-shi.\\nWho is concerned by her? She (is) midget.\\n-------------------------------------------------------------------\\n(PORRZHO_604_46.mp3)\\nPORRZHO\\nI like her.\\nTRANSLATION\\nMe allayafa anna.\\nPHONETIC\\nme AL-la-ya-fa an-na.\\nShe pleases me.\\n-------------------------------------------------------------------\\n(MORO_604_47.mp3)\\nBROZHO\\nShe’s paler than milk.\\nTRANSLATION\\nMe azasqana lamekhoon.\\nPHONETIC\\nme A-zas-qa-na la-me-kho-ON.\\nShe is-paler than-milk.\\n-------------------------------------------------------------------\\n(PORRZHO_604_48.mp3)\\nPORRZHO\\nI bet she gets nice and pink when you pinch her.\\nTRANSLATION\\nAnha azhik memé vafazhoe rivaan kash me nem athacha.\\nCONTINUED: (6)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     150.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 150, 'page_label': '151'}, page_content='CONTINUED: (7)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     151.\\nPHONETIC\\nAN-ha a-ZHIK me-ME VA-fa-zho-e ri-va-AN kash me nem A-tha-cha.\\nI bet that-she pinkens to-the-tips when she is pinched.\\n-------------------------------------------------------------------\\n(RHALKO_604_49.mp3)\\nRHALKO\\nI’d like to know what a Khaleesi tastes like.\\nTRANSLATION\\nAnha zalak nesat ven fini ven athyazhar khaleesisi.\\nPHONETIC\\nAN-ha za-LAK ne-SAT ven FI-ni ven ath-ya-ZHAR KHA-le-e-si-si.\\nI want to-know like what that (is) taste of-khaleesi.\\n-------------------------------------------------------------------\\n(PORRZHO_604_50.mp3)\\nPORRZHO\\nGood. You can suck my dick.\\nTRANSLATION\\nAthdavrazar. Shafka laz addiwee khirrof anni.\\nPHONETIC\\nath-dav-ra-ZAR. SHAF-ka laz AD-di-we-e khir-ROF an-ni.\\nGood. You can moisten dangler my.\\n-------------------------------------------------------------------\\n(MORO_604_51.mp3)\\nMORO\\nShe belongs with the Dosh Khaleen.\\nTRANSLATION\\nMe jif dothrae Doshi Khaleen.\\nCONTINUED: (7)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     151.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 151, 'page_label': '152'}, page_content='CONTINUED: (8)4.29 4.29\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     152.\\nPHONETIC\\nME jif DO-thra-e DO-shi kha-le-EN.\\nShe should ride with-Dosh Khaleen.\\n-------------------------------------------------------------------\\n(QORRO_604_52.mp3)\\nQORRO\\nThe Wise Masters of Yunkai want her. They’re offering ten thousand \\nhorses in exchange. What’s worth more: one pink little girl or ten \\nthousand horses?\\nTRANSLATION\\nAiske Silve Yunkayoon zali mae. Mori vazhi ha thi dalen hrazefaan ha \\nmaan. Fini adavrana: ch’at nayati hannaven che thi dalen hrazef?\\nPHONETIC\\nAIS-ke SIL-ve yun-ka-yo-ON ZA-li ma-e. MO-ri VA-zhi ha THI da-LEN \\nhra-ze-fa-AN ha ma-AN. FI-ni A-dav-ra-na: CHAT NA-ya-ti han-na-VEN \\nche THI da-LEN hra-ZEF?\\nMasters Wise from-Yunkai want her. They will-give for ten thousand \\nhorses for her. What is-worth-more: or-one little-girl pink or ten \\nthousand horses?\\n-------------------------------------------------------------------\\n(MORO_604_53.mp3)\\nMORO\\nFuck the Wise Masters in their perfumed asses. Tell me where their \\nhorses are and I’ll take them for myself. She should stay here. It’s \\nour tradition. She belongs with the Dosh Khaleen.\\nTRANSLATION\\nHiles Aiske Silve vi choyokh dave. Asti anhaan rekke hrazef mori \\nmajin anha aqorak mora h’anhaan zhorre. Me jif vikovarera jinne. Me \\noskimikh kishi. Me zigeree Doshoon Khaleen.\\nPHONETIC\\nhi-LES AIS-ke SIL-ve vi cho-YOKH DA-ve. AS-ti an-ha-AN REK-ke hra-\\nZEF mo-ri main an-ha a-qo-RAK mo-ra han-ha-AN ZHOR-re. ME jif VI-ko-\\nva-re-ra JIN-ne. me os-ki-MIKH ki-shi. me ZI-ge-re-e do-sho-ON kha-\\nle-EN.\\nFuck Masters Wise between asscheeks perfumed. Tell to-me where (are) \\nCONTINUED: (8)4.29 4.29\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     152.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 152, 'page_label': '153'}, page_content=\"CONTINUED: (9)4.29 4.29\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     153.\\nhorses their and I will-claim them to-me specifically. She should \\nstay here. It (is) tradition our. She belongs-with Dosh Khaleen.\\n-------------------------------------------------------------------\\n(DANY_604_54.mp3)\\nDANY\\nDon’t you want to know what I think?\\nTRANSLATION\\nHash yeri vo zali nesat rek dirgak anha?\\nPHONETIC\\nHASH ye-ri vo za-li ne-SAT REK dir-GAK AN-ha?\\nDo you not want to-know what think I?\\n-------------------------------------------------------------------\\n(MORO_604_55.mp3)\\nMORO\\nYou’d rather be sold into slavery? Or maybe you’d like to show \\nRhalko here what you taste like?\\nTRANSLATION\\nYer zali meyer nem vazhi ven zafra? Che ishish me vallayafa yera \\nattihat zhey Rhalkoes athyazharoon yeri?\\nPHONETIC\\nYER ZA-li me-yer nem VA-zhi ven ZAF-ra? che i-SHISH me VAL-la-ya-fa \\nye-ra at-ti-HAT zhey rhal-ko-ES ath-ya-zha-ro-ON ye-ri? \\nYou want that-you will-be given for slave? Or maybe it will-please \\nyou to-show this Rhalko flavor your?\\n-------------------------------------------------------------------\\n(DANY_604_56.mp3)\\nDANY\\nNo, I don’t want either of those things.\\nTRANSLATION\\nVos, anha vo zalok vos at rek osoon.\\nPHONETIC\\nVOS, an-ha VO za-LOK VOS AT rek o-so-ON.\\nCONTINUED: (9)4.29 4.29\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     153.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 153, 'page_label': '154'}, page_content='CONTINUED: (10)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     154.\\nNo, I don’t want not one from-those options.\\n-------------------------------------------------------------------\\n(MORO_604_57.mp3)\\nMORO\\nWe don’t care what you want. This is the Temple of the Dosh Khaleen. \\nYou have no voice here, unless you are Dosh Khaleen. Which you are \\nnot, until we decide you are.\\nTRANSLATION\\nMe vos oldo kisha, jin athzalar yeri. Jini vaesof Doshi Khaleen. Vos \\nfothakhi vekho ha yeraan jinne, vosma yer Dosh Khaleen--ma yer vos, \\nvosma kisha vokkeraki mae.\\nPHONETIC\\nme VOS OL-do ki-sha, jin ath-za-LAR ye-ri. JI-ni va-e-SOF DO-shi kha-\\nle-EN. VOS FO-tha-khi vekho ha ye-ra-AN jin-ne, vosma yer DOSH kha-\\nle-EN--ma yer VOS, vos-ma KI-sha VOK-ke-ra-ki ma-e.\\nIt doesn’t matter to-us, these desires of-yours. This (is) temple of-\\nDosh Khaleen. No voice exists for you here, unless you (are) Dosh \\nKhaleen--and you (are) not, until we decide it.\\n-------------------------------------------------------------------\\n(DANY_604_58.mp3)\\nDANY\\nI know where I am. I have been here before. Right there, on that \\nspot, I ate a stallion’s heart. And the Dosh Khaleen pronounced my \\nchild the Stallion Who Mounts the World.\\nTRANSLATION\\nAnha nesak rekke anha kovarak. Anha ray dothra jinne hatif ajjin. \\nHazze, she haz sorfo, anha adakh zhores vezhoon. Ma Dosh Khaleen \\nhake yal anni Vezh Fin Saja Rhaesheseres.\\nPHONETIC\\nAN-ha ne-SAK REK-ke an-ha ko-va-RAK. AN-ha ray DO-thra JIN-ne ha-TIF \\naj-JIN. HAZ-ze, she HAZ SOR-fo, an-ha a-DAKH zho-RES ve-zho-ON. ma \\nDOSH kha-le-EN HA-ke YAL an-ni VEZH fin SA-ja rha-e-she-se-RES.\\nI know where I stand. I have ridden here before now. There, on that \\nspot, I ate heart from-a-stallion. And Dosh Khaleen named child my \\nStallion Who Mounts (the) World.\\n-------------------------------------------------------------------\\nCONTINUED: (10)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     154.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 154, 'page_label': '155'}, page_content='CONTINUED: (11)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     155.\\n(MORO_604_59.mp3)\\nMORO\\nAnd what happened? You trusted a sorceress, like a fool. Your baby \\nis dead because of you. And so is Khal Drogo.\\nTRANSLATION\\nMa fini meliso? Yer shille maege, ven tokik. Enta yeri driva haji \\nyeroon. Majin Khal Drogo akka.\\nPHONETIC\\nma FI-ni ME-li-so? yer SHIL-le MA-e-ge, ven to-KIK. EN-ta ye-ri DRI-\\nva ha-ji ye-ro-ON. ma-JIN KHAL DRO-go AK-ka.\\nAnd what happened? You trusted sorceress, like fool. Baby your is-\\ndead because-of you. And-so Khal Drogo also.\\n-------------------------------------------------------------------\\n(DANY_604_60.mp3)\\nDANY\\nThis is where Drogo promised to take his khalasar west to where the \\nworld ends. To ride wooden horses across the black salt sea as no \\nkhal has done before.\\nTRANSLATION\\nJinne zhey Drogo ast asqoy vidrie khalasares mae jim, finnaan nakhoe \\nrhaesheser. Dothralat hrazef ido yomme Havazzhifi Kazga ven et vo \\nkhal avvos.\\nPHONETIC\\nJIN-ne zhey DRO-go AST as-QOY VI-dri-e kha-la-sa-RES ma-e JIM, fin-\\nna-AN NA-kho-e rha-e-she-SER. do-thra-LAT hra-ZEF I-do YOM-me HA-\\nvazh-zhi-fi KAZ-ga ven et VO khal av-VOS.\\nHere blessed Drogo spoke (a) promise to guide khalasar his west, to-\\nwhere ends (the) world. To-ride horse wooden across Salt-sea Black \\nlike has-done no khal never.\\n-------------------------------------------------------------------\\n(DANY_604_61.mp3)\\nDANY\\nHe promised to kill the men in their iron suits and tear down their \\nstone houses. He swore it to me. Before the Mother of Mountains, as \\nthe stars looked down in witness.\\nCONTINUED: (11)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     155.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 155, 'page_label': '156'}, page_content='CONTINUED: (12)4.29 4.29\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     156.\\nTRANSLATION\\nMe ast asqoy addrivat mahrazhis fini ondee khogar shiqethi ma \\nohharat okrenegwin mori. Me ast asqoy anhaan. Hatif Maisi Krazaaji, \\nkash shieraki vitihir asavvasoon.\\nPHONETIC\\nme AST as-QOY ad-dri-VAT mah-ra-ZHIS fi-ni ON-de-e kho-GAR SHI-qe-\\nthi ma oh-ha-RAT o-kre-ne-GWIN mo-ri. me AST as-QOY an-ha-AN. ha-TIF \\nMA-i-si KRA-za-a-ji, kash SHI-e-ra-ki vi-ti-HIR a-sav-va-so-ON.\\nHe spoke (a) promise to-kill men who wear suits iron and tear-down \\nstone-tents their. He spoke (this) promise to-me. Before Mother of-\\nMountains, while (the) stars watched from-the-heavens.\\n-------------------------------------------------------------------\\n(MORO_604_62.mp3)\\nMORO\\nAnd you were dumb enough to believe him.\\nTRANSLATION\\nMa yer ven toki ven yer shillo mae.\\nPHONETIC\\nma YER ven TO-ki ven yer SHIL-lo ma-e.\\nAnd you as dumb-were that you believed him.\\n-------------------------------------------------------------------\\n(DANY_604_63.mp3)\\nDANY\\nAnd here, now, what great matters do the Great Khals discuss? Which \\nlittle villages you’ll raid, how many girls you’ll get to fuck, how \\nmany horses you’ll demand in tribute.\\nTRANSLATION\\nMa jinne, ajjin, fin vaese zhokwa jerie Khali Vezhveni? Fin vaesish \\nvemrasoe yeri, finsanney nayat vil ahilee yeri, finsanney hrazef \\naqaffi yeri k’azhi.\\nPHONETIC\\nma JIN-ne, aj-JIN, FIN VA-e-se ZHO-kwa JE-ri-e KHA-li VEZH-ve-ni? \\nFIN va-e-SISH VEM-ra-so-e ye-ri, fin-san-NEY na-YAT vil A-hi-le-e ye-\\nri, fin-san-NEY hra-ZEF A-qaf-fi ye-ri KA-zhi.\\nAnd here, now, what matters great discuss (the) Khals Great? What \\nCONTINUED: (12)4.29 4.29\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     156.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 156, 'page_label': '157'}, page_content=\"CONTINUED: (13)4.29 4.29\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     157.\\nlittle-villages will-raid you, how-many girls will-manage-to fuck \\nyou, how-many horses will-demand you by-tribute.\\n-------------------------------------------------------------------\\n(DANY_604_64.mp3)\\nDANY\\nYou are small men. Under you, the Dothraki will be a small people. \\nNone of you is fit to lead them.\\nTRANSLATION\\nYeri mahrazhi zhikwi. Torga yeri, Dothraki yanqosoraan zhikwi. Vos \\nat yeroa venoe idrilat mora vosecchi.\\nPHONETIC\\nYE-ri MAH-ra-zhi ZHIK-wi. TOR-ga ye-ri DO-thra-ki yan-qo-so-ra-AN \\nZHIK-wi. VOS AT ye-ro-a VE-no-e i-dri-LAT mo-ra vo-SECH-chi.\\nYou-all (are) men small. Under you-all, Dothraki will-be-a-people \\nsmall. Not one from-you is-suitable to-lead them never.\\n-------------------------------------------------------------------\\n(DANY_604_65.mp3)\\nDANY\\nBut I am. So I will.\\nTRANSLATION\\nVosma anha venok. Majin anha vidrik.\\nPHONETIC\\nvos-ma AN-ha ve-NOK. ma-jin AN-ha vi-DRIK.\\nBut I am-suitable. So I will lead.\\n-------------------------------------------------------------------\\n(MORO_604_66.mp3)\\nMORO\\nAll right. No Dosh Khaleen for you. Your choice. Instead, we’ll take \\nturns fucking you. And then we’ll let our bloodriders fuck you.\\nTRANSLATION\\nAthgoshar. Vos Dosh Khaleen ha yeraan. Athvokkerar yeri. Ha rekaan, \\nha jinaan, kisha ahileki yera k’athmajizari. Majin kisha vazhaki \\ndothrakhqoyoon kishi hilelat yera.\\nCONTINUED: (13)4.29 4.29\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     157.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 157, 'page_label': '158'}, page_content='CONTINUED: (14)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     158.\\nPHONETIC\\nath-go-SHAR. VOS DOSH kha-le-EN ha ye-ra-AN. ath-vok-ke-RAR ye-ri. \\nha re-ka-AN, ha ji-na-AN, KI-sha A-hi-le-ki ye-ra KATH-ma-ji-za-ri. \\nma-JIN KI-sha VA-zha-ki do-thrakh-qo-yo-ON ki-shi hi-le-LAT ye-ra.\\nAll-right. No Dosh Khaleen for you. Decision your. Instead of-that, \\nthen this, we will-fuck you by-turns. And then we will-allow \\nbloodriders our to-fuck you.\\n-------------------------------------------------------------------\\n(MORO_604_67.mp3)\\nMORO\\nAnd if there’s anything left of you, we’ll give our horses a turn. \\nHave you ever seen what a horse does to a woman? This is a thing you \\nshould see before you die. And you will. Right before you die.\\nTRANSLATION\\nMajin hash zhille athzinari yeri vekha, hash kisha vazhaki ekh \\nhrazefaan kishi. Hash yer ray tih kifinosi hilee hrazef chiories? \\nJini vekhikh fin eth tihi yer hatif yer drivoe. Ma yer atihi mae. \\nHatif yer drivoe zhorre.\\nPHONETIC\\nma-JIN HASH ZHIL-e ATH-zi-na-ri ye-ri VE-kha, hash KI-sha VA-zha-ki \\nEKH hra-ze-fa-AN ki-shi. HASH yer ray tih KI-fi-no-si HI-le-e hra-\\nZEF chi-o-ri-ES? JI-ni ve-KHIKH fin eth TI-hi yer ha-TIF yer DRI-vo-\\ne. ma yer A-ti-hi ma-e. ha-TIF yer DRI-vo-e ZHOR-re.\\nAnd if any remainder of-you there-is, then we will-give turn to-\\nhorses our. Have you ever seen how fucks horse woman? This (is) \\nthing that must see you before you die. And you will-see it. Before \\nyou die right.\\n-------------------------------------------------------------------\\n(MORO_604_68.mp3)\\nMORO\\nYou crazy cunt. Did you really think we would serve you?\\nTRANSLATION\\nZhey gech yofi. Hash yer shillo k’athjilari mekisha asilaki yera?\\nPHONETIC\\nzhey GECH YO-fi. HASH yer SHIL-lo KATH-ji-la-ri me-KI-sha A-si-la-ki \\nye-ra?\\nCONTINUED: (14)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     158.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 158, 'page_label': '159'}, page_content='CONTINUED: (15)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     159.\\nYou cunt crazy. Did you believe truly that-we will-serve you?)\\n-------------------------------------------------------------------\\n(DANY_604_69.mp3)\\nDANY\\nYou’re not going to serve. You’re going to die.\\nTRANSLATION\\nYeri vos osili vosecchi. Yeri vadrivoe.\\nPHONETIC\\nYE-ri VOS O-si-li vo-SECH-chi. YE-ri VA-dri-vo-e.\\nYou not will-serve never. You will-die.\\n-------------------------------------------------------------------\\nGAME OF THRONES #605\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/19/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nINT. AUDIENCE CHAMBER - NIGHT5.22 5.22\\n(ZANRUSH_605_1.mp3)\\nCONTINUED: (15)4.29 4.29\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     159.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 159, 'page_label': '160'}, page_content='CONTINUED:5.22 5.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     160.\\nZANRUSH\\nYou stand in the presence of Kinvara, High Priestess of the Red \\nTemple of Volantis, the Flame of Truth, the Light of Wisdom, the \\nFirst Servant of the Lord of Light.\\nTRANSLATION\\nJehikary Kinvaro iorat, Eglio Vokto hen Rijibliot Volantihot, Drivo \\nPerzo, Sylvio Oño, Dohaeriro Elio Aeksio Oño syt.\\nPHONETIC\\nje-hi-KA-ri KIN-va-ro i-O-rat, EG-li-o VOK-to hen ri-JIB-li-ot vo-\\nlan-TI-hot, DRI-vo PER-zo, SIL-vi-o ON-yo, do-HAI-ri-ro E-li-o AIK-\\nsi-o ON-yo-sit.\\nPresence of-Kinvaro you-stand, High Priestess from Temple Volantene, \\nTruth’s Flame, Wisdom’s Light, Servant First of-Lord of-Light.\\n-------------------------------------------------------------------\\n(TYRION_605_2.mp3)\\nTYRION\\nWelcome to Meereen.\\nTRANSLATION\\nVa Mirinot jemi jioran.\\nPHONETIC\\nva mi-RI-not JE-mi ji-O-ran.\\nTo Meereen you I-welcome.\\n-------------------------------------------------------------------\\nINT./EXT. MOUTH OF THE CAVE - NIGHT5.27 5.27\\n(COF_605_3.mp3)\\nCHILD OF THE FOREST\\nHold them back!\\nTRANSLATION\\nNguwas shonji!\\nPHONETIC\\nNGU-was SHON-ji!\\nAway-from-us hold-them!\\nCONTINUED:5.22 5.22\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     160.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 160, 'page_label': '161'}, page_content='CONTINUED:5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     161.\\n-------------------------------------------------------------------\\n(COF_605_4.mp3)\\nCHILD OF THE FOREST\\nHere!\\nTRANSLATION\\nNyahl!\\nPHONETIC\\nNYAHL!\\nHere!\\n-------------------------------------------------------------------\\n(COF_605_5.mp3)\\nCHILD OF THE FOREST\\nNo!\\nTRANSLATION\\nHi!\\nPHONETIC\\nHI!\\nNo!\\n-------------------------------------------------------------------\\n(COF_605_6.mp3)\\nCHILD OF THE FOREST\\nProtect Bran!\\nTRANSLATION\\nShagukwamanji, ni-Burán!\\nPHONETIC\\nsha-gu-KWA-man-ji, ni-bu-RAN!\\nYou-him-protect, this-Bran!\\n-------------------------------------------------------------------\\n(COF_605_7.mp3)\\nCONTINUED:5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     161.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 161, 'page_label': '162'}, page_content='CONTINUED: (2)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     162.\\nCHILD OF THE FOREST\\nMove, move, move!\\nTRANSLATION\\nGi, gi, gi!\\nPHONETIC\\nGI, GI, GI!\\nMove, move, move!\\n-------------------------------------------------------------------\\n(COF_605_8.mp3)\\nCHILD OF THE FOREST\\nNow!\\nTRANSLATION\\nNyahahl!\\nPHONETIC\\nNYA-hahl!\\nNow!\\n-------------------------------------------------------------------\\n(COF_605_9.mp3)\\nCHILD OF THE FOREST\\nFire!\\nTRANSLATION\\nTagungi!\\nPHONETIC\\nta-GUNG-i!\\nFire!\\n-------------------------------------------------------------------\\n(COF_605_10.mp3)\\nCHILD OF THE FOREST\\nStop them!\\nCONTINUED: (2)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     162.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 162, 'page_label': '163'}, page_content='CONTINUED: (3)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     163.\\nTRANSLATION\\nShoguldani!\\nPHONETIC\\nsho-GUL-da-ni!\\nStop-them!\\n-------------------------------------------------------------------\\n(COF_605_11.mp3)\\nCHILD OF THE FOREST\\nFor the Earth!\\nTRANSLATION\\nGernat!\\nPHONETIC\\nGER-nat!\\nFor-the-Earth!\\n-------------------------------------------------------------------\\n(COF_605_12.mp3)\\nCHILD OF THE FOREST\\nFor Bran!\\nTRANSLATION\\nBuranat!\\nPHONETIC\\nbu-RA-nat!\\nFor-Bran!\\n-------------------------------------------------------------------\\n(COF_605_13.mp3)\\nCHILD OF THE FOREST\\nThis way!\\nTRANSLATION\\nNyagyesanks!\\nCONTINUED: (3)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     163.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 163, 'page_label': '164'}, page_content='CONTINUED: (4)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     164.\\nPHONETIC\\nnya-GYE-sanks!\\nBy-this-path!\\n-------------------------------------------------------------------\\n(COF_605_14.mp3)\\nCHILD OF THE FOREST\\nInto the cave!\\nTRANSLATION\\nMagulnenya!\\nPHONETIC\\nma-GUL-nen-ya!\\nInto-the-cave!\\n-------------------------------------------------------------------\\n(COF_605_15.mp3)\\nCHILD OF THE FOREST\\nBlock them off!\\nTRANSLATION\\nAwahl utwommani!\\nPHONETIC\\nA-wahl u-TWOM-ma-ni!\\nAt-them do-blocking!\\n-------------------------------------------------------------------\\n(COF_605_16.mp3)\\nCHILD OF THE FOREST\\nGo around them!\\nTRANSLATION\\nAwassa gi!\\nCONTINUED: (4)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     164.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 164, 'page_label': '165'}, page_content='CONTINUED: (5)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     165.\\nPHONETIC\\nA-was-sa GI!\\nAvoiding-them move!\\n-------------------------------------------------------------------\\n?????? ??\\n(COF_605_17.mp3)\\nCHILD OF THE FOREST\\nToo many have died already. We must be ready before more invaders \\ncross over.\\nTRANSLATION\\nDuraha toda oboldasa nyahenya. Gudniyahl ngugwa yahat todanasa \\nojigli.\\nPHONETIC\\nDU-ra-ha TO-da o-BOL-da-sa nya-HEN-ya. GUD-ni-yahl NGU-gwa ya-hat TO-\\nda-NA-sa o-JIG-li.\\nGreat multitudes died-have already. Being-ready-at we-under-are to-\\nthe-time more-enemies cross-over.\\n-------------------------------------------------------------------\\n(COF_605_18.mp3)\\nCHILD OF THE FOREST\\nEither we protect ourselves or we risk losing everything.\\nTRANSLATION\\nNgubgukwamanjiyassa, fowanjildiyahl ngugwi.\\nPHONETIC\\nngub-gu-KWA-man-ji-yas-sa, FO-wan-JIL-di-yahl ngu-gwi.\\nWe-ourselves-protect-or, everything-losing-at we-under-are.\\n-------------------------------------------------------------------\\n(COF_605_19.mp3)\\nCHILD OF THE FOREST\\nThe threat is too great. We must end this now.\\nCONTINUED: (5)5.27 5.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     165.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 165, 'page_label': '166'}, page_content='CONTINUED:?? ??\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     166.\\nTRANSLATION\\nNassa durahahl hafwa. Nyiha guldaniyahl ngugwa nyah.\\nPHONETIC\\nNAS-sa DU-ra-hahl HAF-wa. nyi-ha GUL-da-ni-yahl NGU-gwa NYAH.\\nThreat greatness-at it-is-over. This’s stopping-at we-under-are now.\\n-------------------------------------------------------------------\\nGAME OF THRONES #605\\nSTUNT PERFORMERS’ DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/22/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nAPPROXIMATION\\nA line whose mouth movements match the offical translation.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\n-------------------------------------------------------------------\\n(COF_605_1S.mp3)\\nCHILD OF THE FOREST\\nToo many have died already.\\nAPPROXIMATION\\nDudaha doda obodasa yahiya.\\nPHONETIC\\nDU-da-ha DO-da o-BO-da-sa ya-HI-ya.\\n-------------------------------------------------------------------\\n(COF_605_2S.mp3)\\nCONTINUED:?? ??\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     166.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 166, 'page_label': '167'}, page_content='CONTINUED: (2)?? ??\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     167.\\nCHILD OF THE FOREST\\nEither we protect ourselves or...\\nAPPROXIMATION\\nGuguwamajiyasa...\\nPHONETIC\\ngu-gu-WA-ma-ji-ya-sa...\\n-------------------------------------------------------------------\\n(COF_605_3S.mp3)\\nCHILD OF THE FOREST\\nThe threat is too great.\\nAPPROXIMATION\\nNasa dudahal hafuwa.\\nPHONETIC\\nNA-sa DU-da-hal HA-fu-wa.\\n-------------------------------------------------------------------\\nGAME OF THRONES #606\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 07/21/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nCONTINUED: (2)?? ??\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     167.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 167, 'page_label': '168'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     168.\\nEXT. ESSOS - HILLS - LATER6.27 6.27\\n(DAARIO_606_1.mp3)\\nDAARIO\\nI’m going after her. Wait for me here.\\nTRANSLATION\\nAnha adothrak maan. Ayos anna jinne.\\nPHONETIC\\nAN-ha a-do-THRAK ma-AN. a-YOS an-na JIN-ne.\\nI will-ride after-her. Await me here.\\n-------------------------------------------------------------------\\n(DANY_606_2.mp3)\\nDANY\\nEvery khal who ever lived chose three bloodriders to fight beside \\nhim and guard his way. (beat) But I am not a khal.\\nTRANSLATION\\nEi khal fin thir nakhaan okke sen dothrakhqoy aloji qisi mae \\nm’avijazeri athdinar mae. (beat) Vosma anha vos khal.\\nPHONETIC\\nE-i KHAL fin thir na-kha-AN OK-ke SEN doth-rakh-QOY A-lo-ji qe-si ma-\\ne MA-vi-ja-ze-ri ath-di-NAR ma-e. (beat) VOS-ma AN-ha VOS KHAL.\\nEvery khal who lived ever chose three bloodriders that-they-will-\\nfight beside him and-that-they-will-guard movements his. (beat) But \\nI (am) no khal.\\n-------------------------------------------------------------------\\n(DANY_606_3.mp3)\\nDANY (CONT’D)\\nI will not choose three bloodriders. I choose you all.\\nTRANSLATION\\nAnha vo vokkak sen dothrakhqoy. Anha okkak ei yeri.\\nPHONETIC\\nAN-ha VO vok-KAK SEN doth-rakh-QOY. AN-ha ok-KAK E-i ye-ri.\\nI not will-choose three bloodriders. I choose all of-you.\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     168.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 168, 'page_label': '169'}, page_content='CONTINUED:6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     169.\\n-------------------------------------------------------------------\\n(DANY_606_4.mp3)\\nDANY\\nI ask your oath, that you will live and die as blood of my blood, \\nriding at my side to keep me safe from harm.\\nTRANSLATION\\nAnha qafak asqoy yeroa, majin yeri m’athiri m’adrivoe ven qoy qoyi, \\nm’adothrae anni m’avijezeri anna athzhowakaroon.\\nPHONETIC\\nAN-ha qa-FAK as-QOY ye-ro-a, ma-jin YE-ri MA-thi-ri MA-dri-vo-e ven \\nQOY QO-yi, MA-do-thra-e an-ni MA-vi-je-ze-ri an-na ath-zho-wa-ka-ro-\\nON.\\nI ask oath from-you, and-so you and-will-live and-will-die as blood \\nof-blood (of me), and-will-ride beside-me and-will-protect from-\\nharm.\\n-------------------------------------------------------------------\\n(DOTHRAKI_606_5.mp3)\\nDOTHRAKI\\nBlood of my blood! Blood of my blood!\\nTRANSLATION\\nQoy qoyi! Qoy qoyi!\\nPHONETIC\\nQOY QO-yi! QOY QO-yi!\\nBlood of-blood (of me)! Blood of-blood (of me)!\\n-------------------------------------------------------------------\\n(DANY_606_6.mp3)\\nDANY\\nI will ask more of you than any Khal has ever asked of his khalasar!\\nTRANSLATION\\nAnha aqafak san ale yeroa ei Khaloon ray qaf khalasaroon mae!\\nPHONETIC\\nAN-ha a-qa-FAK SAN A-le ye-ro-a E-i kha-lo-ON ray qaf kha-la-sa-ro-\\nON ma-e!\\nCONTINUED:6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     169.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 169, 'page_label': '170'}, page_content='CONTINUED: (2)6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     170.\\nI will-ask more much from-you any than-Khal has asked from-khalasar \\nhis!\\n-------------------------------------------------------------------\\n(DOTHRAKI_606_5.mp3)\\nDOTHRAKI\\nBlood of my blood! Blood of my blood!\\nTRANSLATION\\nQoy qoyi! Qoy qoyi!\\nPHONETIC\\nQOY QO-yi! QOY QO-yi!\\nBlood of-blood (of me)! Blood of-blood (of me)!\\n-------------------------------------------------------------------\\n(DANY_606_7.mp3)\\nDANY\\nWill you ride the wooden horses across the black salt sea?\\nTRANSLATION\\nHash yeri adothrae hrazef ido yomme Havazzhifi Kazga?\\nPHONETIC\\nHASH ye-ri A-do-thra-e hra-ZEF I-do YOM-me HA-vazh-zhi-fi KAZ-ga?\\nWill you ride horse wooden across salt-sea black?\\n-------------------------------------------------------------------\\n(DANY_606_8.mp3)\\nDANY\\nWill you kill my enemies in their iron suits and tear down their \\nstone houses?\\nTRANSLATION\\nHash yeri vaddrivi dozge anni ma khogaroon shiqethi mori majin \\nvohhari okrenegwin mori?\\nPHONETIC\\nHASH ye-ri VAD-dri-vi DOZ-ge an-ni ma kho-ga-ro-ON SHI-qe-thi mo-ri \\nma-jin VOH-ha-ri o-kre-ne-GWIN MO-ri?\\nCONTINUED: (2)6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     170.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 170, 'page_label': '171'}, page_content='CONTINUED: (3)6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     171.\\nWill you kill enemies my with suits iron their and-then tear-down \\nstone-houses their?\\n-------------------------------------------------------------------\\n(DANY_606_9.mp3)\\nDANY\\nWill you give me the Seven Kingdoms, the gift Khal Drogo promised me \\nbefore the Mother of Mountains as the stars looked down in witness?\\nTRANSLATION\\nHash yeri vazhi anhaan Rhaeshis Andahli, jin azho me-Khal Drogo ast \\nasqoy mehas hatif Maisi Krazaaji kash shieraki vitihir asavvasoon?\\nPHONETIC\\nHASH ye-ri VA-zhi an-ha-AN rhae-SHIS an-DAH-li, jin A-zho me-KHAL \\nDRO-go AST as-QOY me-HAS ha-tif MA-i-si KRA-za-a-ji kash SHI-e-ra-ki \\nvi-ti-HIR a-sav-va-so-ON?\\nWill you give to-me Kingdoms Seven, the gift that-Khal Drogo gave \\noath (his) for-it before Mother of-Mountains while stars watched \\nfrom-heaven?\\n-------------------------------------------------------------------\\n(DANY_606_10.mp3)\\nDANY\\nAre you with me, now and always?!\\nTRANSLATION\\nHash yeri m’anhoon, ma jinne m’ayyeyaan?!\\nPHONETIC\\nHASH ye-ri man-ho-ON, ma JIN-ne ma-yey-ya-AN?!\\nAre you with-me, and now and-until-forever?!\\n-------------------------------------------------------------------\\nGAME OF THRONES #608\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 07/21/15\\nCONTINUED: (3)6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     171.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 171, 'page_label': '172'}, page_content='CONTINUED: (4)6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     172.\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. STREETS OF MEEREEN - DAY8.6 8.6\\n(REDPRIEST_608_1.mp3)\\nRED PRIEST\\nFrom the fire she was reborn to remake the world. Just as her \\ndragons are a gift from the Lord of Light to Daenerys, so then is \\nDaenerys a gift from the Lord to her children. If we are steadfast \\nin our love for the Queen and her faithful advisors, no man will \\never lock us in chains again.\\nTRANSLATION\\nHen perzy vys amazverdagon asittaks. Lo zyhyz zaldrizesse Aeksio Oño \\nirudy Daenerot issi, separ Daenerys Aeksio irudy riñarta zyhot issa. \\nLo Darie se pasabari sytiotapia lotiri jorraeloty, separ dore vala \\narli ilon belmurilza.\\nPHONETIC\\nhen PER-zi VIS a-maz-VER-da-gon a-SIT-taks. lo zi-hiz zal-dri-ZES-se \\nAIK-si-o ON-yo i-RU-di DAI-ne-rot IS-is, se-par DAI-ne-ris AIK-si-o \\ni-RU-di rin-YAR-ta ZI-hot is-sa. lo DA-ri-e se pa-SA-ba-ri si-ti-o-\\nta-pi-A LO-ti-ri jor-RAI-lo-ti, se-par DO-re VA-la AR-li i-lon bel-\\nmu-RIL-za.\\nFrom fire world to-remake she-was-reborn. While her dragons Lord of-\\nLight’s gift to-Daenerys are, thus Daenerys Lord’s gift to-children \\nher is. If Queen and faithful councilors steadfastly we-would-love, \\nthen no man again us will-chain.\\n-------------------------------------------------------------------\\n(REDPRIEST_608_2.mp3)\\nCONTINUED: (4)6.27 6.27\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     172.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 172, 'page_label': '173'}, page_content='CONTINUED:8.6 8.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     173.\\nRED PRIEST\\nWhen our Queen asks us to go to war, we march behind her. And when \\nshe asks us for peace, we throw down our knives.\\nTRANSLATION\\nLo ilva Daria vilibazmo syt ioragon ilot epos, separ zijo inkot \\nmemebili. Se lyko syt ilot epos, ilvi ohilvossa rughili.\\nPHONETIC\\nlo IL-va DA-ri-a vi-li-BAZ-mo-sit i-O-ra-gon i-lot E-pos, se-par ZI-\\njo IN-kot me-ME-bi-li. se LI-ko-sit i-lot E-pos, IL-vi o-hil-VOS-sa \\nru-GHI-li.\\nWhen our Queen war-for to-stand us asks, then her behind we-will-\\nmarch. When peace-for us asks, our knives we-will-drop.\\n-------------------------------------------------------------------\\nEXT. MEEREEN GARDEN TERRACE - DAY8.10 8.10\\n(MISSANDEI_608_3.mp3)\\nMISSANDEI\\nIntimidating.\\nTRANSLATION\\nLárari.\\nPHONETIC\\nLA-ra-ri.\\nIntimidating.\\n-------------------------------------------------------------------\\n(TYRION_608_4.mp3)\\nTYRION\\nA joke.\\nTRANSLATION\\nPirtiapos.\\nPHONETIC\\npir-ti-A-pos.\\nJoke.\\nCONTINUED:8.6 8.6\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     173.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 173, 'page_label': '174'}, page_content='CONTINUED:8.10 8.10\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     174.\\n-------------------------------------------------------------------\\nGAME OF THRONES #609\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 07/21/15\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. PLATEAU - DAY9.14 9.14\\n(GREYWORM_609_1.mp3)\\nGREY WORM\\nYou men have a choice. Fight and die for Masters who would never \\nfight and die for you. Or go home, to your families.\\nTRANSLATION\\nJim vali ezi m’idreno. Ozvilívagho si morghúlegho Aeske zy sko do \\nozvilívizi si morghúlesi jim zy dori. Ja já lintot, va jivi kezari.\\nPHONETIC\\nJIM VA-li e-zi mi-DRE-no. oz-vi-LI-va-gho si mor-GHU-le-gho AIS-ke-\\nzi sko DO oz-vi-LI-vi-zi si mor-GHU-le-si JIM-zi DO-ri. ja JA LIN-\\ntot, va JI-vi ke-ZA-ri.\\nYou men have a-choice. To-fight and to-die masters-for that not \\nwould-fight and would-die you-for never. Or go home, to your \\nfamilies.\\n-------------------------------------------------------------------\\nCONTINUED:8.10 8.10\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     174.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 174, 'page_label': '175'}, page_content='CONTINUED:9.14 9.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     175.\\nTHE BIG THREE #702\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/23/16\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nINT. DRAGONSTONE - MAP ROOM - NIGHT2.1 2.1\\n(GREYWORM_702_1.mp3)\\nGREY WORM\\nForgive me, my queen. A red priestess from Asshai begs a word.\\nTRANSLATION\\nYn ilirí, nya dare. Me vohty mili hin Asshai o pindas m’odhir.\\nPHONETIC\\nin i-li-RI, NI-a DA-re. me VOH-ti MI-li hin a-SHAI o PIN-das MO-\\ndhir.\\nMe forgive, my queen. A priestess red from Asshai you begs a-word.\\n-------------------------------------------------------------------\\nINT. DRAGONSTONE - AUDIENCE CHAMBER - CONTINUOUS2.3 2.3\\n(MELISANDRE_702_2.mp3)\\nCONTINUED:9.14 9.14\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     175.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 175, 'page_label': '176'}, page_content='CONTINUED:2.3 2.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     176.\\nMELISANDRE\\nQueen Daenerys.\\nTRANSLATION\\nDarys Daenerys.\\nPHONETIC\\nDA-ris DAI-ne-ris.\\nQueen Daenerys.\\n-------------------------------------------------------------------\\n(MELISANDRE_702_3A.mp3)\\nMELISANDRE\\nQueen Daenerys. I was a slave once, bought and sold, scourged and \\nbranded. It is an honor to meet the Breaker of Chains.\\nTRANSLATION\\nDarys Daenerys. Dohaeriros istin, sindita liortá, qilonta ozbartá. \\nRiglose Belmot Pryjatys rhaenan.\\nPHONETIC\\nDA-ris DAI-ne-ris. do-HAI-ri-ros IS-tin, SIN-di-ta li-or-TA, ki-LON-\\nta oz-bar-TA. RI-glo-se BEL-mot pri-JA-tis RHAI-nan.\\nQueen Daenerys. Slave I-was-once, bought and-sold, scourged and-\\nbranded. By-honor Chain Breaker I-do-meet.\\n-------------------------------------------------------------------\\n(DANY_702_4A.mp3)\\nDANY\\nThe Red Priests helped bring peace to Meereen. You are very welcome \\nhere. What is your name?\\nTRANSLATION\\nMeli Voktyssy Mirini lyks mazverdagon beldis. Kesir dreji jiorilaks. \\nSkoroso jemele brozia?\\nPHONETIC\\nME-li vok-TIS-si mi-RI-ni LIKS maz-VER-da-gon BEL-dis. KE-sir DRE-ji \\nji-OR-ri-laks. sko-RO-so je-ME-le BRO-zi-a?\\nRed Priests in-Meereen peace brought-about helped. Here truly you-\\nare-welcome. By-what yourself do-you-name?\\nCONTINUED:2.3 2.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     176.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 176, 'page_label': '177'}, page_content='CONTINUED: (2)2.3 2.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     177.\\n-------------------------------------------------------------------\\n(MELISANDRE_702_8.mp3)\\nMELISANDRE\\nI am called Melisandre.\\nTRANSLATION\\nMelisandrose broziks.\\nPHONETIC\\nme-li-SAN-dro-se BRO-ziks.\\nMelisandre I-am-called.\\n-------------------------------------------------------------------\\n(MELISANDRE_702_5.mp3)\\nMELISANDRE\\nKinvara, the High Priestess of the Lord of Light, sent me to help \\nyou, however I can, in the wars to come.\\nTRANSLATION\\nKinvara, Eglie Voktys Aeksiot Oño, massilaro vilibazmoti avy \\nbaelagon, ñuhos kostyso bosajose, yne jittas.\\nPHONETIC\\nkin-VA-ra, EG-li-e VOK-tis AIK-si-ot ON-yo, mas-SI-la-ro vi-li-BAZ-\\nmo-ti A-vi BAI-la-gon, NYU-hos kos-TI-so, bo-SA-jo-se, I-ne JIT-tas.\\nKinvara, High Priest to-lord of-light, coming wars-in you help, my \\nability-by utmost, me sent.\\n-------------------------------------------------------------------\\n(MELISANDRE_702_6.mp3)\\nMELISANDRE\\nThe long night is coming, and the dead come with it. Only the prince \\nwho was promised can bring the dawn.\\nTRANSLATION\\nBosys bantis amazis, se morghor zijomy amazis. Meri kivio darilaros \\noz maghagon kostas.\\nPHONETIC\\nBO-sis BAN-tis a-MA-zis, se MOR-ghor zi-JO-mi a-MA-zis. ME-ri KI-vi-\\no da-ri-LA-ros OZ ma-GHA-gon KOS-tas.\\nCONTINUED: (2)2.3 2.3\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     177.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 177, 'page_label': '178'}, page_content='CONTINUED: (3)2.3 2.3\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     178.\\nLong night comes, and dead with-it come. Only of-promise prince(ss) \\ndawn bring can.\\n-------------------------------------------------------------------\\nINT. DRAGONSTONE - GREY WORM’S CHAMBER - NIGHT2.11A 2.11A\\n(GREYWORM_702_7.mp3)\\nGREY WORM\\nEnter.\\nTRANSLATION\\nMají.\\nPHONETIC\\nma-JI.\\nEnter.\\n-------------------------------------------------------------------\\nTHE BIG THREE #703\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/24/16\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nCONTINUED: (3)2.3 2.3\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     178.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 178, 'page_label': '179'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     179.\\nINT. DRAGONSTONE - AUDIENCE CHAMBER - DAY3.7 3.7\\n(DANY_703_1.mp3)\\nDANY\\nEscort these men to their rooms. Treat them well, but keep an eye on \\nthem.\\nTRANSLATION\\nIdriso jin mahrazhis gacheshaan mori. Ti morea chek, vosma vitihiri \\nmora.\\nPHONETIC\\nI-dri-so jin mah-ra-ZHIS ga-che-sha-AN mo-ri. TI mo-re-a CHEK, vos-\\nma VI-ti-hi-ri mo-ra.\\nEscort these men to-rooms their. Treat them well, but keep-an-eye-on \\nthem.\\n-------------------------------------------------------------------\\nEXT. CASTERLY ROCK - DAY3.31 3.31\\n(GREYWORM_703_3A.mp3)\\nGREY WORM\\nThere are none left.\\nTRANSLATION\\nDory umbas.\\nPHONETIC\\nDO-ri UM-bas.\\nNone remain.\\n-------------------------------------------------------------------\\n(GREYWORM_703_2.mp3)\\nGREY WORM\\nThere are supposed to be more than this. Many more.\\nTRANSLATION\\nInkas hónesko sidri hin bezi. Kara sidri.\\nPHONETIC\\nIN-kas HO-nes-ko SI-dri hin BE-zi. KA-ra SI-dri.\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     179.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 179, 'page_label': '180'}, page_content='CONTINUED:3.31 3.31\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     180.\\nIt-should for-there-to-be more than these. A lot more.\\n-------------------------------------------------------------------\\n(ULIEUTENANT1_703_4A.mp3)\\nUNSULLIED LIEUTENANT 1\\nTorgo Nudho!\\nTRANSLATION\\nTorgo Nudho!\\nPHONETIC\\nTOR-go NU-dho!\\nWorm Grey!\\n-------------------------------------------------------------------\\n(GREYWORM_703_5.mp3)\\nGREY WORM\\nWhere are the rest of the Lannisters?\\nTRANSLATION\\nSkure las v’umbor espo Lanisteri?\\nPHONETIC\\nSKU-re las VUM-bor es-po la-ni-STE-ri?\\nWhere is the-rest of-the Lannisters?\\n-------------------------------------------------------------------\\nTHE BIG THREE #704\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 11/13/16\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nCONTINUED:3.31 3.31\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     180.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 180, 'page_label': '181'}, page_content='CONTINUED: (2)3.31 3.31\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     181.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. SKY ABOVE BATTLEFIELD - CONTINUOUS4.41 4.41\\n(DANY_704_1.mp3)\\nDANY\\nCalm yourself, little flame. Pain be gone, tears be gone, world be \\ngone. Mother has come, and sorrow has fled. Everything will be fine. \\nYou’ll see.\\nTRANSLATION\\nAole lykemás, perzitsos. Odrys sovegon, quvys sovegon, vys sovegon. \\nMuña mastas, se munnon hembistas. Tolvyn syri kessa. Urnila.\\nPHONETIC\\na-O-le li-ke-MAS, per-ZIT-sos. O-dris SO-ve-gon, KU-vis SO-ve-gon, \\nVIS SO-ve-gon. MUN-ya MAS-tas, se MUN-non hem-BIS-tas. TOL-vin SI-ri \\nKES-sa. UR-ni-la.\\nYourself calm, little-flame. Pain be-gone, tears be-gone, world be-\\ngone. Mother has-come, and sorrow has-fled. Everything fine will-be. \\nYou-will-see.\\n-------------------------------------------------------------------\\nGROUPXX XX\\n(DOTHRAKI_704_2.mp3)\\nDOTHRAKI\\nStrength! Attack! Charge! Blood of my blood! For honor! For the \\nkhaleesi! By blood! By my arakh! Die! By my steed! Death to the \\nforeigners! By the dragons!\\nTRANSLATION\\nHajas! Vashas! Goras! Qoy qoyi! K’athchomari! Ki khaleesisi! Ki \\nqoyi! M’arakhaan anni! Drivos! Ki sajosi anni! Athdrivar ifakea! Ki \\nzhavvorsi!\\nCONTINUED: (2)3.31 3.31\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     181.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 181, 'page_label': '182'}, page_content='CONTINUED:XX XX\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     182.\\nPHONETIC\\nha-JAS! va-SHAS! go-RAS! QOY QO-yi! KATH-cho-ma-ri! ki KHA-le-e-si-\\nsi! ki QO-yi! ma-ra-kha-AN AN-ni! dri-VOS! ki SA-jo-si AN-ni! ath-\\ndri-VAR I-fa-ke-a! ki zhav-VOR-si!\\nBe-strong! Attack! Charge! Blood of-blood! By-honor! By khaleesi! By \\nblood! With-arakh my! Die! By steed my! Death to-foreigners! By \\ndragons!\\n-------------------------------------------------------------------\\nTHE BIG THREE #705\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 08/16/16\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. DRAGONSTONE LOCATION - DAY5.12 5.12\\n(DOTHRAKI1_705_1.mp3)\\nDOTHRAKI #1\\nThis man says he is your friend, khaleesi.\\nTRANSLATION\\nJin mahrazh asta memé okeo shafki, zhey khaleesi.\\nPHONETIC\\nJIN mah-RAZH AS-ta me-ME O-ke-o SHAF-ki, zhey KHA-le-e-si.\\nCONTINUED:XX XX\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     182.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 182, 'page_label': '183'}, page_content='CONTINUED:5.12 5.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     183.\\nThis man says that-he (is) friend yours, O khaleesi.\\n-------------------------------------------------------------------\\n(DANY_705_2.mp3)\\nDANY\\nHe is my friend.\\nTRANSLATION\\nMe okeo anni sekosshi.\\nPHONETIC\\nme O-ke-o an-ni se-KOSH-shi.\\nHe (is) friend my truly.\\n-------------------------------------------------------------------\\nTHE BIG THREE #706\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 11/13/16\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nGROUP\\n(GROUP_706_1.mp3)\\nCONTINUED:5.12 5.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     183.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 183, 'page_label': '184'}, page_content='CONTINUED:\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     184.\\nGROUP\\nLord of Light, show us the way. Come to us in our darkness and lead \\nyour servant into your light.\\nTRANSLATION\\nAeksio Oño, geron ilot arris. Ilvro syndrorro ilot umazis se va oñot \\naohot dohaeriros aohi jemas.\\nPHONETIC\\nAIK-si-o ON-yo, GE-ron I-lot ar-RIS. ILV-ro sin-DROR-ro I-lot u-ma-\\nZIS se va ON-yot a-O-hot do-HAI-ri-ros a-O-hi je-MAS.\\nLord of-Light, path to-us show. Our darkness-in to-us come and into \\nlight your servant your lead.\\n-------------------------------------------------------------------\\nTHE BIG THREE #707\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/16/16\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\nEXT. ROAD TO DRAGONPIT - NEW VANTAGE POINT - DAY7.11 7.11\\n(QHONO_707_5.mp3)\\nQHONO\\nYour friend knew how to dress. Too bad he didn’t know how to fight.\\nCONTINUED:\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     184.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 184, 'page_label': '185'}, page_content='CONTINUED:7.11 7.11\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     185.\\nTRANSLATION\\nOkeo yeri nemo shil khogarat. Me ohazha memé vo shilo lajat.\\nPHONETIC\\nO-ke-o ye-ri NE-mo SHIL kho-ga-RAT. me O-ha-zha me-ME VO SHI-lo la-\\nJAT.\\nFriend your himself knew-how to-dress. It is-heavy that-he didn’t \\nknow-how to-fight.\\n-------------------------------------------------------------------\\nEXT. DRAGONPIT - DAY7.23 7.23\\n(DANY_707_1.mp3)\\nDANY\\nTo my son, the Stallion Who Will Mount the World, to him I also \\npledge a gift. To him I will give this iron chair his mother’s \\nfather sat in. I will give Seven Kingdoms.\\nTRANSLATION\\nMa rizhaan anni, Vezh fin Asaja Rhaesheseres: Maan anha valloshak \\nazh akka. Maan anha vazhak jin ador shiqethi finaan neva ave maisi \\nmae. Anha vazhak maan Rhaeshis Andahli.\\nPHONETIC\\nma ri-zha-AN an-ni, VEZH fin A-sa-ja rha-e-she-se-RES: ma-AN AN-ha \\nval-lo-SHAK AZH AK-ka. ma-AN AN-ha va-ZHAK jin a-DOR SHI-qe-thi fi-\\nna-AN NE-va A-ve MA-i-si ma-e. AN-ha va-ZHAK ma-an rha-e-SHIS an-DAH-\\nli.\\nAnd to-son my, stallion who will-mount world: to-him I will-give \\ngift also. To-him I will-give this chair iron on-which sat father of-\\nmother her. I will-give to-him Kingdoms Seven.\\n-------------------------------------------------------------------\\nEXT. DRAGONPIT - MAIN FLOOR - DAY7.25 7.25\\n(DANY_707_2.mp3)\\nDANY\\nA dragon is not a slave.\\nTRANSLATION\\nZaldrizes buzdari iksos daor.\\nCONTINUED:7.11 7.11\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     185.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 185, 'page_label': '186'}, page_content='CONTINUED:7.25 7.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     186.\\nPHONETIC\\nzal-DRI-zes buz-DA-ri IK-sos DAOR.\\nDragon slave is not.\\n-------------------------------------------------------------------\\nINT. DRAGONSTONE - MAP ROOM - DAY7.28 7.28\\n(GREYWORM_707_3A.mp3)\\nGREY WORM\\nMy queen, you will travel to unknown lands. It is not safe. If you \\nstay here, we can protect you--\\nTRANSLATION\\nNya dare, erevozliva va tighuni narédheda. Do sa yho. Lu úmbila \\nkizir, av koti mízagho--\\nPHONETIC\\nNI-a DA-re, e-RE-voz-LI-va va ti-GHU-ni na-RE-dhe-da. DO sa I-ho. lu \\nUM-bi-la KI-zir, av KO-ti MI-za-gho--\\nMy queen, you-will-travel to lands unknown. Not is safe. If you-stay \\nhere, you we-can protect--\\n-------------------------------------------------------------------\\n(DANY_707_4B.mp3)\\nDANY\\nThis is our war now. (beat) And this man is not a stranger anymore.\\nTRANSLATION\\nKesy sir ilva vilibazma issa. (beat) Se bisa vala sir tolmihy iksos \\ndaor.\\nPHONETIC\\nKE-si sir IL-va vi-li-BAZ-ma is-sa. (beat) se BI-sa VA-la sir tol-MI-\\nhi IK-sos DAOR.\\nThis now our war is. (beat) And this man now stranger is not.\\n-------------------------------------------------------------------\\nFAITH OF ANGELS #801\\nMASTER DOCUMENT\\nLanguage Translations\\nCONTINUED:7.25 7.25\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     186.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 186, 'page_label': '187'}, page_content='CONTINUED:7.28 7.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     187.\\nDavid J. Peterson\\nRevised 10/13/17\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. DRAGON HOLDING AREA - DAY1.29 1.29\\n(DANY_801_1.mp3)\\nDANY\\nHow many today?\\nTRANSLATION\\nFinsanneya asshekh?\\nPHONETIC\\nFIN-san-ne-ya ash-SHEKH?\\nHow-many today?\\n-------------------------------------------------------------------\\n(QHONO_801_2.mp3)\\nQHONO\\nTwelve goats. Three sheep.\\nTRANSLATION\\nAkatthi dorve. Sen vaf.\\nPHONETIC\\na-KATH-thi DOR-ve. SEN VAF.\\nCONTINUED:7.28 7.28\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     187.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 187, 'page_label': '188'}, page_content='CONTINUED:1.29 1.29\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     188.\\nTwelve goat. Three sheep.\\n-------------------------------------------------------------------\\nFAITH OF ANGELS #803\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/13/17\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nEXT. BATTLEFIELD - DAY3.23 3.23\\n(DANY_803_1.mp3)\\nDANY\\nRaise your arakhs!\\nTRANSLATION\\nLivano arakh shafki!\\nPHONETIC\\nLI-va-no a-RAKH SHAF-ki!\\nRaise arakh yours!\\n-------------------------------------------------------------------\\nCONTINUED:1.29 1.29\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     188.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 188, 'page_label': '189'}, page_content='(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     189.\\nEXT. FOREST - DAY3.24 3.24\\n(MELISANDRE_803_2.mp3)\\nMELISANDRE\\nLord of Light, cast your light upon us! Lord of Light, defend us! \\nFor the night is dark and full of terrors!\\nTRANSLATION\\nAeksios Oño, aohos oñoso ilon jehikas! Aeksios Oño, ilon misas! \\nKesrio syt bantis zobrie issa se ossyngnoti ledys!\\nPHONETIC\\nAIK-si-os ON-yo, a-O-hos ON-yo-so I-lon je-hi-KAS! AIK-si-os ON-yo, \\nI-lon mi-SAS! KES-ri-o-sit BAN-tis ZO-bri-e is-sa se os-SYNG-no-ti \\nLE-dis!\\nLord of-Light, your light us cast! Lord of-Light, us defend! For \\nnight dark is and terrors full-of!\\n-------------------------------------------------------------------\\nEXT. FOREST - DAY3.72B 3.72B\\n(GREYWORM_803_3.mp3)\\nGREY WORM\\nProtect the retreat! Stand your ground!\\nTRANSLATION\\nMizadá vi zdaguno! Jemel ovadá he ji ghamvaz!\\nPHONETIC\\nmi-za-DA viz-da-GU-no! JE-mel o-va-DA he ji GHAM-vaz!\\nDefend the-flight! Yourselves maintain on the ground!\\n-------------------------------------------------------------------\\nFAITH OF ANGELS #804\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/13/17\\nKEY:\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     189.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 189, 'page_label': '190'}, page_content='CONTINUED:3.72B 3.72B\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     190.\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nINT. MAP ROOM - DAY4.4 4.4\\n(DANY_804_1.mp3)\\nDANY\\nI am safe here, Torgo Nudho. The others can watch over me. You \\nfought hard. You should go rest.\\nTRANSLATION\\nLan yha kizir, Torgo Nudho. Tolyssy yne ozurnebagon kostis. Botose \\nvilipta. Jorilagon avy sytilibas.\\nPHONETIC\\nLAN I-ha KI-zir, TOR-go NU-dho. to-LIS-si i-ne o-zur-NE-ba-gon kos-\\ntis. bo-TO-se vi-LIP-ta. jo-ri-LA-gon a-vi si-ti-LI-bas.\\nI-am safe here, Worm Gray. Others me watch-over can. Hard you-\\nfought. Rest you should.\\n-------------------------------------------------------------------\\nFAITH OF ANGELS #805\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 10/13/17\\nKEY:\\n(Title of Associated .mp3 File)\\nCONTINUED:3.72B 3.72B\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     190.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 190, 'page_label': '191'}, page_content='CONTINUED:4.4 4.4\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     191.\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\n-------------------------------------------------------------------\\n-\\nINT. MAP ROOM - DAY5.6 5.6\\n(DANY_805_1.mp3)\\nDANY\\nIt’s all right, Torgo Nudho. Let me speak with him.\\nTRANSLATION\\nLykso, Torgo Nudho. Ziry ydraon botas.\\nPHONETIC\\nLIK-so, TOR-go NU-dho. ZI-ri I-dra-on bo-TAS.\\nPeace, Worm Gray. Him I-speak-with allow.\\n-------------------------------------------------------------------\\nEXT. GATES - NIGHT5.12 5.12\\n(TYRION_805_2.mp3)\\nTYRION\\nI drink to eat the skull keeper.\\nTRANSLATION\\nNyke mozun ipradagon bartanna raelio.\\nPHONETIC\\nNI-ke MO-zun i-PRA-da-gon bar-TA-na RAI-li-o.\\nI drink to-eat skull keeper.\\n-------------------------------------------------------------------\\n(TYRION_805_3.mp3)\\nCONTINUED:4.4 4.4\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     191.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 191, 'page_label': '192'}, page_content='CONTINUED:5.12 5.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     192.\\nTYRION\\nI want to eat the skull keeper.\\nTRANSLATION\\nNyke jaelio ipradagon bartanno raelia.\\nPHONETIC\\nni-ke JAI-li-o i-PRA-da-gon bar-TA-no RAI-li-a.\\nI want to-eat skull’s keepers.\\n-------------------------------------------------------------------\\n(TYRION_805_4.mp3)\\nTYRION\\nI want to see the--\\nTRANSLATION\\nNyke jaelion urnegon bartanno--\\nPHONETIC\\nni-ke jai-li-on ur-NE-gon bar-TA-no--\\nI want to see skull’s--\\n-------------------------------------------------------------------\\nFAITH OF ANGELS #806\\nMASTER DOCUMENT\\nLanguage Translations\\nDavid J. Peterson\\nRevised 03/02/18\\nKEY:\\n(Title of Associated .mp3 File)\\nCHARACTER NAME\\nEnglish dialogue as written.\\nTRANSLATION\\nOfficial transcription for closed captioning and subtitles.\\nPHONETIC\\nfo-NE-tik REN-dur-ing\\nLiteral translation.\\nCONTINUED:5.12 5.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     192.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 192, 'page_label': '193'}, page_content='CONTINUED: (2)5.12 5.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     193.\\n-------------------------------------------------------------------\\n-\\nEXT. - DAY6.10 6.10\\n(MISSANDEI_806_1A.mp3) & (MISSANDEI_806_1B.mp3)\\nMISSANDEI\\nBlood of my blood! You kept all your promises to me. You killed my \\nenemies in their iron suits! (beat) You tore down their stone \\nhouses!\\nTRANSLATION\\nQoy qoyi! Shafka vernish ei asqoy shafki anhaan. Shafka addrivish \\ndozge anni ma khogaroon shiqethi mori! (beat) Shafka ohharish \\nokrenegwin mori!\\nPHONETIC\\nQOY QO-yi! SHAF-ka ver-NISH E-i as-QOY shaf-ki an-ha-AN. SHAF-ka ad-\\ndri-VISH DOZ-ge an-ni ma kho-ga-ro-ON SHI-qe-thi mo-ri! (beat) SHAF-\\nka oh-ha-RISH o-kre-ne-GWIN mo-ri!\\nBlood of-the-blood! You kept all promises your to-me. You killed \\nenemies my with suits iron their! (beat) You tore-down stone-houses \\ntheir!\\n-------------------------------------------------------------------\\n(MISSANDEI_806_2.mp3)\\nMISSANDEI\\nYou have given me the Seven Kingdoms!\\nTRANSLATION\\nShafka ray azhish anhaan Rhaeshis Andahli!\\nPHONETIC\\nSHAF-ka ray a-ZHISH an-ha-AN rha-e-SHIS an-DAH-li!\\nYou-all have given to-me Kingdoms Seven!\\n-------------------------------------------------------------------\\n(MISSANDEI_806_3.mp3)\\nMISSANDEI\\nTorgo Nudho, you have walked beside me since the Plaza of Pride. You \\nare the bravest of men, the most loyal of soldiers. I name you \\ncommander of all my forces, the Queen’s Master of War.\\nCONTINUED: (2)5.12 5.12\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     193.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 193, 'page_label': '194'}, page_content='CONTINUED:6.10 6.10\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     194.\\nTRANSLATION\\nTorgo Nudho, hin Rangam ez Hozno ynoma dekurupta. Nedyro mentyro \\nhedry pasabarje karaje iksa. Avy tolvio azantyro ñurho jentosy \\nbrozan. Dario Vilibazmaro Aeksyso.\\nPHONETIC\\nTOR-go NU-dho, hin RAN-gam ez HOZ-no i-NO-ma de-ku-RUP-ta. ne-DI-ro \\nmen-ti-RO he-dri pa-sa-BAR-je ka-ra-JE ik-sa. A-vi TOL-vi-o a-ZAN-ti-\\nro NYUR-ho JEN-to-si bro-zan. DA-ri-o vi-li-BAZ-ma-ro aik-SI-so.\\nWorm Gray, from Plaza of Pride with-me you-have-walked. Of-all-\\ncourageous-ones and-of-all-soldiers out-of loyalist and-greatest you-\\nare. You all force my commander I-name. Queen’s war master.\\n-------------------------------------------------------------------\\n(MISSANDEI_806_4A.mp3) & (MISSANDEI_806_4D.mp3)\\nMISSANDEI\\nUnsullied! All of you were torn from your mothers’ arms and raised \\nas slaves. Now you are liberators! You have freed the people of \\nKing’s Landing from the grip of a tyrant! (beat) But the war is not \\nover. We will not lay down our spears until we have liberated all \\nthe people of the world! From Winterfell to Dorne, from Lannisport \\nto Qarth, from the Summer Isles to the Jade Sea, women, men, and \\nchildren have suffered too long beneath the wheel. Will you break \\nthe wheel with me?\\nTRANSLATION\\nDovaogedys! Jeme hen muñoti ñoghoti nadintaks se hae buzdaryti \\nubredaks. Sir daeremirossa iksat! Daro Vililio gierion hen qrinio \\nhilmiot daeredat! (beat) Yn vilibazma tetos daor. Ilvra egralbri \\nqubemiluty daor yn vapar tolvio vyho gieryndi daeredoty! Hen \\nVinterveli va Dornot, hen Laniso Viliniot va Qarthot, hen Jaedria va \\nZeo Embrot, abrar, valar, riñar toli grevo go bottis. Grevi ynoma \\npryjelat?\\nPHONETIC\\ndo-vao-GE-dis! JE-me hen mun-YO-ti nyo-GHO-ti na-DIN-taks se hai buz-\\nda-RI-ti U-bre-daks. SIR dai-re-mi-ROS-sa ik-sat! DA-ro vi-LI-li-o \\ngi-E-ri-on hen KRI-ni-o HIL-mi-ot dai-RE-dat! (beat) in vi-li-BAZ-ma \\nTE-tos DAOR. ILV-ra e-GRAL-bri ku-be-mi-LU-ti DAOR in va-par TOL-vi-\\no VI-ho gi-e-RIN-di dai-RE-do-ti! hen VIN-ter-ve-li va DOR-not, hen \\nLA-ni-so vi-LI-ni-ot va KAR-thot, hen JAI-dri-a va ZE-o EM-brot, A-\\nbrar, VA-lar, rin-YAR TO-li GRE-vo-go BOT-tis. GRE-vi i-NO-ma pri-JE-\\nlat?\\nUnsullied! You-all from mother’s arms were-ripped and like slaves \\nwere-raised. Now liberators you-are! King’s Landing’s people from \\ntyrant’s fist you-have-freed! (beat) But war has-finished not. Our \\nCONTINUED:6.10 6.10\\n(MORE)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     194.'),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 194, 'page_label': '195'}, page_content=\"CONTINUED: (2)6.10 6.10\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     195.\\nspears we-will-lower not but until all world’s peoples we-should-\\nfree! From Winterfell to Dorne, from Lanis’s Port to Qarth, from \\nSummer-Isles to Jade Sea, women, men, and-children too-long wheel-\\nunder they-have-suffered. Wheel with-me will-you-all-break?\\n-------------------------------------------------------------------\\n(MISSANDEI_806_5.mp3)\\nMISSANDEI\\nTake him.\\nTRANSLATION\\nZiry najikatas.\\nPHONETIC\\nZI-ri na-ji-ka-TAS.\\nHim remove.\\n-------------------------------------------------------------------\\nEXT. SHIP’S DECK - DAY6.19 6.19\\n(UCAPTAIN_806_6.mp3)\\nUNSULLIED CAPTAIN\\nAll the men have boarded.\\nTRANSLATION\\nUni vali lis va loghor.\\nPHONETIC\\nU-ni VA-li LIS va LO-ghor.\\nAll men are on board.\\n-------------------------------------------------------------------\\n(GREYWORM_806_7.mp3)\\nGREY WORM\\nGood.\\nTRANSLATION\\nSyz.\\nPHONETIC\\nSIZ.\\nCONTINUED: (2)6.10 6.10\\nPHONETIC (CONT'D)\\n(CONTINUED)\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     195.\"),\n",
       " Document(metadata={'producer': 'macOS Version 10.15.4 (Build 19E266) Quartz PDFContext', 'creator': 'Final Draft 11', 'creationdate': \"D:20200412013934Z00'00'\", 'title': 'Master Dialogue for Game of Thrones Seasons 3 through 8', 'author': 'David J. Peterson', 'moddate': \"D:20200412013934Z00'00'\", 'source': 'E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf', 'total_pages': 196, 'page': 195, 'page_label': '196'}, page_content='CONTINUED:6.19 6.19\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     196.\\nGood.\\n-------------------------------------------------------------------\\n(GREYWORM_806_8.mp3)\\nGREY WORM\\nWe sail for the Isle of Naath.\\nTRANSLATION\\nSuli va v’Ajo Naath.\\nPHONETIC\\nSU-li va VA-jo NAATH.\\nWe-sail to the-Island Naath.\\n-------------------------------------------------------------------\\nEXT. NORTH BATTLEFIELD6.115 6.115\\n(DRIDER_806_9.mp3)\\nDOTHRAKI RIDER\\nGet ready! Get ready!\\nTRANSLATION\\nHethkos! Hethkos!\\nPHONETIC\\nheth-KOS! heth-KOS!\\nGet-ready! Get-ready!\\n-------------------------------------------------------------------\\nCONTINUED:6.19 6.19\\nGAME OF THRONES   Master Dialogues (Season 3-8)   04/11/20     196.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"E:/Agentic AI/25-05-2025/DataIngestion/game_of_thrones_master_dialogue_s3s8.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac3b0e",
   "metadata": {},
   "source": [
    "### Web based loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3086b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://python.langchain.com/docs/concepts/output_parsers/\",),\n",
    "    bs_kwargs=dict( parse_only = bs4.SoupStrainer(\n",
    "        class_=(\"post-title\",\n",
    "            \"post-content\",\n",
    "            \"post-date\")\n",
    "    )),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b355a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/concepts/output_parsers/'}, page_content='')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webdoc = web_loader.load()\n",
    "webdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "756023ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "rp_doc = ArxivLoader(query=\"1706.03762\", load_max_docs=2)\n",
    "\n",
    "content_rp = rp_doc.load()\n",
    "content_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99203ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India\\'s highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\\n\\n\\n== Early life ==\\nDhoni was born on 7 July 1981 in Ranchi, Bihar (now in Jharkhand) in a Hindu Rajput family to Pan Singh and Devaki Devi. His parents hailed from Lwali village in Uttar Pradesh (now Uttarakhand) and he was the youngest of three children. His family spells the surname as \"Dhauni\". The spelling \"Dhoni\" emerged due to a spelling mistake in his school certificates and, despite repeated attempts by his family, has never been rectified.\\nDhoni did his schooling from DAV Jawahar Vidya Mandir, where he started playing football as a goalkeeper, but later moved to play cricket on the suggestion of his coach Keshav Banerjee. From 2001 to 2003, Dhoni worked as a Travelling Ticket Examiner (TTE) at Kharagpur under South Eastern Railway zone of Indian Railways.\\n\\n\\n== Youth career ==\\nHe played as a wicket-keeper for Commando cricket club from 1995 to 1998 and Central Coal Fields Limited (CCL) team in 1998. At CCL, he batted higher up the order and helped the team qualify to the higher division. Based on his performance at club cricket, he was picked for the 1997/98 season of Vinoo Mankad Trophy under-16 championship. In the 1998–99, Dhoni played for Bihar U-19 team in the Cooch Behar Trophy and scored 176 runs in 5 matches. In the 1999–2000 Cooch Behar Trophy, the Bihar U-19 cricket team made it to the finals, where Dhoni made 84 in a losing cause. Dhoni\\'s contribution in the tournament included 488 runs in nine matches with five fifties, 17 catches and seven stumpings. Dhoni made it to the East Zone U-19 squad for the C. K. Nayudu Trophy in the 1999–2000 season and scored only 97 runs in four matches, as East Zone lost all the matches and finished last in the tournament.\\nDhoni made his Ranji Trophy debut for Bihar against Assam in the 1999–2000 season, as an eighteen-year-old scoring 68 runs '), Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\\n== Plot ==\\nIn the pre-credits sequence there is a scene of the 2011 Cricket World Cup Final. MS Dhoni, India's captain, walks out to bat after Virat Kohli's wicket.\\nThe film begins in Ranchi, 7 July 1981. At the hospital maternity unit, Paan Singh Dhoni is confused whether he has got a girl or boy. He later names his baby boy Mahendra 'Mahi' Singh Dhoni. Paan Singh is a pump operator who waters the practice ground. Fourteen years later, Mahi is spotted by a cricket coach while goalkeeping in a football game. He invites him to try out for the school cricket team as a wicketkeeper and selects him after being impressed. Mahi improves his batting and becomes a regular member of the team.\\nThree years later, a grown up Mahi helps win an inter-school cricket match. After achieving much fame, Mahi is selected for the Ranji Trophy but his draft notice is held up due to which he is late in reaching Kolkata despite his friends' help. But Mahi does not give up and, to please his father, he joins the Kharagpur Station as a ticket collector. Years later, Mahi's sister Jayanti is married to his friend Gautam Gupta.\\nAfter some time, Mahi is depressed with his job. With the insistence of his manager, Mahi decides to play cricket alongside his work, and after his day-shifts he goes to practice cricket. He participates in different tournaments and as a result he gets selected for the Railways. After a good performance, he tries-out for the India national under-19 cricket team selections. Bihar loses to Punjab where Yuvraj Singh scores 301 and Mahi does not succeed though he is selected for the Duleep Trophy.\\nMahi leaves his job and admits to his father that cricket is his only ambition and he wants to become a professional cricketer. He works hard and is selected in the national team and makes his debut. He meets and befriends Priyanka Jha, an office consultant, and scores a century after meeting her. She buys a watch for him as a Valentine's Day gift but dies in a truck accident on her way. Mahi again goes into depression and has bad form in the 2007 Cricket World Cup. As captain of the national side, he wins the T-20 World Cup, and leads India to the number one ranking in Test matches.\\nIn 2010, Mahi arrives at a hotel. Sakshi Singh Rawat, a hotel intern (catering management student) fails to recognize him and later apologizes to him. They soon start dating and Mahi eventually proposes marriage to her after she mentions buying him a Valentine's Day's gift which he refuses. They marry and Mahi begins training for the 2011 World Cup. He eve\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "wiki_load = WikipediaLoader(query=\"MS Dhoni\", load_max_docs=2).load()\n",
    "\n",
    "print(wiki_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b355c",
   "metadata": {},
   "source": [
    "## text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac109da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "final_doc = text_splitter.split_documents(content_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf5aa579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='relying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='block, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='used successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='layers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='i=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='i , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='information flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='consists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='dff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Operations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='different layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='sentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='A single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='target tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='warmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='surpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='dropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Table 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='in different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='per-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='models have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='for the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='semi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Recurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='plan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='tensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='network grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Information Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='In International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='arXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='layer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='arXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='It\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis'),\n",
       " Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content=',\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b2205d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbc7710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cac8dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,' metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n"
     ]
    }
   ],
   "source": [
    "print(final_doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e527b",
   "metadata": {},
   "source": [
    "#### Charactertextsplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bd34642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 808, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 525, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 905, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 527, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 527, which is longer than the specified 500\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 590, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "\n",
    "char_text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "websplit = char_text_splitter.split_documents(wiki_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f5cdb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.'),\n",
       " Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='Born in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.'),\n",
       " Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='In the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.'),\n",
       " Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content=\"In 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\\n== Early life ==\"),\n",
       " Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='== Early life ==\\nDhoni was born on 7 July 1981 in Ranchi, Bihar (now in Jharkhand) in a Hindu Rajput family to Pan Singh and Devaki Devi. His parents hailed from Lwali village in Uttar Pradesh (now Uttarakhand) and he was the youngest of three children. His family spells the surname as \"Dhauni\". The spelling \"Dhoni\" emerged due to a spelling mistake in his school certificates and, despite repeated attempts by his family, has never been rectified.'),\n",
       " Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='Dhoni did his schooling from DAV Jawahar Vidya Mandir, where he started playing football as a goalkeeper, but later moved to play cricket on the suggestion of his coach Keshav Banerjee. From 2001 to 2003, Dhoni worked as a Travelling Ticket Examiner (TTE) at Kharagpur under South Eastern Railway zone of Indian Railways.\\n== Youth career =='),\n",
       " Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content=\"He played as a wicket-keeper for Commando cricket club from 1995 to 1998 and Central Coal Fields Limited (CCL) team in 1998. At CCL, he batted higher up the order and helped the team qualify to the higher division. Based on his performance at club cricket, he was picked for the 1997/98 season of Vinoo Mankad Trophy under-16 championship. In the 1998–99, Dhoni played for Bihar U-19 team in the Cooch Behar Trophy and scored 176 runs in 5 matches. In the 1999–2000 Cooch Behar Trophy, the Bihar U-19 cricket team made it to the finals, where Dhoni made 84 in a losing cause. Dhoni's contribution in the tournament included 488 runs in nine matches with five fifties, 17 catches and seven stumpings. Dhoni made it to the East Zone U-19 squad for the C. K. Nayudu Trophy in the 1999–2000 season and scored only 97 runs in four matches, as East Zone lost all the matches and finished last in the tournament.\"),\n",
       " Document(metadata={'title': 'MS Dhoni', 'summary': \"Mahendra Singh Dhoni ( ; born 7 July 1981) is an Indian professional cricketer who plays as a right-handed batter and a wicket-keeper. Widely regarded as one of the most prolific wicket-keeper batsmen and captains and one of the greatest ODI batsmen, he represented the Indian cricket team and was the captain of the side in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. Dhoni has captained the most international matches and is the most successful Indian captain. He has led India to victory in the 2007 ICC World Twenty20, the 2011 Cricket World Cup, and the 2013 ICC Champions Trophy, being the only captain to win three different limited overs ICC tournaments. He also led the teams that won the Asia Cup in 2010, 2016 and was a member of the title winning squad in 2018.\\nBorn in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014 but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\\nIn the Indian Premier League (IPL), Dhoni plays for Chennai Super Kings (CSK), leading them to the final on ten occasions and winning it five times (2010, 2011, 2018, 2021 and 2023 ) jointly sharing this title with Rohit Sharma . He has also led CSK to two Champions League T20 titles in 2010 and 2014. Dhoni is among the few batsmen to have scored more than five thousand runs in the IPL, as well as being the first wicket-keeper to do so.\\nIn 2008, Dhoni was awarded India's highest sport honour Major Dhyan Chand Khel Ratna Award by Government of India. He received the fourth highest civilian award Padma Shri in 2009 and third highest civilian award Padma Bhushan in 2018. Dhoni holds an honorary rank of Lieutenant colonel in the Parachute Regiment of the Indian Territorial Army which was presented to him by the Indian Army in 2011.\", 'source': 'https://en.wikipedia.org/wiki/MS_Dhoni'}, page_content='Dhoni made his Ranji Trophy debut for Bihar against Assam in the 1999–2000 season, as an eighteen-year-old scoring 68 runs'),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content='M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.'),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"The idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\"),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content='The film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).'),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"== Plot ==\\nIn the pre-credits sequence there is a scene of the 2011 Cricket World Cup Final. MS Dhoni, India's captain, walks out to bat after Virat Kohli's wicket.\"),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"The film begins in Ranchi, 7 July 1981. At the hospital maternity unit, Paan Singh Dhoni is confused whether he has got a girl or boy. He later names his baby boy Mahendra 'Mahi' Singh Dhoni. Paan Singh is a pump operator who waters the practice ground. Fourteen years later, Mahi is spotted by a cricket coach while goalkeeping in a football game. He invites him to try out for the school cricket team as a wicketkeeper and selects him after being impressed. Mahi improves his batting and becomes a regular member of the team.\"),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"Three years later, a grown up Mahi helps win an inter-school cricket match. After achieving much fame, Mahi is selected for the Ranji Trophy but his draft notice is held up due to which he is late in reaching Kolkata despite his friends' help. But Mahi does not give up and, to please his father, he joins the Kharagpur Station as a ticket collector. Years later, Mahi's sister Jayanti is married to his friend Gautam Gupta.\"),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content='After some time, Mahi is depressed with his job. With the insistence of his manager, Mahi decides to play cricket alongside his work, and after his day-shifts he goes to practice cricket. He participates in different tournaments and as a result he gets selected for the Railways. After a good performance, he tries-out for the India national under-19 cricket team selections. Bihar loses to Punjab where Yuvraj Singh scores 301 and Mahi does not succeed though he is selected for the Duleep Trophy.'),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"Mahi leaves his job and admits to his father that cricket is his only ambition and he wants to become a professional cricketer. He works hard and is selected in the national team and makes his debut. He meets and befriends Priyanka Jha, an office consultant, and scores a century after meeting her. She buys a watch for him as a Valentine's Day gift but dies in a truck accident on her way. Mahi again goes into depression and has bad form in the 2007 Cricket World Cup. As captain of the national side, he wins the T-20 World Cup, and leads India to the number one ranking in Test matches.\"),\n",
       " Document(metadata={'title': 'M.S. Dhoni: The Untold Story', 'summary': \"M.S. Dhoni: The Untold Story is a 2016 Indian Hindi-language biographical sports drama film directed and co-written by Neeraj Pandey. It is based on the life of former Test, ODI and T20I captain of the Indian national cricket team, Mahendra Singh Dhoni. The film stars the late Sushant Singh Rajput as MS Dhoni, along with Disha Patani, Kiara Advani, and Anupam Kher. The film chronicles the life of Dhoni from a young age through a series of life events.\\nThe idea of the biopic was put forward by Dhoni's manager, Arun Pandey, after encountering an incident at an airport after the 2011 Cricket World Cup Final. Development began two years later, with the consent of Dhoni. Neeraj Pandey was later approached to helm the film while he was working on Baby. Pandey recruited a number of people for researching into Dhoni's background and his life events. Dhoni eventually became a consultant on the film.\\nThe film was released on 30 September 2016 by Fox Star Studios and received the widest release ever for a Bollywood film across 61 countries. In addition to being released in Hindi language, it was also dubbed in Tamil, Telugu, and Marathi languages, although the Marathi release was later cancelled due to opposition. Upon release, the film became a critical and commercial success. It is the fifth highest-grossing Bollywood film of 2016 and sixth highest grossing Indian film of 2016 worldwide ₹215.48 crore (US$25 million).\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/M.S._Dhoni:_The_Untold_Story'}, page_content=\"In 2010, Mahi arrives at a hotel. Sakshi Singh Rawat, a hotel intern (catering management student) fails to recognize him and later apologizes to him. They soon start dating and Mahi eventually proposes marriage to her after she mentions buying him a Valentine's Day's gift which he refuses. They marry and Mahi begins training for the 2011 World Cup. He eve\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c447f4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'E:\\\\Agentic AI\\\\25-05-2025\\\\DataIngestion\\\\sample.json', 'seq_num': 1}, page_content='{\"product_id\": \"SKU123456\", \"product_name\": \"Motorola Edge Fusion\", \"brand\": \"Motorola\", \"category\": \"Smartphones\", \"price\": {\"currency\": \"USD\", \"value\": 649.99, \"discount\": {\"type\": \"percentage\", \"value\": 10}, \"final_price\": 584.99}, \"availability\": \"in_stock\", \"ratings\": {\"average_rating\": 4.5, \"total_reviews\": 324}, \"product_details\": {\"description\": \"Experience blazing speed and vibrant visuals with the Motorola Edge Fusion, equipped with a Snapdragon 888 processor and a 6.7-inch OLED display.\", \"specs\": {\"display\": \"6.7-inch OLED\", \"processor\": \"Qualcomm Snapdragon 888\", \"ram\": \"8GB\", \"storage\": \"128GB\", \"battery\": \"4500mAh\", \"camera\": {\"rear\": \"50MP + 12MP + 8MP\", \"front\": \"16MP\"}, \"os\": \"Android 12\", \"network\": \"5G\"}}, \"images\": [\"https://example.com/images/motorola_edge_fusion_front.jpg\", \"https://example.com/images/motorola_edge_fusion_back.jpg\"], \"shipping\": {\"free_shipping\": true, \"estimated_delivery_days\": 5, \"return_policy\": \"30-day return window\"}, \"available_on\": [{\"platform\": \"Amazon\", \"url\": \"https://www.amazon.com/Motorola-Edge-Fusion\"}, {\"platform\": \"Flipkart\", \"url\": \"https://www.flipkart.com/motorola-edge-fusion\"}]}')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "json_doc = JSONLoader(\n",
    "    file_path=\"E:/Agentic AI/25-05-2025/DataIngestion/sample.json\",\n",
    "    jq_schema=\".\",\n",
    "    text_content=False  # Set True if you want to treat extracted objects as strings\n",
    ")\n",
    "\n",
    "loaded_doc = json_doc.load()\n",
    "print(loaded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ed8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
